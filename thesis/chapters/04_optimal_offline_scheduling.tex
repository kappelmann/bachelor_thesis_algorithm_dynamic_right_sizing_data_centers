% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Optimal Offline Scheduling}
Now, as we have finished our preliminary work, we are able to derive our first two algorithms; more precisely, we will derive two optimal offline algorithms in this chapter.
To begin, we will reduce our problem instance $\inp$ to a shortest path problem of a weighted directed acyclic graph~$G$. Then we proceed to find a shortest path in $G$ and thereby an optimal schedule for~$\inp$ in pseudo-polynomial time $\Theta(Tm^2)$.
After that, we refine our initial approach to derive an improved algorithm with pseudo-linear time complexity $\Theta(Tm)$.

\section{A Pseudo-Polynomial-Time Algorithm}\label{sec:opt_offline_pseudo_poly}
Let $\inp$ be a problem instance. Thanks to our preliminary work, we know that there exists an optimal schedule which is identifiable by its sequence of the number of active servers~$\mx$. In order to find this sequence, we consider the weighted directed acyclic graph $G$ defined as follows:
\begin{align*}
	&V\coloneqq\bigl\{v_{x,t}\mid x\in[m]_0,t\in[T]\bigr\}\dotcup\{v_{0,0},v_{0,T+1}\}\\
	&E\coloneqq\bigl\{(v_{x,t},v_{x',t+1})\mid x,x'\in[m]_0,t\in[T]_0,v_{x,t},v_{x',t+1}\in V\bigr\}\\
	&c_G(v_{x,t},v_{x',t+1})\coloneqq\costs(x,x',\lambda_{t+1})\\
	&G\coloneqq(V,E,c_G)
\end{align*}
For any possible number of active servers $x$ and any time slot $t$, we add a node $v_{x,t}$. Moreover, we add a start node $v_{0,0}$ as well as an end node $v_{0,T+1}$. Next, we connect all nodes to their successors with respect to time. Semantically, $v_{x,t}$ denotes the state of distributing the arrival rate $\lambda_{t}$ evenly to $x$ servers in time slot $t$. For any edge connecting~$v_{x,t}$ with $v_{x',t+1}$, we assign costs $\costs(x,x',\lambda_{t+1})$, that is the edge's cost corresponds to switching from $x$ to $x'$ machines and processing the load $\lambda_{t+1}$ with $x'$ machines. A graphical representation can be found in the following figure.
\begin{figure}[H]
\includestandalone[width=\textwidth]{../figures/graph_pseudo_poly}
\caption{Weighted directed acyclic graph for a pseudo-polynomial-time optimal offline algorithm}
\label{fig:graph_pseudo_poly}
\end{figure}
As we can see in Figure~\ref{fig:graph_pseudo_poly}, the cost of a path $P=(v_{0,0},v_{x_1,1},\dotsc,v_{x_T,T},v_{0,T+1})$ from our start node $v_{0,0}$ to our end node $v_{0,T+1}$ is given by 
\begin{equation}
	\costs(P)=\costs(0,x_1,\lambda_1)+\sum\limits_{t=2}^{T}\costs(x_{t-1},x_{t},\lambda_{t})+\overbrace{\costs(x_T,0,0)}^{0}=\costs(0,x_1,\lambda_1)+\sum\limits_{t=2}^{T}\costs(x_{t-1},x_{t},\lambda_{t})\label{eq:v0_VT1_path_costs}
\end{equation}
Note that the cost of such a path directly corresponds to that of a schedule $\mx$ (see~\eqref{eq:mx_schedule_total_costs}).
Any shortest path from $v_{0,0}$ to $v_{0,T+1}$ is thus forced to minimize the cost of the corresponding schedule. Needless to say, this demands for a proof of correctness.
\begin{lem}\label{lem:sched_path_pseudo_poly}
Let $\bm{\mx}$ be the set of all schedules $\mx$ for $\inp$, and let $\bm{\mathcal{P}}$ the set of all paths from $v_{0,0}$ to $v_{0,T+1}$. The map
\begin{equation*}
	\Phi:\bm{\mathcal{P}}\rightarrow\bm{\mx},\quad (v_{0,0},v_{x_1,1},v_{x_2,2},\dotsc,v_{x_T,T},v_{0,T+1})\mapsto (x_1,\dotsc,x_T)
\end{equation*}
is a bijection with inverse map
\begin{equation*}
	\Psi:\bm{\mx}\rightarrow\bm{\mathcal{P}},\quad(x_1,\dotsc,x_T)\mapsto (v_{0,0},v_{x_1,1},v_{x_2,2},\dotsc,v_{x_T,T},v_{0,T+1})
\end{equation*}
satisfying $\costs(\mx)=\costs\bigl(\Psi(\mx)\bigr)$.
\end{lem}
\begin{proof}
It is easy to check that $\Psi\circ\Phi=id_{\bm{\mathcal{P}}}$ and $\Phi\circ\Psi=id_{\bm{\mx}}$ (the functions merely extract and embed the states $x_t$). Thus, $\Phi$ is indeed bijective with inverse map $\Psi$. Next, let $\mx=(x_1,\dotsc,x_T)$ be a schedule for $\inp$. We have
\begin{equation*}
	P\coloneqq\Psi(\mx)=(v_{0,0},v_{x_1,1},v_{x_2,2},\dotsc,v_{x_T,T},v_{0,T+1})
\end{equation*}
We examine the cost of $\mx$ and conclude
\begin{equation*}
\costs(\mx)\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{T}\costs(x_{t-1},x_{t},\lambda_t)=\underbrace{\costs(x_0,x_1,\lambda_1)}_{=\costs(0,x_1,\lambda_1)}+\sum\limits_{t=2}^{T}\costs(x_{t-1},x_{t},\lambda_t)\stackrel{\eqref{eq:v0_VT1_path_costs}}{=}\costs(P)
\end{equation*}
which shows that $\Psi$ and, as a consequence, $\Phi$ are cost-preserving maps.
\end{proof}
We can now use this bijection to obtain our desired result: the correspondence between optimal schedules and shortest paths.
\begin{thm}\label{thm:opt_sched_short_path_pseudo_poly}
There exists a cost-preserving bijection between optimal schedules $\mx^*$ for~$\inp$ and shortest paths from $v_{0,0}$ to $v_{0,T+1}$.
\end{thm} 
\begin{proof}
By Lemma~\ref{lem:sched_path_pseudo_poly}, we have a bijection $\Psi$ between schedules~$\mx$ and paths from~$v_{0,0}$ to~$v_{0,T+1}$ obeying $\costs(\mx)=\costs\bigl(\Psi(\mx)\bigr)$. Thus, we have 
\begin{equation*}
	\costs(\mx)\text{ minimal}\iff \costs\bigl(\Psi(\mx)\big)\text{ minimal}
\end{equation*}
and the claim follows.
\end{proof}
In the following, we give an algorithm based on our just verified construction. 
We split our procedure into two subroutines. 

\textproc{shortest\_paths} calculates the minimum costs of the graph's nodes, layer by layer; that is, it calculates the shortest paths following the graph's topological sorting. It returns the minimum costs to all nodes as well as the predecessor of any node with respect to its shortest path.

\textproc{extract\_schedule} uses the predecessors calculated by \textproc{shortest\_paths} in order to obtain the sequence of nodes describing a shortest path; thereby, it calculates an optimal schedule for our problem instance.

\begin{algorithm}[ht]
  \caption{Pseudo-polynomial-time optimal offline scheduling}
  \begin{algorithmic}[1]
  \Function{optimal\_offline\_scheduling}{$m,T,\Lambda,\beta,f$}
	  \Let{$(C,P)$}{\Call{shortest\_paths}{$m,T,\Lambda,\beta,f$}}
	  \Let{$\mx$}{\Call{extract\_schedule}{$C,P,T$}}
	  \State \Return{$\mx$}
  \EndFunction
  \Statex
  \Function{shortest\_paths}{$m,T,\Lambda,\beta,f$}
	\Blet{$C[0\dotso m,1\dotso T]$ and $P[0\dotso m,1\dotso T]$}{new tables}
	\Statex \Comment{Allocate cost and predecessor tables}
	\For{$x \gets 0 \textrm{ to } m$}\Comment{Initialize first layer}
		\Let{$P[x,1]$}{$0$ and $C[x,1]\gets\costs(0,x,\lambda_1)$}
	\EndFor
	\For{$t \gets 2 \textrm{ to } T$}\Comment{Iteratively calculate costs and predecessors}
		\For{$x' \gets 0 \textrm{ to } m$}
			\Let{$P[x',t]$}{$\argmin\limits_{x\in[m]_0}\bigl\{C[x,t-1]+\costs(x,x',\lambda_t)\bigr\}$}\Comment{Get best preceding choice}
			\Let{$C[x',t]$}{$C\bigl[P[x',t],t-1\bigr]+c\bigl(P[x',t],x',\lambda_t\bigr)$}\Comment{Set the node's cost}
		\EndFor
	\EndFor
	\State \Return{$(C,P)$}
  \EndFunction
  \Statex
  \Function{extract\_schedule}{$C,P,T$}
	\Blet{$\mx[1\dotso T]$}{a new array}\Comment{Allocate the schedule array}
	  \Let{$\mx[T]$}{$\argmin\limits_{x\in[m]_0}\bigl\{C[x,T]\bigr\}$}\Comment{Find best choice for last time slot}
        \For{$t \gets T-1 \textrm{ to } 1$}\Comment{Iteratively obtain schedule from predecessor table}
		\Let{$\mx[t]$}{$P\bigl[\mx[t+1],t+1\bigr]$}
	\EndFor
	\State \Return{$\mx$}
  \EndFunction
  \end{algorithmic}
\label{alg:opt_offline_pseudo_poly}
\end{algorithm}
The correctness of Algorithm~\ref{alg:opt_offline_pseudo_poly} directly follows from Theorem~\ref{thm:opt_sched_short_path_pseudo_poly} and the correctness of the shortest path calculation for directed acyclic graphs (for a proof see~\parencite[Section~24.2]{intro-algo}).

Naturally, we are interested in our algorithm's time and memory complexity.
For this, we first need to consider the size of our input $\inp=(m,T,\Lambda,\beta,f)$. In theory, our function $f:[0,1]\rightarrow \mathbb{R}$ may be easily defined by saying, for example, $f(\lambda)\coloneqq \lambda^2$; however, practically, it is difficult to answer how such a function may be specified and what the size of such a function as part of the input may be. For simplicity, we consider the size of~$f$ negligible in comparison with the remaining input variables' size. Further, we assume that a non-negative real number $r$ requires $\Theta\bigl(\log_2(r)\bigr)$ bits for its encoding; if $r$ turns out to be smaller than $1$, we assume some minor constant encoding size. The size of our input is then given by
\begin{align}
	size(\inp)&=size(m)+size(T)+size(\Lambda)+size(\beta)\nonumber\\
	&=\Theta\bigl(\log_2(m)\bigr)+\Theta\bigl(\log_2(T)\bigr)+\sum\limits_{i=1}^T \Theta\bigl(\log_2(\lambda_i)\bigr)+\Theta\bigl(\log_2(\beta)\bigr)\nonumber\\
	&=\Theta\bigl(\log_2(m)+\log_2(T)+\log_2(\beta)\bigr)+\sum\limits_{i=1}^T \Theta\bigl(\log_2(\overbrace{\lambda_i}^{\le m})\bigr)\nonumber\\
	&\le\Theta\bigl(\log_2(m)+\log_2(T)+\log_2(\beta)\bigr)+\mathcal{O}\bigl(T\log_2(m)\bigr)\nonumber\\
	&\le\mathcal{O}\bigl(T\log_2(m)+\log_2(\beta)\bigr)\label{eq:inp_size}
\end{align}
For our runtime analysis, we assume that calling the operating cost function $f(\cdot)$, and as a consequence also $\costs(\cdot,\cdot,\cdot)$, incurs constant cost. Subroutine \textproc{shortest\_paths} requires $\Theta(m)$ steps for its initialization and $\Theta(Tm^2)$ steps for the iterative calculation of the nodes' predecessors and costs. In addition, \textproc{extract\_schedule} needs $\Theta(m)$ steps for its initial minimization search and $\Theta(T)$ iterations for its schedule retrieval. Thus, Algorithm~\ref{alg:opt_offline_pseudo_poly} has a time complexity of
\begin{equation*}
	\Theta(m+Tm^2+m+T)=\Theta(Tm^2)
\end{equation*}
The runtime is polynomial in the numeric value of $m$ and $T$; however, it is exponential in the size of the input since, as we saw in~\eqref{eq:inp_size}, we only need $\log_2(m)$ bits to encode $m$. Hence, the algorithm is pseudo-polynomial.

Our memory demand is determined by the size of the array $\mx$ and the size of the tables $C$ and $P$. The former is of size $\Theta(T)$, and the latter are of size $\Theta(Tm)$. Thus, Algorithm~\ref{alg:opt_offline_pseudo_poly} has a memory complexity of $\Theta(T+2Tm)=\Theta(Tm)$.

\section{A Pseudo-Linear-Time Algorithm}\label{sec:opt_offline_pseudo_lin}
The algorithm developed in Section~\ref{sec:opt_offline_pseudo_poly} is of a quite simple nature. Its underlying graph~$G$ is able to represent any possible schedule $\mx$ since we simply add an edge for any possible scheduling choice at any possible time slot. This approach seems rather intuitive and readily verifiable, but this convenience comes with a cost: The density of $G$ causes a quadratic runtime in the number of servers $m$. In order to improve the runtime to pseudo-linear complexity, we need to ``thin out'' our graph. 

Let us revise our initial approach. Our graph consists of nodes $v_{x,t}$, any of which represents the state of distributing the arrival rate $\lambda_{t}$ evenly to~$x$ servers in time slot~$t$. The algorithm calculates the minimum costs to all those nodes. Thus, for any node~$v_{x,t}$, it returns the lowest achievable cost up to time slot~$t$ of all schedules~$\mx$ that assign~$x$ servers at time~$t$ to process the arrival rate $\lambda_t$. In particular, the cost of the end node $v_{0,T+1}$ tells us the minimum cost of all schedules. This approach, however, does not consider the possibility to schedule $y\neq x$ servers in time slot~$t$ and to switch to~$x$ servers just at the very last moment of~$t$ when calculating the cost of $v_{x,t}$. Consider the following example:
\begin{exmpl}
Let $\inp_i=(m=1,T=2,\Lambda_i,\beta=1,f)$ be the inputs for two problem instances where $f(\lambda)=\lambda^2+1$, $\Lambda_1=(1,0)$, and $\Lambda_2=(0,1)$. Below we illustrate the graphs of the two problem instances and the corresponding calculations done by Algorithm~\ref{alg:opt_offline_pseudo_poly}.
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{../figures/graph_pseudo_poly_shortcome_1}
\caption{\underline{Problem $\inp_1$:} State $v_{0,1}$ could be reached with cost $3$ by moving down from node $v_{1,1}$.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{../figures/graph_pseudo_poly_shortcome_2}
\caption{\underline{Problem $\inp_2$:} State $v_{1,1}$ could be reached with cost $1$ by moving up from $v_{0,1}$.}
\end{subfigure}
\caption{Two examples depicting a shortcoming of our initial approach. The calculated costs are highlighted in red. Dashed edges are not part of Algorithm~\ref{alg:opt_offline_pseudo_poly}.}
\end{figure}
\end{exmpl}
Although our algorithm delivers the correct end results, its immediate steps are somewhat unsatisfying. We want our states to capture a more general notion than given in Section~\ref{sec:opt_offline_pseudo_poly}; preferably, we would like a node $v_{x,t}$ to denote the state of having $x$ active servers at the end of time slot $t$. In practice, we may reach such a state $v_{x,t}$ by moving down from a state $v_{y^\downarrow,t}$ where $y^\downarrow>x$ with cost $0$ or by moving up from a state $v_{y^\uparrow,t}$ where $y^\uparrow<x$ with cost $\beta(x-y^\uparrow)$. In order to allow for these new possibilities, given a problem instance $\inp$, we define a weighted directed acyclic graph as follows:
\begin{align*}
	V&\coloneqq\bigl\{v_{x,t\downarrow}\mid x\in[m]_0,t\in[T]\bigr\}\dotcup\bigl\{v_{x,t\uparrow}\mid x\in[m]_0,t\in[T-1]\bigr\}\dotcup\{v_{0,0}\}\\
	E_s&\coloneqq\bigl\{(v_{0,0},v_{x,1\downarrow})\mid x\in[m]_0\bigr\}\\
	E_\downarrow&\coloneqq\bigl\{(v_{x,t\downarrow},v_{x-1,t\downarrow})\mid x\in[m],t\in[T]\bigr\}\\
	E_\uparrow&\coloneqq\bigl\{(v_{x-1,t\uparrow},v_{x,t\uparrow})\mid x\in[m],t\in[T-1]\bigr\}\\
	E_{\downarrow\uparrow}&\coloneqq\bigl\{(v_{x,t\downarrow},v_{x,t\uparrow})\mid x\in[m]_0,t\in[T-1]\bigr\}\\
	E_{\uparrow\downarrow}&\coloneqq\bigl\{(v_{x,t\uparrow},v_{x,t+1\downarrow})\mid x\in[m]_0,t\in[T-1]\bigr\}\\
	E&\coloneqq E_s\dotcup E_\downarrow\dotcup E_\uparrow\dotcup E_{\downarrow\uparrow}\dotcup E_{\uparrow\downarrow}\\
	c_G(e)&\coloneqq
	\begin{cases}
		\costs(0,x,\lambda_1), & \text{if $e=(v_{0,0},v_{x,1\downarrow})\in E_s$}\\
		\opcosts(x,\lambda_{t+1}), & \text{if $e=(v_{x,t\uparrow},v_{x,t+1\downarrow})\in E_{\uparrow\downarrow}$}\\
		\beta, & \text{if $e\in E_\uparrow$}\\
		0, & \text{if $e\in(E_\downarrow\dotcup E_{\downarrow\uparrow})$}
	\end{cases}\\
	G&\coloneqq(V,E,c_G)
\end{align*}
A more appealing, graphical representation can be found in the following figure.
\begin{figure}[H]
\includestandalone[width=\textwidth]{../figures/graph_pseudo_lin}
\caption{Graph for a pseudo-linear-time optimal offline algorithm; the path of the topological sorting is highlighted in red.}
\label{fig:graph_pseudo_lin}
\end{figure}
For any possible number of active servers $x$ and any time slot $t$, we add a node~$v_{x,t\downarrow}$. Semantically, the cost of $v_{x,t\downarrow}$ will denote the minimum cost up to time slot $t$ when processing $\lambda_t$ with $x$ or more servers. Further, for any time slot $t\in[T-1]$, we add a node~$v_{x,t\uparrow}$. The cost of $v_{x,t\uparrow}$ will denote the minimum cost of having $x$ active servers at the end of time slot $t$. Moreover, we add a start node $v_{0,0}$ and an end node $v_{0,T\downarrow}$.

The set of edges $E_s$ denotes the start initialization step. An edge $(v_{x,t\uparrow},v_{x,t+1\downarrow})\in E_{\uparrow\downarrow}$ accounts for the operating costs that incur when processing the arrival rate $\lambda_{t+1}$ with $x$ active servers.

After any time step from $t-1$ to $t$, we have a minimization step in our graph. For this, we first move down the chain $v_{m,t\downarrow},v_{m-1,t\downarrow},\dotsc,v_{0,t\downarrow}$ using edges from $E_\downarrow$ with cost $0$. Then we proceed to move to the right from $v_{0,t\downarrow}$ to $v_{0,t\uparrow}$. Lastly, we move up the chain $v_{0,t\uparrow},v_{1,t\uparrow},\dotsc,v_{m,t\uparrow}$ using edges from $E_\uparrow$ with cost $\beta$. In order to have the possibility to keep the calculated cost of $v_{x,t\downarrow}$ while moving up, we add edges $(v_{x,t\downarrow},v_{x,t\uparrow})\in E_{\downarrow\uparrow}$ with cost $0$.  
This minimization step is the key to our runtime improvement. It facilitates the determination of the best predecessor of a state $v_{x,t\downarrow}$ since we already know that the minimum cost of having $x$ servers at the end of time slot $t-1$ is stored in $v_{x,t-1\uparrow}$. Thus, the cheapest possibility to process the next arrival rate $\lambda_t$ using $x$ servers can simply be calculated by adding $\opcosts(x,\lambda_t)$ to the cost of $v_{x,t-1\uparrow}$. Consequently, the cost of $v_{x,t\downarrow}$ is given by the minimum of $v_{x,t-1\uparrow}+\opcosts(x,\lambda_t)$ and $v_{x+1,\downarrow}$.

As one can see in Figure~\ref{fig:graph_pseudo_lin}, we stretched our graph but at the same time also greatly reduced the number of edges compared to our initial approach in Section~\ref{sec:opt_offline_pseudo_poly}. By following the colored path of the topological sorting, we can work our way through the graph to calculate the shortest paths, ultimately reaching the destination $v_{0,T\downarrow}$. The cost of our destination $v_{0,T\downarrow}$ will denote the minimum cost up to time slot $T$ when processing $\lambda_T$ with~$0$ or more servers. Hence, it will contain our desired end result -- the minimum cost of all possible schedules.

Our next task shall be the verification of our new construction. We first examine the possible paths from $v_{0,0}$ to $v_{0,T\downarrow}$ in our graph.
In contrast to our approach in Section~\ref{sec:opt_offline_pseudo_poly}, our new graph contains paths that do not directly correspond to a schedule $\mx$. For example, consider the following path:
\begin{equation*}
	P\coloneqq(v_{0,0},v_{1,1\downarrow},v_{0,1\downarrow},v_{0,1\uparrow},v_{1,1\uparrow},v_{2,1\uparrow},v_{2,2\downarrow},\dotsc,v_{0,T\downarrow})
\end{equation*}
A schedule corresponding to $P$ would use one active server in its first time slot, then power down this server, and subsequently turn on two servers to process the next arrival rate. This seems unreasonable: We could just keep the initial server on to save switching costs (note the correspondence to Proposition~\ref{prop:reasonable_switching}). In fact, this behavior cannot even be represented using our schedule notation $\mx$ and optimization condition~\eqref{eq:mx_schedule_total_costs}. We can, however, modify $P$ to represent a more reasonable sequence by setting
\begin{equation*}
	P'\coloneqq(v_{0,0},v_{1,1\downarrow},v_{1,1\uparrow},v_{2,1\uparrow},v_{2,2\downarrow},\dotsc,v_{0,T\downarrow})
\end{equation*}
This revised path pleasantly translates to a schedule $\mx$, in this case $\mx=(1,2,\ldots)$. We now want to give a more formal definition of our observation.
\begin{defn}[Reasonable paths]\label{defn_reasn_paths}
Let $P$ be a path from $v_{0,0}$ to $v_{0,T\downarrow}$. For any time slot $t\in[T]$, let $E_\downarrow^t$ be the set of edges $(v_{x,t\downarrow},v_{x-1,t\downarrow})\in E_\downarrow$ used by $P$ at time $t$. Similarly, let~$E_\uparrow^t$ be the set of edges $(v_{x,t\uparrow},v_{x+1,t\uparrow})\in E_\uparrow$ used by $P$ at time $t\in[T-1]$.
The path $P$ is called \emph{reasonable} if for any time slot $t\in[T-1]$, the path does not shut down and power on servers simultaneously at $t$. More formally, $P$ must satisfy the formula \makebox{$\forall t\in[T-1]:F$} where $F$ is defined as
\begin{equation}
	F\coloneqq\left(E_\downarrow^t=\emptyset\right)\lor\left(E_\uparrow^t=\emptyset\right)\label{eq:reasn_path}
\end{equation}
\end{defn}
The next proposition justifies that our reasonable paths indeed deserve to be called \textit{reasonable}.
\begin{prop}\label{prop:path_to_reasn_path}
Any given path $P$ from $v_{0,0}$ to $v_{0,T\downarrow}$ can be transformed to a reasonable path $P'$ with $\costs(P')\le\costs(P)$.
\end{prop}
\begin{proof}
Let $P$ be a path from $v_{0,0}$ to $v_{0,T\downarrow}$. We give a procedure that repeatedly modifies~$P$ such that it satisfies~\eqref{eq:reasn_path} and reduces or retains its cost.

Let $t\in[T-1]$ be the first time slot that falsifies~\eqref{eq:reasn_path}. If there does not exist such a time slot, we are finished. Otherwise, let $E_\downarrow^t$ and $E_\uparrow^t$ be its sets of edges as defined in Definition~\ref{defn_reasn_paths}. Since $P$ falsifies~\eqref{eq:reasn_path} at time $t$, both $E_\downarrow^t$ and $E_\uparrow^t$ must be non-empty. Thus, we can obtain the ``maximum'' nodes involved in these sets.
\begin{align*}
	x_s\coloneqq&\max\bigl\{x\in[m]\mid (v_{x,t\downarrow},v_{x-1,t\downarrow})\in E_\downarrow^t\bigr\}\\
	x_e\coloneqq&\max\bigl\{x\in[m]\mid (v_{x-1,t\uparrow},v_{x,t\uparrow})\in E_\uparrow^t\bigr\}
\end{align*}
Next, consider the subpath $S=(v_{x_s,t\downarrow},\dotsc,v_{x_e,t\uparrow})$ of $P$. Note that the subpath in particular uses all edges from $E_\downarrow^t$ and $E_\uparrow^t$.
Evidently, a most cost-efficient path $S'$ from $v_{x_s,t\downarrow}$ to $v_{x_e,t\uparrow}$ minimizes the number of edges $(v_{x,t\uparrow},v_{x+1,t\uparrow})$ since each of these edges incurs cost $\beta\ge 0$. For the construction of $S'$, we observe that we must not shut down servers if $x_s\le x_e$, and that we must not power on servers if $x_s\ge x_e$; we thus consider three cases for $S'$:
\begin{equation*}
	S'\coloneqq
	\begin{cases}
		(v_{x_s,t\downarrow},v_{x_s,t\uparrow},v_{x_s+1,t\uparrow},\dotsc,v_{x_e,t\uparrow}), & \text{if $x_s< x_e$}\\
		(v_{x_s,t\downarrow},v_{x_s-1,t\downarrow},\dotsc,v_{x_e,t\downarrow},v_{x_e,t\uparrow}), & \text{if $x_s>x_e$}\\
		(v_{x_s,t\downarrow},v_{x_e,t\uparrow}), & \text{if $x_s=x_e$}
	\end{cases}
\end{equation*}
In each case, $S'$ uses edges from at most one of the sets $E_\downarrow^t$ and $E_\uparrow^t$. Thus, by replacing the subpath $S$ of $P$ with $S'$, we obtain a new path $P'$ that satisfies~\eqref{eq:reasn_path} up to and including time slot $t$. Moreover, the paths $P$ and $P'$ coincide in their costs before visiting $v_{x_s,t\downarrow}$ and after visiting $v_{x_e,t\uparrow}$; however, they differ in that there are less switching costs $\beta$ at time $t$ using $P'$. As we assume $\beta\ge0$, we conclude $\costs(P')\le \costs(P)$.

Hence, by repeating described process on $P'$, we obtain a terminating procedure that returns a path satisfying the conditions.
\end{proof}
As a result of Proposition~\ref{prop:path_to_reasn_path}, we shall focus our attention on reasonable paths. Our goal is to establish the connection between reasonable paths $P$ of our graph and schedules~$\mx$. Intuitively, one can see that such a path $P$ is uniquely determined by the ``maximum'' nodes $v_{x,t\downarrow}$ taken by $P$ at any time slot $t$; these nodes represent the state of processing $\lambda_t$ with $x$ servers. Consider the following example:
\begin{exmpl}
Let $\inp=\bigl(m=3,T=3,\Lambda=(3,0,1),\beta,f\bigr)$ be the input for a problem instance. Then one example of a reasonable path is given by
\begin{align*}
	P&\coloneqq(v_{0,0},v_{3,1\downarrow},v_{2,1\downarrow},v_{1,1\downarrow},v_{0,1\downarrow},v_{0,1\uparrow},v_{0,2\downarrow},v_{0,2\uparrow},v_{1,2\uparrow},v_{1,3\downarrow},v_{0,3\downarrow})
\end{align*}
A schedule corresponding to $P$ would use three active server in its first time slot, then shut down all servers for the second time slot, and ultimately power on one server to process the last arrival rate; that is, the corresponding schedule of $P$ is $\mx=(3,0,1)$. As can be seen in Figure~\ref{fig:graph_pseudo_lin_example_max_nodes}, this sequence also corresponds to the sequence of ``maximum'' nodes~$v_{x,t\downarrow}$ taken by $P$. 
Conversely, given the schedule $\mx=(3,0,1)$, it can easily be seen that there exists only one reasonable path corresponding to $\mx$, namely $P$.
\begin{figure}[H]
\includestandalone[width=\textwidth]{../figures/graph_pseudo_lin_example_max_nodes}
\caption{Illustration of the path $P$. The path is highlighted in red. The ``maximum'' nodes $v_{x,t\downarrow}$ taken by $P$ are highlighted in blue.}
\label{fig:graph_pseudo_lin_example_max_nodes}
\end{figure}
\end{exmpl}
Before we approach our next lemma, which formalizes these ideas, we need to give a precise definition of what we understand as \emph{``maximum'' nodes $v_{x,t\downarrow}$} used by a reasonable path.
\begin{defn}\label{defn:max_path_node}
Let $P$ be a reasonable path. For any $t\in[T]$, let $V_\downarrow^t$ be the set of nodes $v_{x,t\downarrow}$ visited by $P$. The ``maximum'' node $v_{x,t\downarrow}$ at each time slot $t\in[T]$ can then be identified by
\begin{equation*}
	x_t\coloneqq\max\bigl\{x\mid v_{x,t\downarrow}\in V_\downarrow^t\bigr\}
\end{equation*}
\end{defn}
Remember that a schedule $\mx=(x_1,\dotsc,x_T)$ also uses the notation $x_t$ to identify the number of active severs at time $t$. Needless to say, this clash of names is intentional and justified by the next lemma.
\begin{lem}\label{lem:sched_reasn_path_pseudo_lin}
Let $\bm{\mx}$ be the set of all schedules $\mx$ for $\inp$, and let $\bm{\mathcal{P}}$ be the set of all reasonable paths. The map
\begin{equation*}
	\Phi:\bm{\mathcal{P}}\rightarrow\bm{\mx},\quad P\mapsto (x_1,\dotsc,x_T)
\end{equation*}
is a bijection satisfying $\costs(P)=\costs\bigl(\Phi(P)\bigr)$.
\end{lem}
\begin{proof}
First, we check that $\Phi$ is bijective, i.e.\ $\Phi$ is injective and surjective. 

Let $P$ and $P'$ be reasonable paths with $\Phi(P)=\Phi(P')$. Then $P$ as well as $P'$ must start with the edge $(v_{0,0},v_{x_1,1\downarrow})$. As $P$ and $P'$ are reasonable, they both satisfy $E_\downarrow^t=\emptyset \lor E_\uparrow^t=\emptyset$ for each time slot $t$. Due to this restriction, their paths between $v_{x_t,t\downarrow}$ and $v_{x_{t+1},t+1\downarrow}$ must coincide for $t\in[T-1]$. Further, the path from $v_{x_T,T}$ to $v_{0,T}$ is unique in our graph. Thus, we conclude $P=P'$, which shows the injectivity of $\Phi$.

Next, let $\mx=(x_1,\dotsc,x_T)\in\bm{\mx}$ be a schedule for $\inp$. For any $t\in[T-1]$, we set
\begin{equation*}
	S_t\coloneqq
	\begin{cases}
		(v_{x_t,t\downarrow},v_{x_t,t\uparrow},v_{x_t+1,t\uparrow},\dotsc,v_{x_{t+1},t\uparrow}), & \text{if $x_t<x_{t+1}$}\\
		(v_{x_t,t\downarrow},v_{x_t-1,t\downarrow},\dotsc,v_{x_{t+1},t\downarrow},v_{x_{t+1},t\uparrow}), & \text{if $x_t>x_{t+1}$}\\
		(v_{x_t,t\downarrow},v_{x_{t+1},t\uparrow}), & \text{if $x_t=x_{t+1}$}
	\end{cases}
\end{equation*}
We then concatenate these subpaths to construct a reasonable path
\begin{equation*}
	P\coloneqq(v_{0,0},S_1,S_2,\dotsc,S_{T-1},v_{x_T,T\downarrow},v_{x_T-1,T\downarrow},\dotsc,v_{0,T\downarrow})
\end{equation*}
Evidently, the constructed path satisfies $\Phi(P)=\mx$, which shows the surjectivity of $\Phi$.

It remains to show that $\Phi$ is cost-preserving. For this, let $P$ be a reasonable path and let $\mx\coloneqq\Phi(P)=(x_1,\dotsc,x_T)$ be its image. 
As $P$ is reasonable, we have $E_\downarrow^t=\emptyset \lor E_\uparrow^t=\emptyset$ for any time slot $t$. If $E_\uparrow^t$ is empty, we have $x_{t-1}\ge x_t$. The cost between the nodes $v_{x_{t-1},t-1\downarrow}$ and $v_{x_t,t\downarrow}$ is then given by $\opcosts(x_t,\lambda_t)$. If $E_\uparrow^t$ is non-empty, we have $x_{t-1}<x_t$. In this case, the cost is given by $\beta(x_t-x_{t-1})+\opcosts(x_t,\lambda_t)$. Using these observations, the cost of $P$ can be calculated by 
\begin{align*}
	\costs(P)&\stackrel{\phantom{\eqref{eq:mx_schedule_sw_costs}}}{=}\costs(0,x_1,\lambda_1)+\sum\limits_{t=2}^{T}\bigl(\overbrace{\beta\max\{0,x_t-x_{t-1}\}}^{\swcosts(x_{t-1},x_t)}+\opcosts(x_{t},\lambda_t)\bigr)\\
	&\stackrel{\eqref{eq:mx_schedule_sw_costs}}{=}c(0,x_1,\lambda_1)+\sum\limits_{t=2}^{T}\bigl(\overbrace{\swcosts(x_{t-1},x_t)+\opcosts(x_{t},\lambda_t)}^{\costs(x_{t-1},x_t,\lambda_t)}\bigr)\\
	&\stackrel{\eqref{eq:mx_schedule_costs}}{=}c(0,x_1,\lambda_1)+\sum\limits_{t=2}^{T}\costs(x_{t-1},x_t,\lambda_t)=\sum\limits_{t=1}^{T}\costs(x_{t-1},x_t,\lambda_t)\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}c(\mx)
\end{align*}
which shows that $\Phi$ is a cost-preserving map.
\end{proof}
Again, we can use the established bijection to obtain our desired result: the correspondence between optimal schedules and shortest reasonable paths.
\begin{thm}\label{thm:opt_sched_reasn_path}
There exists a cost-preserving bijection between optimal schedules $\mx^*$ for~$\inp$ and shortest reasonable paths.
\end{thm} 
\begin{proof}
By Lemma~\ref{lem:sched_reasn_path_pseudo_lin}, we have a bijection $\Phi$ between reasonable paths $P$ and schedules~$\mx$ obeying $\costs(P)=\costs\bigl(\Phi(P)\bigr)$. Thus, we have 
\begin{equation*}
	\costs(P)\text{ minimal}\iff \costs\bigl(\Phi(P)\big)\text{ minimal}
\end{equation*}
and the claim follows.
\end{proof}
Naturally, common shortest path algorithms do not have any knowledge about our ``reasonable'' paths. What if we obtain a shortest path that coincidentally is not reasonable? This shall not turn out to be a problem, as the next corollary shows us.
\begin{cor}\label{cor:opt_sched_short_path_pseudo_lin}
Any shortest path $P$ from $v_{0,0}$ to $v_{0,T\downarrow}$ can be transformed to an optimal schedule $\mx^*$ for~$\inp$ with $\costs(\mx^*)=\costs(P)$.
\end{cor}
\begin{proof}
Let $P$ be a shortest path from $v_{0,0}$ to $v_{0,T\downarrow}$. By Proposition~\ref{prop:path_to_reasn_path}, the path $P$ can be transformed to a reasonable path $P'$ with $\costs(P')\le\costs(P)$.
In turn, $P'$ corresponds to an optimal schedule $\mx^*$ with $\costs(\mx^*)=\costs(P')$ by Theorem~\ref{thm:opt_sched_reasn_path}. Since $P$ is a shortest path, we know that $\costs(P)\le\costs(P')$ and thus conclude $\costs(P')=\costs(P)$. Hence, we have $c(\mx^*)=c(P)$, and the claim follows.
\end{proof}
Like in Section~\ref{sec:opt_offline_pseudo_poly}, we next give an algorithm based on our just verified construction. Although we could search for an arbitrary shortest path in our graph and transform it to an optimal schedule, as shown in Corollary~\ref{cor:opt_sched_short_path_pseudo_lin}, our algorithm only considers reasonable paths. Again, we split our procedure into two subroutines.

\textproc{shortest\_paths} calculates the minimum costs of the graph's nodes by following the graph's topological sorting. When calculating a node's minimum cost after time slot~$t$, it further keeps track of the selection that has to be made at time $t$ to obtain the node's minimum cost. It merges the costs of the nodes $v_{x,t\downarrow}$ and $v_{x,t\uparrow}$ in each time slot $t\in[T-1]$ and ultimately only keeps the relevant information for each node $v_{x,t\uparrow}$, namely the node's cost and best scheduling selection at time $t$. The information that would tell us the path between $v_{x,t\uparrow}$ and its best scheduling selection at time $t$ in our graph is lost; however, this information is of no concern since we only want to consider reasonable paths, which are already uniquely identified by their scheduling choices (see Lemma~\ref{lem:sched_reasn_path_pseudo_lin}). The restriction to reasonable paths additionally reduces the algorithm's memory demand as we can reduce the sizes of the tables $C$ and $S$ from $2mT$ to $mT$. The function returns the minimum costs to all nodes $v_{x,T\downarrow}$ and to all nodes $v_{x,t\uparrow}$ for $t\in[T-1]$ as well as the selections that have to be made to obtain the nodes' minimum costs.\unsure{Is the difference between table $S$ and $P$ from the previous alg.\ clear?}

\textproc{extract\_schedule} uses the selections calculated by \textproc{shortest\_paths} in order to obtain the sequence of nodes describing a shortest path; thereby, it calculates an optimal schedule for our problem instance.
\begin{algorithm}[H]
  \caption{Pseudo-linear-time optimal offline scheduling}
  \begin{algorithmic}[1]
  \Function{optimal\_offline\_scheduling}{$m,T,\Lambda,\beta,f$}
	  \Let{$(C,S)$}{\Call{shortest\_paths}{$m,T,\Lambda,\beta,f$}}
	  \Let{$\mx$}{\Call{extract\_schedule}{$S,T$}}
	  \State \Return{$\mx$}
  \EndFunction
  \Statex
  \Function{shortest\_paths}{$m,T,\Lambda,\beta,f$}
	\Blet{$C[0\dotso m,1\dotso T]$ and $S[0\dotso m,1\dotso T]$}{new tables}
	\Statex \Comment{Allocate cost and selection tables}
	\Let{$S[m,1]$}{$m$ and $C[m,1]\gets\costs(0,m,\lambda_1)$}\Comment{Initialize first node in first layer}
	\For{$x \gets m-1 \textrm{ to } 0$}\Comment{Initialize first layer (downward minimization step)}
		\If {$C[x+1,1]<\costs(0,x,\lambda_1)$}
			\Let{$S[x,1]$}{$S[x+1,1]$ and $C[x,1]\gets C[x+1,1]$}
		\Else
			\Let{$S[x,1]$}{$x$ and $C[x,1]\gets\costs(0,x,\lambda_1)$}
		\EndIf
	\EndFor
	\For{$t \gets 1 \textrm{ to } T-1$}\Comment{Iteratively calculate costs and selections}
		\For{$x \gets 1 \textrm{ to } m$}\Comment{Upward minimization step}
			\If {$C[x-1,t]+\beta<C[x,t]$}
			    \Let{$S[x,t]$}{$S[x-1,t]$ and $C[x,t]\gets C[x-1,t]+\beta$}
			\EndIf
		\EndFor
		\Let{$S[m,t+1]$}{$m$}\Comment{Move to next time slot and initialize first node}
		\Let{$C[m,t+1]$}{$C[m,t]+\opcosts(m,\lambda_{t+1})$}
		\For{$x \gets m-1 \textrm{ to } 0$}\Comment{Downward minimization step}
			\If {$C[x+1,t+1]<C[x,t]+\opcosts(x,\lambda_{t+1})$}
				\Let{$S[x,t+1]$}{$S[x+1,t+1]$ and $C[x,t+1]\gets C[x+1,t+1]$}
			\Else
				\Let{$S[x,t+1]$}{$x$ and $C[x,t+1]\gets C[x,t]+\opcosts(x,\lambda_{t+1})$}
			\EndIf
		\EndFor
	\EndFor
	\State \Return{$(C,S)$}
  \EndFunction
  \Statex
  \Function{extract\_schedule}{$S,T$}
	\Blet{$\mx[1\dotso T]$}{a new array}
    	\Let{$\mx[T]$}{$S[0,T]$}\Comment{Get best selection for last time slot}
        \For{$t \gets T-1 \textrm{ to } 1$}\Comment{Iteratively obtain schedule from selection table}
		\Let{$\mx[t]$}{$S\bigl[\mx[t+1],t\bigr]$}
	\EndFor
	\State \Return{$\mx$}
  \EndFunction
  \end{algorithmic}
\label{alg:opt_offline_pseudo_linear}
\end{algorithm}
The correctness of Algorithm~\ref{alg:opt_offline_pseudo_linear} directly follows from Theorem~\ref{thm:opt_sched_reasn_path} and the correctness of the shortest path calculation for directed acyclic graphs (for a proof see~\parencite[Section~24.2]{intro-algo}).

For our runtime analysis, we take the same assumptions as done for Algorithm~\ref{alg:opt_offline_pseudo_poly}. Subroutine \textproc{shortest\_paths} needs $\Theta(m)$ steps for its initialization and $\Theta(2Tm)$ steps for the iterative calculation of the selections and costs. In addition, \textproc{extract\_schedule} requires $\Theta(T)$ iterations for its schedule retrieval. Thus, Algorithm~\ref{alg:opt_offline_pseudo_linear} has a time complexity of
\begin{equation*}
	\Theta(m+2Tm+T)=\Theta(Tm)
\end{equation*}
The runtime is linear in the numeric value of $m$ and $T$; however, it is exponential in the size of the input since, as we saw in~\eqref{eq:inp_size}, we only need $\log_2(m)$ bits to encode $m$. Thus, the algorithm is pseudo-linear, which is an improvement over the pseudo-polynomial complexity $\Theta(Tm^2)$ of Algorithm~\ref{alg:opt_offline_pseudo_poly}.

Our memory demand is determined by the size of the array $\mx$ and the size of the tables $C$ and $S$. The former is of size $\Theta(T)$, and the latter are of size $\Theta(Tm)$. Thus, Algorithm~\ref{alg:opt_offline_pseudo_linear} has a memory complexity of $\Theta(T+2Tm)=\Theta(Tm)$, which shows that the memory complexity does not change in comparison to Algorithm~\ref{alg:opt_offline_pseudo_poly}.

Although we have achieved a notable improvement compared to our initial approach, we shall not stop here. Our runtime is, strictly speaking, still exponential; hence, a large value of $m$ may cause undesired long execution times. Our next goal is therefore the reduction of our runtime to sub-exponential complexity.
