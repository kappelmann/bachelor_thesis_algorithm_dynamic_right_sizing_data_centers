% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Approximative Offline Scheduling}\label{chap:approx_offline_scheduling}
Heretofore, we have derived two optimal offline algorithms for our scheduling problem. Unfortunately, the algorithm's runtime complexities are exponential in the input size of the number of servers $m$. Needless to say, we want to reduce this exponential runtime. For this, we must slightly loosen our aspirations, that is we move to approximative methods. Further, as we will see in the next section, we need to assume that our convex operating costs function $f$ is non-negative and monotonically increasing; however, this restriction is of no great significance since in practice most operating costs functions fulfill this requirement.\unsure{Any reference?} 

In this chapter, we will first modify our algorithm derived in Section~\ref{sec:opt_offline_pseudo_lin} to obtain a 2-optimal offline algorithm with linear runtime complexity. As a next step, we will generalize our first approach to allow for arbitrary $1+\beps$-approximations with TODO complexity.

\section{A 2-Optimal Algorithm}
Recall our algorithm and its corresponding graph $G$ derived in Section~\ref{sec:opt_offline_pseudo_lin}. The algorithm's runtime complexity of $\Theta(Tm)$ is determined by the number of nodes and edges of $G$. Since we desire to reduce our runtime complexity, we need to reduce the number of nodes and edges in $G$. In particular, we must get rid of the factor $m$. This factor is a consequence of the ``height'' of our graph, that is the number of nodes in each layer. Therefore, we have to ``thin out'' $G$ by reducing its number of nodes in each layer.

As we saw in Equation~\eqref{eq:inp_size}, the size of our input $\inp$ is given by $\mathcal{O}\bigl(T\log_2(m)+\log_2(\beta)\bigr)$. Consequently, in order to obtain a linear runtime complexity, we want to reduce the graph's height from $m+1$ to a logarithmic height of $\mathcal{O}\bigl(\log(m)\bigr)$. Given this observation, it seems natural for a computer scientist to choose a logarithmic scale for the number of servers in each layer; that is, instead of adding a node for each possible number of active servers (i.e.\ $0,1,\dotsc,m$), we only add nodes for logarithmic choices (i.e.\ $0,2^0,2^1,\dotsc,2^{\lfloor\log_2(m)\rfloor},m$). More formally, given a problem instance $\inp$, we set
\begin{align*}
	b&\coloneqq\lfloor\log_2(m)\rfloor\\
	B&\coloneqq\{0,2^0,2^1,\dotsc,2^b,m\}
\end{align*}
where $B$ will subsequently represent the set of possible scheduling choices at each time slot. Using this set of possible choices, we can then consider the following adaption of our former graph:
\begin{align*}
	V&\coloneqq\bigl\{v_{x,t\downarrow}\mid x\in B,t\in[T]\bigr\}\dotcup\bigl\{v_{x,t\uparrow}\mid x\in B, t\in[T-1]\bigr\}\dotcup\{v_{0,0}\}\\
	E_s&\coloneqq\bigl\{(v_{0,0},v_{x,1\downarrow})\mid x\in B\bigr\}\\
	E_\downarrow&\coloneqq\bigl\{(v_{2^i,t\downarrow},v_{2^{i-1},t\downarrow})\mid i\in[b],t\in[T]\bigr\}\dotcup\bigl\{(v_{2^0,t\downarrow},v_{0,t\downarrow})\mid t\in[T]\bigr\}\dotcup\\
	&\phantom{{}\coloneqq{}}\bigl\{(v_{m,t\downarrow},v_{2^b,t\downarrow})\mid t\in[T]\bigr\}\\
	E_\uparrow&\coloneqq\bigl\{(v_{2^{i-1},t\uparrow},v_{2^i,t\uparrow})\mid i\in[b],t\in[T-1]\bigr\}\dotcup\bigl\{(v_{0,t\uparrow},v_{2^0,t\uparrow})\mid t\in[T-1]\bigr\}\dotcup\\
	&\phantom{{}\coloneqq{}}\bigl\{(v_{2^b,t\uparrow},v_{m,t\uparrow})\mid t\in[T-1]\bigr\}\\
	E_{\downarrow\uparrow}&\coloneqq\bigl\{(v_{x,t\downarrow},v_{x,t\uparrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E_{\uparrow\downarrow}&\coloneqq\bigl\{(v_{x,t\uparrow},v_{x,t+1\downarrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E&\coloneqq E_s\dotcup E_\downarrow\dotcup E_\uparrow\dotcup E_{\downarrow\uparrow}\dotcup E_{\uparrow\downarrow}\\
	c_G(e)&\coloneqq
	\begin{cases}
		\costs(0,x,\lambda_1), & \text{if $e=(v_{0,0},v_{x,1\downarrow})\in E_s$}\\
		\opcosts(x,\lambda_{t+1}), & \text{if $e=(v_{x,t\uparrow},v_{x,t+1\downarrow})\in E_{\uparrow\downarrow}$}\\
		(x'-x)\beta, & \text{if $e=(v_{x,t\uparrow},v_{x',t\uparrow})\in E_\uparrow$}\\
		0, & \text{if $e\in(E_\downarrow\dotcup E_{\downarrow\uparrow})$}
	\end{cases}\\
	G&\coloneqq(V,E,c_G)
\end{align*}
A more appealing, graphical representation can be found in the following figure.
\begin{figure}[H]
\includestandalone[width=\textwidth]{../figures/graph_lin_approx_2}
\caption{Graph for a linear, 2-optimal offline algorithm; the path of the topological sorting is highlighted in red. Note that $\beta(2^i-2^{i-1})=\beta2^{i-1}$.}
\label{fig:graph_lin_approx_2}
\end{figure}
The nodes' and edges' semantical meaning and the graph's working principle stays similar to that given in Section~\ref{sec:opt_offline_pseudo_lin}. Again, by following the colored path of the topological sorting, we can work our way through the graph to calculate the shortest paths, ultimately reaching the destination $v_{0,T\downarrow}$. Since some possible scheduling choices, however, are not representable in this new graph, we may just obtain approximative costs for our nodes. Thus, the shortest path in our graph might not correspond to an optimal schedule, but it will at least correspond to an approximative one. Before we start to establish the graph's approximation guarantee, we first have to conduct some observations. We start by making a convenient definition that helps us to identify schedules that are representable in our graph.
\begin{defn}[Restricted schedules]
Given an input $\inp$ and a set $A\subseteq\fromto{0}{m}$, we say that a schedule $\mx=(x_1,\dotsc,x_T)$ is \emph{$A$-restricted} if $\mx$ only uses scheduling coices contained in $A$, that is $\mx$ satisfies $\forall t\in[T](x_t\in A)$.
\end{defn}
Evidently, our graph is able to represent every $B$-restricted schedule. We now examine the incurring operating costs of such a $B$-restricted schedule $\mx'$. Since we are forced to schedule a number of servers contained in $B$, we might not be able to choose an optimal scheduling choice that minimizes the schedule's operating costs. Instead, we may choose the nearest scheduling choice which is contained in $B$. For instance, if the optimal scheduling strategy at some timeslot $t$ would be to choose $x_t=3$ servers (which is not a power of two), we may instead have to choose $x_t'=4\in B$ servers for $\mx'$. One might suspect that this strategy would incur at most twice as much operating costs as an optimal schedule. This, however, is sadly not the case, as one can see in the following example.
\begin{exmpl}
Let $\inp=\bigl(m=4,T=5,\Lambda=(3,3,3,3,3),\beta=0,f\bigr)$ be the input for a problem instance where $f(\lambda)=(\lambda-1)^2$. Since we need at least 3 active servers at any timeslot, any $B$-restricted schedule $\mx'=(x_1',\dotsc,x_5')$ forces us to constantly use $x_t'=4$ active machines. An optimal schedule $\mx=(x_1,\dotsc,x_5)$, on the other hand, is able to minimize its costs by constantly scheduling $x_t=3$ servers. Let us compare the costs between $\mx'$ and $\mx$. The schedules' costs are given by
\begin{align*}
	\costs(\mx)&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_tf(\lambda_t/x_t)}^{3(3/3-1)^2}+\overbrace{\swcosts(x_{t-1},x_t)}^{0\max\{\,\cdots\}}\bigr)=5\cdot3\Bigl(\frac{3}{3}-1\Bigr)^2=0\\
	\costs(\mx')&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_t'f(\lambda_t/x_t')}^{4(3/4-1)^2}+\overbrace{\swcosts(x_{t-1}',x_t')}^{0\max\{\,\cdots\}}\bigr)=5\cdot4\Bigl(\frac{3}{4}-1\Bigr)^2=\frac{20}{16} 
\end{align*}
Albeit our approximative schedule uses only one server in addition, the schedule's cost is already inestimably higher than that of an optimal schedule $\mx$, preventing any sensible approximation estimation. Naturally, we may ask ourselves how this explosion of costs is even possible. Evidently, the switching costs are not the root of this explosion since $\beta=0$. Thus, we shall take a closer look on the used operating costs function. The optimal schedule $\mx$ evenly distributes every load $\lambda_t=3$ to $x_t=3$ servers. Hence, every active server has to process a load of $\frac{\lambda_t}{x_t}=1$ at every time step, incurring costs of $f(1)=0$. On the other hand, the approximative schedule $\mx'$ is able to distribute every load to 4 active machines. Thus, every machine incurs costs of $f(\frac{3}{4})=\frac{1}{16}$. This observation seems rather surprsing: Although every server has to process a smaller load using $\mx'$, the incurred operating costs of each server turn out to be higher. Intuitevly, however, we would expect that a less stressed machine would incur less costs. This surprising behavior is due to the fact that our operating costs function $f$ is not monotonically increasing, as one can see in the following figure.
\begin{figure}[H]
\centering
\includestandalone{../figures/non_mono_incr_f}
\caption{Example of a non-monotonically increasing operating costs function \makebox{$f(\lambda)=(\lambda-1)^2$}, where smaller loads incur higher costs.}
\label{fig:non_mono_incr_f}
\end{figure}
\end{exmpl}
The above example shows us that our graph may not able to deliver a sensible approximation when dealing with general convex operating costs functions $f$. Luckily, this inconvenience can solved by additionally assuming that $f$ is non-negative and monotonically increasing\unsure{note that in practice used functions are mostly monotone+non-neg.?}. To see this, assume that at some timeslot $t$ the scheduling choice $x_t$ minimizes the operating costs to process the load $\lambda_t$. Then let $x_t'\in B$ the next scheduling choice representable in $G$. Since $x_t$ minimizes the operating costs at timeslot $t$, we have
\begin{equation*}
	\opcosts(x_t,\lambda_t)\le\opcosts(x_t',\lambda_t)\stackrel{\eqref{eq:mx_schedule_op_costs}}{=}x_t'f\Bigl(\frac{\lambda_t}{x_t'}\Bigr)
\end{equation*}
Further, since $B$ contains all powers of two up to $m$, we have $x_t\le x_t'\le 2x_t$. If we additionally assume that $f$ is non-negative, we can infer that
\begin{equation*}
	x_t'f\Bigl(\frac{\lambda_t}{x_t'}\Bigr)\le2x_tf\Bigl(\frac{\lambda_t}{x_t'}\Bigr)
\end{equation*}
Now, using the fact that $x_t\le x_t'$ and assuming that $f$ is monotonically increasing, we have
\begin{equation*}
	2x_tf\Bigl(\frac{\lambda_t}{x_t'}\Bigr)\le 2x_tf\Bigl(\frac{\lambda_t}{x_t}\Bigr)\stackrel{\eqref{eq:mx_schedule_op_costs}}{=}2\opcosts(x_t,\lambda_t)
\end{equation*}
Ultimately, we can combine our observations and conclude
\begin{equation*}
	\opcosts(x_t,\lambda_t)\le\opcosts(x_t',\lambda_t)\le2\opcosts(x_t,\lambda_t)
\end{equation*}
which shows us that our approximative scheduling choice incurs at most twice as much operating costs as an optimal scheduling strategy. We thus subsequently restrict ourselves to non-negative, monotonically increasing convex cost functions.

Next, we examine the incurring switching costs of a $B$-restricted schedule $\mx'$. Again, given an optimal scheduling choice $x_t$, we use the idea to choose the nearest scheduling choice $x_t'$ that is contained in $B$ to construct $\mx'$. Once more, one might hope that $\mx'$ would incur at most twice as much switching costs as an optimal schedule. Needless to say, the next example dashes this hope.
\begin{exmpl}\label{exmpl:oscillating_schedule}
Let $\inp=\bigl(m=8,T=5,\Lambda=(9,7,9,7,9),\beta=1,f\bigr)$ be the input for a problem instance where $f(\lambda)=0$. Our possible scheduling choices are then given by $B=\{0,1,2,4,8\}$, and one optimal schedule is given by $\mx=(9,7,9,7,9)$. The $B$-restricted schedule corresponding to $\mx$ is then given by $\mx'=(16,8,16,8,16)$. The schedules' costs amount to
\begin{align*}
	\costs(\mx)&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_tf(\lambda_t/x_t)}^{x_t\cdot 0}+\overbrace{\swcosts(x_{t-1},x_t)}^{\max\{0,x_t-x_{t-1}\}}\bigr)=9+0+2+0+2=13\\
	\costs(\mx')&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_t'f(\lambda_t/x_t')}^{x_t'\cdot 0}+\overbrace{\swcosts(x_{t-1}',x_t')}^{\max\{0,x_t'-x_{t-1}'\}}\bigr)=16+0+8+0+8=32
\end{align*}
The approximative schedule's cost is thus not 2-competetive compared to the optimal schedule. Of course, we are again curious about how this cost explosion is possible. Since we set $f(\lambda)=0$, that is our servers do not incur operating costs, this explosion is evidently due to the increased switching costs of $\mx'$. The problem in this case is the oscillating behavior of $\mx$ around a power of two (namely $8=2^3$), as one can see in the following figure. 
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.5\textwidth}
\includestandalone[width=\textwidth]{./../figures/switching_costs_not_2_opt_1}
	\caption{\underline{Optimal schedule $\mx$:} Note how the schedule oscillates around 8 (a power of two).}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.5\textwidth}
\includestandalone[width=\textwidth]{../figures/switching_costs_not_2_opt_2}
	\caption{\underline{Approximative schedule $\mx'$:} The approximative schedule is restricted to powers of two.}
\end{subfigure}
\caption{Comparison between an optimal and an approximative schedule. The approximative schedule incurs more than twice as much switching costs.}
\label{fig:adaption-schedule}
\end{figure}
\end{exmpl}
So, is this the end of our hunt for a 2-optimal approximation? No, certainly not! Although our naive approach was to no avail, there is indeed a better $B$-restricted schedule for our example. Instead of following the optimal schedule's oscillation, we can simply use a schedule that stays put during these oscillating steps, that is we can use the schedule $\mx'=(16,16,16,16,16)$ with costs $\costs(\mx')=16$. Obviously, this seems like a rather trivial example since we set $f(\lambda)=0$, and hence we do not need to worry about the new schedule's operating costs. However, it indeed turns out that making the right choice between following the optimal schedule's oscillation and staying put will always allow us to acquire a 2-optimal approximation. This observation is a key part of the next lemma's proof. Before we dive into the lemma and its proof, we make a handy definition:
\begin{defn}
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule and $t\in[T]$. We say that $\mx$ changes its \emph{2-state} at time $t$ if $x_t$ lies between a different pair of powers of two than its predecessor, that is $x_t$ satisfies
\begin{equation*}
	x_t\notin\bigl[2^{\lfloor \log_2(x_{t-1})\rfloor},2^{\lceil \log_2(x_{t-1}+1)\rceil}\bigr] \lor\overbrace{(x_{t-1}=0\land x_t\neq 0)}^{\text{special case for }x_{t-1}=0}
\end{equation*}
\end{defn}
Now we are geared up to deal with the main work of this section.
\begin{lem}\label{lem:transform_schedule_approx_2}
Let $\mx$ be a schedule for $\inp$. There exists a $B$-restricted schedule $\mx'$ satisfying \makebox{$\costs(\mx')\le2\costs(\mx)$}.
\end{lem}
\begin{proof}
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule for $\inp$. We need to construct a $B$-restricted schedule $\mx'=(x_1',\dotsc,x_T')$ such that \makebox{$\costs(\mx')\le2\costs(\mx)$}.
First, if $\mx$ is not feasible, we have $\costs(\mx)=\infty$, and thus any arbitrary $B$-restricted schedule $\mx'$ satisfies $\costs(\mx')\le2\costs(\mx)$; hence, assume that $\mx$ is feasible. We note that if $\mx$ powers down all its servers at some timeslot $t\in[T]$ (i.e.\ $x_t=0$), we can split $\mx$ into two subschedules \makebox{$\mx_1\coloneqq(x_1,\dotsc,x_{t-1})$} and \makebox{$\mx_2\coloneqq(x_{t+1},\dotsc,x_T)$}. It then suffices to prove the claim for $\mx_1$ and $\mx_2$, as we can then construct the 2-competitive schedule by setting $\mx'\coloneqq(\mx_1',x_t'\coloneqq0,\mx_2')$. Thus, by recursively applying this method, we can reduce our proof to a list of subschedules $\mx_1,\dotsc,\mx_N$ that never power down all servers. Consequently, without loss of generality, we subsequently assume that $\mx$ never powers down all its servers, that is $x_t>0$ for all $t\in[T]$. 
	
Next, we show that every period between two 2-state changes of $\mx$ can be iteratively transformed to a 2-competitive period in $\mx'$. For this, let $i\in[T]$ and $j+1\in\fromto{2}{T+1}$ with $i<j+1$ be the first unprocessed timeslots at which $\mx$ changes its 2-state. To conveniently refer to the schedules' periods between $i$ and $j$, we define the subschedules $\mx_{i,j}\coloneqq(x_i,\dotsc,x_j)$ and $\mx'_{i,j}\coloneqq(x_i',\dotsc,x_j')$. In the following steps, we need to refer to the lower and upper bound of $\mx_{i,j}$, namely
\begin{flalign*}
	&&l\coloneqq 2^{\lfloor\log_2(x_i)\rfloor}&&\text{and}&&u\coloneqq\min\bigl\{2^{\lceil\log_2(x_i+1)\rceil},m\bigr\}&&&
\end{flalign*}
as well as to the lower and upper bound of $\mx$ at time $j+1$:
\begin{flalign*}
	&&l'\coloneqq\begin{cases}
		2^{\lfloor\log_2(x_{j+1})\rfloor}, & \text{if $x_{j+1}\neq 0$}\\
		0, & \text{if $x_{j+1}=0$}
	\end{cases}
&&\text{and}&&u'\coloneqq\min\bigl\{2^{\lceil\log_2(x_{j+1}+1)\rceil},m\bigr\}&&&
\end{flalign*}
Note that for any $x\in\mathbb{N}$, the inequality $\min\bigl\{2^{\lceil\log_2(x+1)\rceil},m\bigr\}\le 2x$ holds. As an invariant of the following transformations, we are going to ensure that $\mx'$ can potentially move to $u$ at time $i$ in a 2-competitive manner, that is we ensure that we can set $x_i'\coloneqq u'$ with 2-competitive switching costs---whether this step will be taken in the end or not. For the initial start-up process (i.e.\ $i=1$), we can simply move to the next power of 2 larger than $x_1$ contained in $B$, that is we set $x_1'\coloneqq u\in B$. Since $x_1\in\mathbb{N}$, we know that $x_1'\le2x_1$ and thus $\beta x_1'\le\beta 2x_1$; in other words, the start-up switching costs of $\mx'$ are 2-competitive and the invariant initially holds. Note that we can (and indeed will have to) use the difference of switching costs $\beta(2x_1-x_1')=\beta(2x_1-u)$ as a switching cost credit to compensate for subsequent, more expensive switching steps.

Now, let us have a closer look on the possible behaviors of $\mx$ between its 2-state changes. We are going to classify the behaviors based on how $\mx$ enters and leaves the interval $[l,u)$. First, we consider the cases where $\mx$ leaves the interval $[l,u)$ by turning off servers, as depicted in Figure~\ref{fig:schedule_behavior_down}.
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_down}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_up_down}
\end{subfigure}
\caption{The original schedule comes from above or below, stays between $[l,u)$, and then descends to $[l',u')$. The approximative schedule stays put at $u$ for timeslots $i\le t\le j$.}
\label{fig:schedule_behavior_down}
\end{figure}
Due to our invariant, we know that we can set $x_i'\coloneqq u$ with 2-competitive switching costs. Consequently, setting $x_i'\coloneqq x_{i+1}\coloneqq\dotsb\coloneqq x_j'\coloneqq u\in B$ gives us a strategy with 2-competitive switching costs between $i$ and $j$. Further, since $x_t\le u\le2x_t$ for any $i\le t\le j$, and $f$ is non-negative and monotonically increasing, the operating costs of $\mx'_{i,j}$ can be estimated by
\begin{equation*}
	\opcosts(\mx_{i,j}')\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\le\sum\limits_{t=i}^j\Bigl(2x_tf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\le2\sum\limits_{t=i}^j\Bigl(x_tf\Bigl(\frac{\lambda_t}{x_t}\Bigr)\Bigr)\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}2\opcosts(\mx_{i,j})
\end{equation*}
Thus, the operating costs of $\mx_{i,j}'$ are 2-competitive. Further, since $u'<u$, satisfying our invariant at time $j+1$ does not incur additional switching costs; however, one shall note that we do not yet know whether this step to $u'$ should indeed be taken (c.f. Figure~\ref{fig:schedule_behavior_down_up}). 
	
Next, we consider the case where $\mx$ rises to $[l,u)$ and then further ascends to $[l',u')$, as illustrated in Figure~\ref{fig:schedule_behavior_up_up}.
\begin{figure}[H]
\centering
\includestandalone[width=0.5\textwidth]{../figures/schedule_behavior_up_up}	
\caption{The original schedule comes from below, stays between $[l,u)$, and then rises to $[l',u')$. The approximative schedule stays put at $u$ for timeslots $i\le t\le j$ and rises to $u'$ at time $j+1$.}
\label{fig:schedule_behavior_up_up}
\end{figure}
In this case, we set $x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq u\in B$ and $x_{j+1}'\coloneqq u'\in B$. Evidently, as in the previous case, the operating costs of $\mx_{i,j}'$ are 2-competitive. Further, due to our invariant, we can again safely move to $u$ with 2-competitve switching costs. Lastly, we need to ensure that our invariant at time $j+1$ holds. Since $\mx$ has to power on at least $x_{j+1}-x_i$ servers between $i$ and $j+1$, it suffices to show that $\beta(u'-u)\le2\beta(x_{j+1}-x_i)$ holds; however, it is clear that this must not be the case (just take $u'=8,u=4,x_{j+1}=4,x_i=3$). Nevertheless, using an amortized analysis, that is using our switching cost credit, we can see that
\begin{align*}
	2\beta(x_{j+1}-x_i)+\overbrace{\beta(2x_i-u)}^{\text{credit}}=\beta(2x_{j+1}-u)\ge\beta(u'-u)
\end{align*}
where the last inequality follows from $u'\le2x_{j+1}$. Thus, our invariant at time $j+1$ is satisfied. Note again that the difference $\beta(2x_{j+1}-u)-\beta(u'-u)=\beta(2x_{j+1}-u')$ can be used as a switching cost credit for subsequent operations. 
	
Finally, we have to consider the case where $\mx$ descends to $[l,u)$ and then ascends to $[l',u')$. As we have seen in Example~\ref{exmpl:oscillating_schedule}, this case turns out to be slightly more complicated, since simply following a possible oscillating behavior of $\mx$ can lead to a cost explosion for $\mx'$. However, we can solve this issue by considering two different strategies for $\mx_{i,j}'$, namely
\begin{align*}
	\mx_{i,j}^u&\coloneqq(x_i^u\coloneqq u,\dotsc,x_j^u\coloneqq u)\\
	\mx_{i,j}^{\hat{u}}&\coloneqq(x_i^{\hat{u}}\coloneqq\hat{u},\dotsc,x_j^{\hat{u}}\coloneqq\hat{u})
\end{align*}
where $\hat{u}\coloneqq\min\{2^{\lceil\log_2(u+1)\rceil},m\}\in B$. Both strategies are illustrated in Figure~\ref{fig:schedule_behavior_down_up}.
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_1}
\caption{Strategy $\mx_{i,j}^u$ stays put at $u$ for $i\le t\le j$ and then rises to $u'$.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_2}
\caption{Strategy $\mx_{i,j}^{\hat{u}}$ stays put at $\hat{u}$ for $i\le t\le j$ and then rises to $u'$.}
\end{subfigure}
\caption{The original schedule comes from above, stays between $[l,u)$, and then rises to $[l',u')$. The approximative schedule has two different possibilities.}
\label{fig:schedule_behavior_down_up}
\end{figure}
We are now going to prove that either $\mx_{i,j}^u$ or $\mx_{i,j}^{\hat{u}}$, including possible switching costs to satisfy our invariant for $j+1$, must be 2-competitive. 
First, we examine the switching costs of $\mx$ from $i$ up to $j+1$. To do so, we set $d\coloneqq\min\{x_t\mid i\le t\le j\}$ to refer to the smallest number of active servers used by $\mx_{i,j}$. Then, since $\mx$ has to power on at least $x_{j+1}-d$ servers between $i$ and $j+1$, we can give a lower bound for its switching costs:
\begin{equation*}
	\sum\limits_{t=i+1}^{j+1}\swcosts(x_{t-1},x_t)\ge\beta(x_{j+1}-d)
\end{equation*}
Now we can see that if $\mx_{i,j}$ at some point powers down to $l$, that is $d=l$, the switching costs of $\mx_{i,j}^u$ must be 2-competitive since
\begin{equation*}
	\beta(u'-u)\le\beta(2x_{j+1}-u)\le\beta(2_x{j+1}-2d)=2\beta(x_{j+1}-d)
\end{equation*}
Further, as in the previous cases, the operating costs of $\mx_{i,j}^u$ are 2-competitive, and thus $\mx_{i,j}^u$ is 2-competitive if $d=l$. Hence, we subsequently assume that $d>l$. 

TODO reduce to proof of switching up to $\hat{u}$. and $\hat{u}-u-2d>0$

If $\mx_{i,j}^u$ is 2-competitive, we are done; hence, assume that this is not the case, that is we have 
\begin{equation*}
	\opcosts(\mx_{i,j}^u)+\beta(u'-u)>2\bigl(\opcosts(\mx_{i,j})+\beta d\bigr)
\end{equation*}
We then need to show that $\mx_{i,j}^{\hat{u}}$ is 2-competitive.
First note that $\mx_{i,j}^u$ uses at most twice as many active servers as $\mx_{i,j}$, and thus $\mx_{i,j}^u$ incurs at most twice as much operating costs as $\mx_{i,j}$. Consequently, the switching costs of $\mx_{i,j}^u$ must significantly outweight those of $\mx_{i,j}$. Hence, it seems interesting to get an estimation for $\beta$. Rearranging the previous inequality gives us
\begin{align*}
	&&\opcosts(\mx_{i,j}^u)+\beta(\hat{u}-u)&>2\bigl(\opcosts(\mx_{i,j})+\beta d\bigr)&\\
	&\stackrel{\phantom{\hat{u}-u-2d>0}}{\iff}&\beta(\hat{u}-u-2d)&>2\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^u)&\\
	&\stackrel{\hat{u}-u-2d>0}{\iff}&\beta&>\frac{2\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^u)}{\hat{u}-u-2d}
\end{align*}
This allows us to find a lower bound for the costs of using $\mx_{i,j}$:
\begin{align*}
	\opcosts(\mx_{i,j})+\beta d&>\opcosts(\mx_{i,j})+\frac{2\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^u)}{\hat{u}-u-2d}d\\
	&=\frac{\opcosts(\mx_{i,j})(\hat{u}-u-2d)+2d\opcosts(\mx_{i,j})-d\opcosts(\mx_{i,j}^u)}{\hat{u}-u-2d}\\
	&=\frac{(\hat{u}-u)\opcosts(\mx_{i,j})-d\opcosts(\mx_{i,j}^u)}{\hat{u}-u-2d}
\end{align*}
Next, since we want to prove that $\opcosts(\mx_{i,j}^{\hat{u}})$ is 2-competitive, we have to show that the following cost difference is non-negative:
\begin{align*}
	2\bigl(\opcosts(\mx_{i,j})+\beta d\bigr)-\opcosts(\mx_{i,j}^{\hat{u}})
\end{align*}
We can use our just derived lower bound for $\opcosts(\mx_{i,j})+\beta d$ to estimate the difference:
\begin{align*}
	2\bigl(\opcosts(\mx_{i,j})+\beta d\bigr)-\opcosts(\mx_{i,j}^{\hat{u}})&>2\Bigl(\frac{(\hat{u}-u)\opcosts(\mx_{i,j})-d\opcosts(\mx_{i,j}^u)}{\hat{u}-u-2d}\Bigr)-\opcosts(\mx_{i,j}^{\hat{u}})\\
	&=\frac{2(\hat{u}-u)\opcosts(\mx_{i,j})-2d\opcosts(\mx_{i,j}^u)-(\hat{u}-u-2d)\opcosts(\mx_{i,j}^{\hat{u}})}{\hat{u}-u-2d}
\end{align*}
Thus, it suffices to show that
\begin{align*}
	&&\frac{2(\hat{u}-u)\opcosts(\mx_{i,j})-2d\opcosts(\mx_{i,j}^u)-(\hat{u}-u-2d)\opcosts(\mx_{i,j}^{\hat{u}})}{\hat{u}-u-2d}
&\ge 0\\
	&\stackrel{\hat{u}-u-2d>0}{\iff}&2(\hat{u}-u)\opcosts(\mx_{i,j})-2d\opcosts(\mx_{i,j}^u)-(\hat{u}-u-2d)\opcosts(\mx_{i,j}^{\hat{u}})&\ge 0
\end{align*}
For this, we have to take a closer look on the the schedules' operating costs:
\begin{align*}
	&&&2(\hat{u}-u)\opcosts(\mx_{i,j})-2d\opcosts(\mx_{i,j}^u)-(\hat{u}-u-2d)\opcosts(\mx_{i,j}^{\hat{u}})\\
	&\stackrel{\eqref{eq:mx_schedule_op_costs}}{=}&&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl(x_tf\Bigl(\frac{\lambda_t}{x_t}\Bigr)\Bigr)-2d\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-u-2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{\hat{u}}\Bigr)\Bigr)
\end{align*}
First, we note that $u<\hat{u}$ and $x_t<u$ for $i\le t\le j$. Together with the fact that $f$ is monotonically increasing, we infer that
\begin{align*}
	&&&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl(x_tf\Bigl(\frac{\lambda_t}{x_t}\Bigr)\Bigr)-2d\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-u-2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{\hat{u}}\Bigr)\Bigr)\\
	&\ge&&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl(x_tf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-2d\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-u-2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)
\end{align*}
Next, TODO f pos.\ and no oscillating
\begin{align*}
	&&&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl(x_tf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-2d\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-u-2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\\
	&\ge&&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl((u-d)f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-2d\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-u-2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)
\end{align*}
Hence, to finish the case, it suffices to show that
\begin{align*}
	2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl((u-d)f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-2d\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-u-2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\ge 0
\end{align*}
Further rearranging the left hand side gives us
\begin{align*}
	&&&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl((u-d)f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-2d\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-u-2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\\
	&=&&\sum\limits_{t=i}^j\Bigl((2u\hat{u}-2d\hat{u}-2u^2+2du)f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-\sum\limits_{t=i}^j\Bigl(duf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-\sum\limits_{t=i}^j\Bigl((\hat{u}^2-u\hat{u}-2d\hat{u})f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\\
	&=&&\sum\limits_{t=i}^j\Bigl((2u\hat{u}-2d\hat{u}-2u^2+2du-2du-\hat{u}^2+u\hat{u}+2d\hat{u})f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\\
	&=&&\sum\limits_{t=i}^j\Bigl((-\hat{u}^2+3u\hat{u}-2u^2)f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)=\sum\limits_{t=i}^j\Bigl(\underbrace{-(u-\hat{u})(2u-\hat{u})}_{g(\hat{u})\coloneqq}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr) 
\end{align*}
Now, since $f$ is non-negative, we just have to verify that $g(\hat{u})\ge0$ for $u\le \hat{u}\le 2u$. We note that $g(\hat{u})$ is an inverted parabola with roots $u$ and $2u$. Thus, we know that $g(u)=g(2u)=0$ and $g(\hat{u})>0$ for $l<\hat{u}<2l$, which finishes the case.

By iteratively applying above procedure to $\mx$, starting with $i=1$ and stopping with $j=T+1$, we obtain a $B$-restricted schedule $\mx'$ that satisfies $\costs(\mx')\le2\costs(\mx)$.
\end{proof}
Safe in the knowledge that there always exists a 2-optimal, $B$-restricted schedule, we are just left with the verification of our new graph. We want to establish a bijection between $B$-restricted schedules and reasonable paths. Since we did not change the general construction idea of our graph, this verification turns out to be very similar to that done in Section~\ref{sec:opt_offline_pseudo_lin}. For the proof of the next lemma, we need to recall our notion of ``maximum'' nodes $x_t$ in reasonable paths $P$, as described in Definition~\ref{defn:max_path_node}, as well as that $B=\{0,2^0,2^1,\dotsc,2^b,m\}$.
\begin{lem}\label{lem:sched_reasn_path_approx_2}
Let $\bm{\mx}$ be the set of all $B$-restricted schedules for $\inp$, and let $\bm{\mathcal{P}}$ be the set of all reasonable paths. The map
\begin{equation*}
	\Phi:\bm{\mathcal{P}}\rightarrow\bm{\mx},\quad P\mapsto (x_1,\dotsc,x_T)
\end{equation*}
is a bijection satisfying $\costs(P)=\costs\bigl(\Phi(P)\bigr)$.
\end{lem}
\begin{proof}
The proof is analogous to the proof of Lemma~\ref{lem:sched_reasn_path_pseudo_lin} considering that we conduct logarithmic instead of incremental switching steps.\unsure{Is this okay?}
\end{proof}
Finally, we arrive at the main result of this section.
\begin{thm}\label{thm:approx_2}
Any shortest, reasonable path $P$ corresponds to a 2-optimal, $B$-restricted schedule $\mx$ for $\inp$ with $\costs(P)=\costs(\mx)$.
\end{thm} 
\begin{proof}
By Lemma~\ref{lem:sched_reasn_path_approx_2}, we have a bijection $\Phi$ between reasonable paths $P$ and $B$-restricted schedules obeying $\costs(P)=\costs\bigl(\Phi(P)\bigr)$. Thus, we have 
\begin{equation*}
	\costs(P)\text{ minimal}\iff \costs\bigl(\Phi(P)\big)\text{ minimal}
\end{equation*}
Now let $P$ be a shortest, reasonable path, and let $\mx^*$ be an optimal schedule for $\inp$. We have to verify that $\costs\bigl(\Phi(P)\bigr)\le 2\costs(\mx^*)$. By Lemma~\ref{lem:transform_schedule_approx_2}, we know that there exists a $B$-restricted schedule $\mx'$ such that $\costs(\mx')\le 2\costs(\mx^*)$. Since $\Phi$ is a bijection, and $P$ is a shortest, reasonable path, we know that $\costs\bigl(\Phi(P)\bigr)\le\costs(\mx')$. Thus, we conclude that 
\begin{equation*}
	\costs(P)=\costs\bigl(\Phi(P)\bigr)\le\costs(\mx')\le 2\costs(\mx^*)
\end{equation*}
and the claim follows.
\end{proof}
Again, it is not difficult to show that also shortest paths which are not reasonable can be transformed to a desired 2-optimal schedule.
\begin{cor}\label{cor:opt_sched_short_path_pseudo_lin}
Any shortest path $P$ from $v_{0,0}$ to $v_{0,T\downarrow}$ can be transformed to a 2-optimal, $B$-restricted schedule $\mx$ for $\inp$ with $\costs(\mx)=\costs(P)$.
\end{cor}
\begin{proof}
Let $P$ be a shortest path from $v_{0,0}$ to $v_{0,T\downarrow}$. By using a small adaption of Proposition~\ref{prop:path_to_reasn_path} that uses logarithmic instead of incremental switching steps\unsure{Is this okay?}, we can transform $P$ to a reasonable path $P'$ with $\costs(P')=\costs(P)$.
In turn, $P'$ corresponds to a 2-optimal, \makebox{$B$-restricted} schedule $\mx$ with $\costs(\mx)=\costs(P')=\costs(P)$ by Theorem~\ref{thm:approx_2}, which finishes the proof.
\end{proof}


\section{A $1+\beps$-Optimal Algorithm}
