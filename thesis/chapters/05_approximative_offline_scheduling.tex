% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Approximative Offline Scheduling}\label{chap:approx_offline_scheduling}
Heretofore, we have derived two optimal offline algorithms for our scheduling problem. Unfortunately, the algorithms' time complexities are exponential in the input size of the number of servers $m$. Needless to say, we want to reduce this exponential runtime. For this, we must slightly loosen our aspirations, that is we move to approximative methods. Further, we will need to assume that our convex operating cost function $f$ is non-negative and monotonically increasing; however, this restriction is of no great significance in practice, as will be discussed in the next section.
To obtain an approximative schedule, we will first modify our algorithm derived in Section~\ref{sec:opt_offline_pseudo_lin} to obtain a 2-approximative offline algorithm with linear time complexity. As a next step, we will generalize our first approach to allow for arbitrary $(1+\beps)$-approximations with TODO time complexity.

\section{A 2-Approximative Linear-Time Algorithm}
Recall our algorithm and its corresponding graph $G$ derived in Section~\ref{sec:opt_offline_pseudo_lin}. The algorithm's time complexity of $\Theta(Tm)$ is determined by the number of nodes and edges of $G$. Since we desire to reduce our runtime complexity, we need to reduce the number of nodes and edges in $G$. In particular, we must get rid of the factor $m$. This factor is a consequence of the ``height'' of our graph, i.e.\ the number of nodes in each layer. Therefore, we have to ``thin out'' $G$ by reducing its number of nodes in each layer.

As we saw in Equation~\eqref{eq:inp_size}, the size of our input $\inp$ is given by $\mathcal{O}\bigl(T\log_2(m)+\log_2(\beta)\bigr)$. Consequently, in order to obtain a linear time complexity, we want to reduce the graph's height from $m+1$ to a logarithmic height of $\mathcal{O}\bigl(\log(m)\bigr)$. Given this observation, it seems natural for a computer scientist to choose a logarithmic scale for the number of servers in each layer, to wit, instead of adding a node for each possible number of active servers (i.e.\ $0,1,\dotsc,m$), we only add nodes for logarithmic choices (i.e.\ $0,2^0,2^1,\dotsc,2^{\floor{\log_2(m)}},m$). More formally, given a problem instance $\inp$, we set
\begin{align*}
	b&\coloneqq\floor{\log_2(m)}\\
	B&\coloneqq\{0,2^0,2^1,\dotsc,2^b,m\}
\end{align*}
where $B$ will subsequently represent the set of possible scheduling choices at each time slot. Using this set of possible choices, we can then consider the following adaption of our former graph:
\begin{align*}
	V&\coloneqq\bigl\{v_{x,t\downarrow}\mid x\in B,t\in[T]\bigr\}\dotcup\bigl\{v_{x,t\uparrow}\mid x\in B, t\in[T-1]\bigr\}\dotcup\{v_{0,0}\}\\
	E_s&\coloneqq\bigl\{(v_{0,0},v_{x,1\downarrow})\mid x\in B\bigr\}\\
	E_\downarrow&\coloneqq\bigl\{(v_{2^i,t\downarrow},v_{2^{i-1},t\downarrow})\mid i\in[b],t\in[T]\bigr\}\dotcup\bigl\{(v_{2^0,t\downarrow},v_{0,t\downarrow})\mid t\in[T]\bigr\}\dotcup\\
	&\phantom{{}\coloneqq{}}\bigl\{(v_{m,t\downarrow},v_{2^b,t\downarrow})\mid t\in[T]\bigr\}\\
	E_\uparrow&\coloneqq\bigl\{(v_{2^{i-1},t\uparrow},v_{2^i,t\uparrow})\mid i\in[b],t\in[T-1]\bigr\}\dotcup\bigl\{(v_{0,t\uparrow},v_{2^0,t\uparrow})\mid t\in[T-1]\bigr\}\dotcup\\
	&\phantom{{}\coloneqq{}}\bigl\{(v_{2^b,t\uparrow},v_{m,t\uparrow})\mid t\in[T-1]\bigr\}\\
	E_{\downarrow\uparrow}&\coloneqq\bigl\{(v_{x,t\downarrow},v_{x,t\uparrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E_{\uparrow\downarrow}&\coloneqq\bigl\{(v_{x,t\uparrow},v_{x,t+1\downarrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E&\coloneqq E_s\dotcup E_\downarrow\dotcup E_\uparrow\dotcup E_{\downarrow\uparrow}\dotcup E_{\uparrow\downarrow}\\
	c_G(e)&\coloneqq
	\begin{cases}
		\costs(0,x,\lambda_1), & \text{if $e=(v_{0,0},v_{x,1\downarrow})\in E_s$}\\
		\opcosts(x,\lambda_{t+1}), & \text{if $e=(v_{x,t\uparrow},v_{x,t+1\downarrow})\in E_{\uparrow\downarrow}$}\\
		(x'-x)\beta, & \text{if $e=(v_{x,t\uparrow},v_{x',t\uparrow})\in E_\uparrow$}\\
		0, & \text{if $e\in(E_\downarrow\dotcup E_{\downarrow\uparrow})$}
	\end{cases}\\
	G&\coloneqq(V,E,c_G)
\end{align*}
If $m$ is a power of two, i.e.\ $m=2^b$, it happens that we add unnecessary loops in~$E_\downarrow$ and~$E_\uparrow$, which we can simply ignore for our following works.
A graphical representation of $G$ can be found in the following figure.
\begin{figure}[H]
\includestandalone[width=\textwidth]{../figures/graph_lin_approx_2}
\caption{Graph for a 2-approximative linear-time offline algorithm; the path of the topological sorting is highlighted in red. Note that $\beta(2^i-2^{i-1})=\beta2^{i-1}$.}
\label{fig:graph_lin_approx_2}
\end{figure}
The nodes' and edges' semantical meaning and the graph's working principle stays similar to that given in Section~\ref{sec:opt_offline_pseudo_lin}. Again, by following the colored path of the topological sorting, we can work our way through the graph to calculate the shortest paths, ultimately reaching the destination $v_{0,T\downarrow}$. However, since some possible scheduling choices are not representable in this new graph, we may just obtain approximative costs for our nodes. Thus, the shortest path in our graph might not correspond to an optimal schedule, but it will at least correspond to an approximative one. Before we start to establish the graph's approximation guarantee, we first have to conduct some observations. We start by making a convenient definition that helps us to identify schedules that are representable in our graph.
\begin{defn}[Restricted schedules]
Given an input $\inp$ and a set $A\subseteq[m]_0$, we say that a schedule $\mx=(x_1,\dotsc,x_T)$ is \emph{$A$-restricted} if $\mx$ only uses scheduling choices contained in~$A$, that is $\mx$ satisfies the formula $\forall t\in[T]:x_t\in A$.
\end{defn}
Evidently, our graph is able to represent every $B$-restricted schedule. We now examine the incurring operating costs of such a $B$-restricted schedule $\mx'$. Since we are forced to schedule a number of servers contained in $B$, we might not be able to choose an optimal scheduling choice that minimizes the schedule's operating costs. Instead, we may choose the nearest scheduling choice which is contained in $B$. For instance, if the optimal scheduling strategy at some timeslot $t$ would be to choose $x_t=3$ servers (which is not a power of two), we may instead have to choose $x_t'=4\in B$ servers for $\mx'$. One might suspect that this strategy would incur at most twice as much operating costs as an optimal schedule. This, however, is sadly not the case, as one can see in the following example.
\begin{exmpl}
Let $\inp=\bigl(m=4,T=5,\Lambda=(3,3,3,3,3),\beta=0,f\bigr)$ be the input for a problem instance where $f(\lambda)=(\lambda-1)^2$. Since we need at least 3 active servers at any timeslot, any $B$-restricted schedule $\mx'=(x_1',\dotsc,x_5')$ forces us to constantly use $x_t'=4$ active machines. An optimal schedule $\mx=(x_1,\dotsc,x_5)$, on the other hand, is able to minimize its cost by constantly scheduling $x_t=3$ servers. Let us compare the costs between $\mx'$ and $\mx$. The schedules' costs are given by
\begin{align*}
	\costs(\mx)&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_tf(\lambda_t/x_t)}^{3(3/3-1)^2}+\overbrace{\swcosts(x_{t-1},x_t)}^{0\max\{\,\cdots\}}\bigr)=5\cdot3\left(\frac{3}{3}-1\right)^2=0\\
	\costs(\mx')&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_t'f(\lambda_t/x_t')}^{4(3/4-1)^2}+\overbrace{\swcosts(x_{t-1}',x_t')}^{0\max\{\,\cdots\}}\bigr)=5\cdot4\left(\frac{3}{4}-1\right)^2=\frac{20}{16} 
\end{align*}
Albeit our approximative schedule uses only one server in addition, the schedule's cost is already inestimably higher than that of an optimal schedule $\mx$, preventing any sensible approximation estimation. Naturally, we may ask ourselves how this explosion of costs is even possible. Evidently, the switching costs are not the root of this explosion since $\beta=0$. Thus, we shall take a closer look on the used operating cost function. The optimal schedule $\mx$ evenly distributes every load $\lambda_t=3$ to $x_t=3$ servers. Hence, every active server has to process a load of $\frac{\lambda_t}{x_t}=1$ at every time step, incurring costs of $f(1)=0$. On the other hand, the approximative schedule $\mx'$ is able to distribute every load to 4 active machines. Thus, every machine incurs costs of $f(3/4)=\frac{1}{16}$. This observation seems rather surprising: Although every server has to process a smaller load using $\mx'$, the incurring operating costs of each server turn out to be higher. Intuitively, however, we would expect that a less stressed machine would incur less costs. This surprising behavior is due to the fact that our operating cost function $f$ is not monotonically increasing, as one can see in the following figure.
\begin{figure}[H]
\centering
\includestandalone{../figures/non_mono_incr_f_1}
\caption{Example of a non monotonically increasing operating cost function \makebox{$f(\lambda)=(\lambda-1)^2$}, where smaller loads incur higher costs.}
\label{fig:non_mono_incr_f}
\end{figure}
\end{exmpl}
The above example shows us that our graph may not able to deliver a sensible approximation when dealing with general convex operating cost functions $f$. Luckily, this inconvenience can be solved by additionally assuming that $f$ is non-negative and monotonically increasing. To see this, assume that at some timeslot $t$ the scheduling choice $x_t$ minimizes the operating costs to process the load $\lambda_t$. Then let $x_t'\in B$ the next scheduling choice representable in~$G$. Since $x_t$ minimizes the operating costs at timeslot $t$, we have
\begin{equation*}
	\opcosts(x_t,\lambda_t)\le\opcosts(x_t',\lambda_t)\stackrel{\eqref{eq:mx_schedule_op_costs}}{=}x_t'f(\lambda_t/x_t')
\end{equation*}
Further, since $B$ contains all powers of two up to $m$, we have $x_t\le x_t'\le 2x_t$. If we additionally assume that $f$ is non-negative, we can infer that
\begin{equation*}
	x_t'f(\lambda_t/x_t')\le2x_tf(\lambda_t/x_t')
\end{equation*}
Now, using the fact that $x_t\le x_t'$ and assuming that $f$ is monotonically increasing, we can see that
\begin{equation*}
	2x_tf(\lambda_t/x_t')\le 2x_tf(\lambda_t/x_t)\stackrel{\eqref{eq:mx_schedule_op_costs}}{=}2\opcosts(x_t,\lambda_t)
\end{equation*}
Ultimately, we can combine our observations and conclude
\begin{equation*}
	\opcosts(x_t,\lambda_t)\le\opcosts(x_t',\lambda_t)\le2\opcosts(x_t,\lambda_t)
\end{equation*}
which shows us that our approximative scheduling choice incurs at most twice as much operating costs as an optimal scheduling strategy. We thus subsequently restrict ourselves to non-negative, monotonically increasing convex cost functions. This evidently reduces the theoretical generality of our initial approach, but it does not interfere with practical applicability. On the one hand, negative cost functions would semantically allow to ``generate profit by consuming energy'', which seems unreasonable in practice. On the other hand, if we have a non monotonically increasing convex cost function, we can simply add artifical loads to our machines to reduce our costs in given circumstances. For instance, in our previous example, we could assign every machine a load of $1$ instead of $\frac{3}{4}$ to reduce the approximative schedule's cost. This trick, which is exemplarily outlined in Figure~\ref{fig:transform_to_mono_incr}, allows us to transform any arbitrary convex cost function to a monotonically increasing one.
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.5\textwidth}
\includestandalone[width=\textwidth]{./../figures/non_mono_incr_f_2}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.5\textwidth}
\includestandalone[width=\textwidth]{../figures/mono_incr_f}
\end{subfigure}
\caption{The non monotonically increasing convex function $f$ can be transformed to the monotonically increasing convex function $f'$ by adding artifical loads $\lambda^+$ to assignments $\lambda\in[0,0.5)$ such that we obtain a new assignment $\lambda'\coloneqq\lambda+\lambda^+=0.5$.}
\label{fig:transform_to_mono_incr}
\end{figure}
Next, we examine the incurring switching costs of a $B$-restricted schedule $\mx'$. Again, given an optimal scheduling choice $x_t$, we use the idea to choose the nearest scheduling choice $x_t'$ that is contained in $B$ to construct $\mx'$. Once more, one might hope that $\mx'$ would incur at most twice as much switching costs as an optimal schedule. Needless to say, the next example dashes this hope.
\begin{exmpl}\label{exmpl:oscillating_schedule}
Let $\inp=\bigl(m=16,T=5,\Lambda=(9,7,9,7,9),\beta=1,f\bigr)$ be the input for a problem instance where $f(\lambda)=0$. Our possible scheduling choices are then given by $B=\{0,1,2,4,8,16\}$, and one optimal schedule is given by $\mx=(9,7,9,7,9)$. The $B$-restricted schedule corresponding to $\mx$ is then given by $\mx'=(16,8,16,8,16)$. The schedules' costs amount to
\begin{align*}
	\costs(\mx)&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_tf(\lambda_t/x_t)}^{x_t\cdot 0}+\overbrace{\swcosts(x_{t-1},x_t)}^{\max\{0,x_t-x_{t-1}\}}\bigr)=9+0+2+0+2=13\\
	\costs(\mx')&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_t'f(\lambda_t/x_t')}^{x_t'\cdot 0}+\overbrace{\swcosts(x_{t-1}',x_t')}^{\max\{0,x_t'-x_{t-1}'\}}\bigr)=16+0+8+0+8=32
\end{align*}
which shows that $\mx'$ is not 2-approximative. Of course, we are again curious about how this cost explosion is possible. Since we set $f(\lambda)=0$, our servers do not incur operating costs, which means that the cost explosion must be due to the increased switching costs of $\mx'$. The problem in this case is the oscillating behavior of $\mx$ around a power of two (namely $8=2^3$), as one can see in the following figure. 
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{./../figures/switching_costs_not_2_opt_1}
	\caption{\underline{Optimal schedule $\mx$:} Note how the schedule oscillates around 8 (a power of two).}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{../figures/switching_costs_not_2_opt_2}
	\caption{\underline{Approximative schedule $\mx'$:} The approximative schedule is restricted to powers of two.}
\end{subfigure}
\caption{Comparison between an optimal and an approximative schedule. The approximative schedule incurs more than twice as much switching costs.}
\label{fig:adaption-schedule}
\end{figure}
\end{exmpl}
So, is this the end of our hunt for a 2-approximative algorithm? No, certainly not! Although our naive approach was to no avail, there is indeed a better $B$-restricted schedule for our example. Instead of following the optimal schedule's oscillation, we can simply use a schedule that stays put during these oscillating steps, namely the schedule $\mx'=(16,16,16,16,16)$ with cost $\costs(\mx')=16$. Obviously, this seems like a rather trivial example since we set $f(\lambda)=0$, and hence we do not need to worry about the new schedule's operating costs. However, it indeed turns out that making the right choice between following the optimal schedule's oscillation and staying put will always allow us to acquire a 2-approximative solution. This observation will be a key part of the next lemma's proof. 

In the proof, we are going to divide a schedule $\mx$ into periods at which its plot crosses a power of two. We then show that every such period can be transformed to a period in a $B$-restricted schedule $\mx'$ such that the transformed period incurs at most twice as much costs. In order to formalize how to exactly split our schedules into such periods, we make a handy definition.
\begin{defn}
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule and $t\in[T+1]$. We say that $\mx$ changes its \emph{2-state} at time $t$ if $\mx$ satisfies the formula
\begin{equation*}
	x_{t-1}\neq x_t\land \left(x_{t-1}=0 \lor x_t\notin\left[2^{\floor{\log_2(x_{t-1})}},2^{\floor{\log_2(x_{t-1})}+1}\right)\right)
\end{equation*}
\end{defn}
As an example, the schedule $\mx=(5,4,8,7,8,15,10,16)$ changes its 2-state at times \makebox{$t\in\{1,3,4,5,8,9\}$}. To put it roughly, $\mx$ changes its 2-state at time $t$ if $x_t$ lies between a different pair of powers of two than its predecessor. More precisely, as one can see in Figure~\ref{fig:schedule_2_states}, $\mx$ changes its 2-state if its plot ascends and touches a next higher power of two or descends and leaves the current pair of powers of two.
\begin{figure}[H]
\centering
\includestandalone[width=0.7\textwidth]{../figures/schedule_2_states}
\caption{Visual representation of a schedule and its 2-state changes.}
\label{fig:schedule_2_states}
\end{figure}
We are now geared up to deal with the main work of this section.
\begin{lem}\label{lem:transform_schedule_approx_2}
Let $\mx$ be a schedule for $\inp$. There exists a $B$-restricted schedule $\mx'$ satisfying \makebox{$\costs(\mx')\le2\costs(\mx)$}.
\end{lem}
\begin{proof}
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule for $\inp$. We need to construct a $B$-restricted schedule $\mx'=(x_1',\dotsc,x_T')$ such that \makebox{$\costs(\mx')\le2\costs(\mx)$}.
First, if $\mx$ is not feasible, we have $\costs(\mx)=\infty$, and thus any arbitrary $B$-restricted schedule $\mx'$ satisfies $\costs(\mx')\le2\costs(\mx)$; hence, assume that $\mx$ is feasible. Next, we notice that if $\mx$ shuts down all its servers at some timeslot $t\in[T]$ (i.e.\ $x_t=0$), we can split $\mx$ into two subschedules \makebox{$\mx_1\coloneqq(x_1,\dotsc,x_{t-1})$} and \makebox{$\mx_2\coloneqq(x_{t+1},\dotsc,x_T)$}. It then suffices to prove the claim for $\mx_1$ and $\mx_2$, since we can then construct the 2-approximative schedule by setting $x_t'\coloneqq 0$ and $\mx'\coloneqq(\mx_1',x_t',\mx_2')$. Thus, by recursively applying this method, we can reduce our proof to a list of subschedules $\mx_1,\dotsc,\mx_N$ that never shut down all servers. Consequently, without loss of generality, we subsequently assume that $\mx$ never powers down all its servers, that is $x_t>0$ for all $t\in[T]$. 
	
Next, we show that we can iteratively construct $\mx'$ by transforming every period between two 2-state changes of $\mx$ to a \unsure{``2-approximative periond''\ldots wortwahl?} 2-approximative period in $\mx'$. For this, let $i$ and $j+1$ with \makebox{$i,j\in[T]$} and $i\le j$ be the first unprocessed timeslots at which $\mx$ changes its \makebox{2-state}. To conveniently refer to the schedules' periods between $i$ and $j$, we define the subschedules \makebox{$\mx_{i,j}\coloneqq(x_i,\dotsc,x_j)$} and $\mx'_{i,j}\coloneqq(x_i',\dotsc,x_j')$. We then have to show that $\mx_{i,j}$ can be transformed to $\mx_{i,j}'$ with 2-approximative costs. Note that the transformations are consecutive, i.e.\ after processing the period $[i,j]$, the next pair of indices $i',j'$ will be chosen such that $j+1=i'$. To conduct the transformations, we will need to refer to the lower and upper bound of~$\mx_{i,j}$, namely
\begin{flalign*}
	&&l\coloneqq2^{\floor{\log_2(x_i)}}&&\text{and}&&u\coloneqq\min\bigl\{2l,m\bigr\}&&&
\end{flalign*}
as well as to the lower and upper bound of $\mx$ at time $j+1$:
\begin{flalign*}
	&&l'\coloneqq\begin{cases}
		2^{\floor{\log_2(x_{j+1})}}, & \text{if $x_{j+1}\neq 0$}\\
		0, & \text{if $x_{j+1}=0$}
	\end{cases}
&&\text{and}&&u'\coloneqq\min\bigl\{2l',m\bigr\}&&&
\end{flalign*}
Note that $l,u,l',u'\in B$ and that $x_t\le u\le 2x_t$ holds for any $i\le t\le j$ since $\mx$ does not change its 2-state between $i$ and $j$. To prove that our following transformations are 2-approximative, we conduct an amortized analysis for the switching costs of $\mx'$ using the \emph{accounting method}. The basic idea of the accounting method is to overcharge some operations and to save the excess charge as a \emph{credit}, which can be used to compensate for subsequent, more expensive operations. More information about the accounting method can be found in~\parencite[Section~17.2]{intro-algo}.
	
As an \emph{invariant} of the following transformations, we are going to ensure that $\mx'$ can potentially move to $u$ at time $i$ in a 2-approximative manner, i.e.\ we ensure that~$\mx'$ incurs at most twice as much switching costs as $\mx$ if we set $x_i'\coloneqq u$ -- whether this step will be taken in the end or not. Further, we are going to ensure that $x_t'\ge u$ holds for any $i\le t\le j$ after every transformation step. 
	
First, we have to check that our invariant initially holds. For the initial start-up process \makebox{(i.e.\ $i=1$)}, we can simply move to the next power of 2 larger than $x_1$ contained in $B$, that is we set $x_1'\coloneqq u\in B$. Since we know that $x_1'\le2x_1$, we can conclude that $\beta x_1'\le\beta 2x_1$, which shows that $\mx'$ has 2-approximative start-up switching costs. Thus, the invariant initially holds. Moreover, we can use the difference of switching costs $\beta(2x_1-x_1')=\beta(2x_1-u)$ as a credit for our amortized analysis.

Now, let us have a closer look on the possible behaviors of $\mx$ between its 2-state changes. The behaviors can be classified based on how $\mx$ enters and leaves the interval $[l,u)$. We have to consider four different cases:
\begin{enumerate}[label=(\alph*)]
	\item $\mx$ enters $[l,u)$ from below and then descends to $[l',u')$.\label{itm:schedule_behavior_up_down}
	\item $\mx$ enters $[l,u)$ from above and then descends to $[l',u')$.\label{itm:schedule_behavior_down_down}
	\item $\mx$ enters $[l,u)$ from below and then ascends to $[l',u')$.\label{itm:schedule_behavior_up_up} 
	\item $\mx$ enters $[l,u)$ from above and then ascends to $[l',u')$.\label{itm:schedule_behavior_down_up} 
\end{enumerate}
To finish the proof, we need to show that any case can be transformed to a 2-approximative period in $\mx'$ while ensuring that our invariant will hold at the beginning of the next period, i.e.\ that we can set $x_{j+1}\coloneqq u'$ with 2-approximative switching costs. Additionally, we have to verify that the credit of our amortized analysis stays non-negative. In particular, we will show that our credit will be greater than or equal to $\beta(2x_{j+1}-u')$ if $\mx$ exits $[l,u)$ by ascending to $[l',u')$. Since the transformations are consecutive, this equally means that the credit will be greater than or equal to $\beta(2x_i-u)$ if $\mx$ enters $[l,u)$ from below. Note that we already showed that this claim holds for our initial start-up process.
	
We begin with cases~\ref{itm:schedule_behavior_up_down} and~\ref{itm:schedule_behavior_down_down}, which are both depicted in Figure~\ref{fig:schedule_behavior_down}.
\begin{figure}[ht]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_up_down}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_down}
\end{subfigure}
\caption{The original schedule comes from below (case~\ref{itm:schedule_behavior_up_down}) or above (case~\ref{itm:schedule_behavior_down_down}), stays between $[l,u)$, and then descends to $[l',u')$. The approximative schedule stays put at $u$ for timeslots $i\le t\le j$.}
\label{fig:schedule_behavior_down}
\end{figure}
Due to our invariant, we know that we can set $x_i'\coloneqq u$ with \unsure{wortwahl}2-approximative switching costs. Consequently, setting $x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq u\in B$ gives us a strategy with 2-approximative switching costs between $i$ and $j$. Further, since $x_t\le u\le2x_t$ for any $i\le t\le j$, and $f$ is non-negative and monotonically increasing, the operating costs of $\mx'_{i,j}$ can be estimated by
\begin{equation*}
	\opcosts(\mx_{i,j}')\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}\sum\limits_{t=i}^j\left(uf\left(\frac{\lambda_t}{u}\right)\right)\le\sum\limits_{t=i}^j\left(2x_tf\left(\frac{\lambda_t}{u}\right)\right)\le2\sum\limits_{t=i}^j\left(x_tf\left(\frac{\lambda_t}{x_t}\right)\right)\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}2\opcosts(\mx_{i,j})
\end{equation*}
Thus, the operating costs of $\mx_{i,j}'$ are 2-approximative. Further, since $u'<u$, we do not need to account for additional switching costs to satisfy our invariant at time $j+1$; however, we want to stress that one cannot tell at this step if we should indeed set $x_{j+1}\coloneqq u'$ (c.f. Figure~\ref{fig:schedule_behavior_down_up} and its related case). Lastly, we note that the credit of our amortized analysis stays untouched in both cases, i.e.\ the credit stays non-negative.
	
Next, we consider case~\ref{itm:schedule_behavior_up_up}, which is illustrated in Figure~\ref{fig:schedule_behavior_up_up}.
\begin{figure}[ht]
\centering
\includestandalone[width=0.5\textwidth]{../figures/schedule_behavior_up_up}	
\caption{The original schedule (case~\ref{itm:schedule_behavior_up_up}) comes from below, stays between $[l,u)$, and then ascends to $[l',u')$. The approximative schedule stays put at $u$ for timeslots $i\le t\le j$.}
\label{fig:schedule_behavior_up_up}
\end{figure}
Again, due to our invariant, we know that setting $x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq u\in B$ gives us a strategy with \makebox{2-approximative} switching costs. Moreover, as in the previous case, the operating costs of~$\mx_{i,j}'$ are \makebox{2-approximative}. In order to verify that our invariant holds at time $j+1$, we note that $\mx$ has to power on at least $x_{j+1}-x_i$ servers between $i$ and $j+1$. Thus, it suffices to show that $\beta(u'-u)\le2\beta(x_{j+1}-x_i)$ holds; however, it is clear that this must not be the case (just take $u'=8,u=4,x_{j+1}=4,x_i=3$). Nevertheless, since we entered $[l,u)$ from below, we know that our credit is greater than or equal to $\ge\beta(2x_i-u)$. Thus, we can use our credit to compensate for our costs:
\begin{align*}
	2\beta(x_{j+1}-x_i)+\overbrace{\beta(2x_i-u)}^{\text{credit}}=\beta(2x_{j+1}-u)\ge\beta(u'-u)
\end{align*}
where the last inequality follows from $u'\le2x_{j+1}$. Thus, our invariant at time $j+1$ is satisfied. Further, we can use the difference $\beta(2x_{j+1}-u)-\beta(u'-u)=\beta(2x_{j+1}-u')$ as our new credit for subsequent operations. 
	
Finally, we have to consider the case where $\mx$ descends to $[l,u)$ and then ascends to $[l',u')$. As we have seen in Example~\ref{exmpl:oscillating_schedule}, this case turns out to be slightly more complicated, since simply following an oscillating behavior of $\mx$ can lead to a cost explosion for $\mx'$. Nevertheless, we can solve this issue by considering two different strategies for $\mx_{i,j}'$, namely
\begin{align*}
	\mx_{i,j}^u&\coloneqq(x_i^u\coloneqq u,\dotsc,x_j^u\coloneqq u)\\
	\mx_{i,j}^{\hat{u}}&\coloneqq(x_i^{\hat{u}}\coloneqq\hat{u},\dotsc,x_j^{\hat{u}}\coloneqq\hat{u})
\end{align*}
where $\hat{u}\coloneqq\min\{2^{\ceil{\log_2(u+1)}},m\}\in B$. Due to our invariant and the fact that $\mx$ descends at time $i$, we know that $x_{i-1}'\ge \hat{u}\ge u$, and thus both strategies do no incur switching costs up to time $j$ (but possibly at time $j+1$). Both strategies are illustrated in Figure~\ref{fig:schedule_behavior_down_up}.
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_1}
\caption{Strategy $\mx_{i,j}^u$ stays put at $u$ for $i\le t\le j$.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_2}
\caption{Strategy $\mx_{i,j}^{\hat{u}}$ stays put at $\hat{u}$ for $i\le t\le j$.}
\end{subfigure}
\caption{The original schedule (case~\ref{itm:schedule_behavior_down_up}) comes from above, stays between $[l,u)$, and then rises to $[l',u')$. The approximative schedule has two different possibilities.}
\label{fig:schedule_behavior_down_up}
\end{figure}
We are now going to prove that either the costs of $\mx_{i,j}^u$ or of $\mx_{i,j}^{\hat{u}}$, including possible switching costs to satisfy our invariant at time $j+1$, must be 2-competitive. 
First, we examine the switching costs of $\mx$ from $i$ up to $j+1$. To do so, we set $d\coloneqq\min\{x_t\mid i\le t\le j\}$ to refer to the smallest number of active servers used by $\mx_{i,j}$. Then, since $\mx$ has to power on at least $x_{j+1}-d$ servers between $i$ and $j+1$, we can give a lower bound for its switching costs:
\begin{equation*}
	\sum\limits_{t=i+1}^{j+1}\swcosts(x_{t-1},x_t)\ge\beta(x_{j+1}-d)
\end{equation*}
The next idea is to split $\mx_{i,j}$ and its associated switching costs at the final switching step into two parts; more precisely, we split $\beta(x_{j+1}-d)$ into $\beta(x_{j+1}-u)$ and $\beta(u-d)$. It then suffices to show that either $\mx_{i,j}^u$ or $\mx_{i,j}^{\hat{u}}$ can process the loads $\lambda_i,\dotsc,\lambda_j$ and eventually switch to $\hat{u}$ with 2-competitive costs under the assumption that $\mx_{i,j}$ only switches to $u$ as a first step. This is due to the fact that the remaining switching costs to ascend to $u'$ are 2-competitive anyway:
\begin{equation*}
	\beta(u'-\hat{u})\le\beta(2x_{j+1}-\hat{u})=\beta(2x_{j+1}-2u)=2\beta(x_{j+1}-u)
\end{equation*}
where we assumed that $\hat{u}=2u$ holds; otherwise, we have $u'=\hat{u}=m$ and the inequality trivially holds. Further, if $\hat{u}=2u$, we can once more use the difference $2\beta(x_{j+1}-u)-\beta(u'-\hat{u})=\beta(2x_{j+1}-u')$ as a switching cost credit for subsequent operations. If $\hat{u}\neq2u$, no further credit is needed, since we already reached the limit of servers $m$.

To proceed, we notice that if $\hat{u}-u\le2(u-d)$ holds, strategy $\mx_{i,j}^u$ has 2-competitive switching costs:
\begin{equation*}
	\beta(\hat{u}-u)\le2\beta(u-d)
\end{equation*}
Further, as in the previous cases, the operating costs of $\mx_{i,j}^u$ are 2-competitive, and thus $\mx_{i,j}^u$ is 2-competitive if $\hat{u}-u\le2(u-d)$. Hence, we subsequently assume that 
\begin{align}\label{eq:proof_2_competitive_asm_dist_pos}
	\hat{u}-u>2(u-d)&&\text{or equivalently}&&\hat{u}-3u+2d>0
\end{align}
Next, by the law of excluded middle, either $\mx_{i,j}^u$ is 2-competitive, or it is not 2-competitive. If it is 2-competitive, we are done; hence, from now on, we assume that $\mx_{i,j}^u$ is not 2-competitive, that is we have 
\begin{equation*}
	\opcosts(\mx_{i,j}^u)+\beta(\hat{u}-u)>2\bigl(\opcosts(\mx_{i,j})+\beta(u-d)\bigr)
\end{equation*}
We then need to show that $\mx_{i,j}^{\hat{u}}$ is 2-competitive.
First note that $\mx_{i,j}^u$ uses at most twice as many active servers as $\mx_{i,j}$, and therefore $\mx_{i,j}^u$ incurs at most twice as much operating costs as $\mx_{i,j}$. Consequently, the switching costs of $\mx_{i,j}^u$ must significantly outweigh those of $\mx_{i,j}$. Hence, it seems interesting to get an estimation for $\beta$. Rearranging the previous inequality gives us
\begin{align*}
	&&\opcosts(\mx_{i,j}^u)+\beta(\hat{u}-u)&>2\bigl(\opcosts(\mx_{i,j})+\beta(u-d)\bigr)&\\
	&\stackrel{\phantom{\hat{u}-3u+2d)>0}}{\iff}&\beta(\hat{u}-3u+2d)&>2\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^u)&\\
	&\stackrel{\hat{u}-3u+2d>0}{\iff}&\beta&>\frac{2\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}
\end{align*}
where the last step is justified by Assumption~\eqref{eq:proof_2_competitive_asm_dist_pos}.
This allows us to find a lower bound for the costs of using $\mx_{i,j}$:
\begin{align*}
	\opcosts(\mx_{i,j})+\beta(u-d)&>\opcosts(\mx_{i,j})+\frac{2\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}(u-d)\\
	&=\frac{(\hat{u}-3u+2d)\opcosts(\mx_{i,j})+2(u-d)\opcosts(\mx_{i,j})-(u-d)\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}\\
	&=\frac{(\hat{u}-u)\opcosts(\mx_{i,j})-(u-d)\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}
\end{align*}
Next, since we want to prove that $\opcosts(\mx_{i,j}^{\hat{u}})$ is 2-competitive, we have to show that the following cost difference is non-negative:
\begin{align*}
	2\bigl(\opcosts(\mx_{i,j})+\beta(u-d)\bigr)-\opcosts(\mx_{i,j}^{\hat{u}})
\end{align*}
We can use our just derived lower bound for $\opcosts(\mx_{i,j})+\beta(u-d)$ to estimate the difference:
\begin{align*}
	2\bigl(\opcosts(\mx_{i,j})+\beta(u-d)\bigr)-\opcosts(\mx_{i,j}^{\hat{u}})>2\Bigl(\frac{(\hat{u}-u)\opcosts(\mx_{i,j})-(u-d)\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}\Bigr)-\opcosts(\mx_{i,j}^{\hat{u}})
\end{align*}
Thus, it suffices to show that
\begin{align*}
	2\Bigl(\frac{(\hat{u}-u)\opcosts(\mx_{i,j})-(u-d)\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}\Bigr)-\opcosts(\mx_{i,j}^{\hat{u}})\ge 0
\end{align*}
which, using Assumption~\eqref{eq:proof_2_competitive_asm_dist_pos}, that is $\hat{u}-3u+2d>0$, can be simplified to
\begin{equation*}
	2(\hat{u}-u)\opcosts(\mx_{i,j})-2(u-d)\opcosts(\mx_{i,j}^u)-(\hat{u}-3u+2d)\opcosts(\mx_{i,j}^{\hat{u}})\ge 0
\end{equation*}
To show this inequality, we have to take a closer look on the the schedules' operating costs:
\begin{align*}
	&&&2(\hat{u}-u)\opcosts(\mx_{i,j})-2(u-d)\opcosts(\mx_{i,j}^u)-(\hat{u}-3u+2d)\opcosts(\mx_{i,j}^{\hat{u}})\\
	&\stackrel{\eqref{eq:mx_schedule_op_costs}}{=}&&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl(x_tf\Bigl(\frac{\lambda_t}{x_t}\Bigr)\Bigr)-2(u-d)\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-3u+2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{\hat{u}}\Bigr)\Bigr)
\end{align*}
First, we notice that $u\le\hat{u}$ and $x_t<u$ for $i\le t\le j$. Together with the fact that $f$ is monotonically increasing, we infer that
\begin{align*}
	&&&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl(x_tf\Bigl(\frac{\lambda_t}{x_t}\Bigr)\Bigr)-2(u-d)\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-3u+2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{\hat{u}}\Bigr)\Bigr)\\
	&\ge&&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl(x_tf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-2(u-d)\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-3u+2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)
\end{align*}
Since $d$ is the smallest number of active servers scheduled by $\mx$, and $f$ is non-negative, we can conclude that
\begin{align*}
	&&&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl(x_tf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-2(u-d)\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-3u+2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\\
	&\ge&&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl(df\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-2(u-d)\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-3u+2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)
\end{align*}
Hence, to finish the case, it suffices to show that
\begin{align*}
	2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl(df\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-2(u-d)\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-3u+2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\ge 0
\end{align*}
Further rearranging the left hand side gives us
\begin{align*}
	&2(\hat{u}-u)\sum\limits_{t=i}^j\Bigl(df\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-2(u-d)\sum\limits_{t=i}^j\Bigl(uf\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-(\hat{u}-3u+2d)\sum\limits_{t=i}^j\Bigl(\hat{u}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\\
	=&\sum\limits_{t=i}^j\Bigl((2d\hat{u}-2du)f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-\sum\limits_{t=i}^j\Bigl(2u^2-2du)f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)-\sum\limits_{t=i}^j\Bigl((\hat{u}^2-3u\hat{u}+2d\hat{u})f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\\
	=&\sum\limits_{t=i}^j\Bigl((2d\hat{u}-2du-2u^2+2du-\hat{u}^2+3u\hat{u}-2d\hat{u})f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)\\
	=&\sum\limits_{t=i}^j\Bigl((-\hat{u}^2+3u\hat{u}-2u^2)f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr)=\sum\limits_{t=i}^j\Bigl(\underbrace{-(u-\hat{u})(2u-\hat{u})}_{g(\hat{u})\coloneqq}f\Bigl(\frac{\lambda_t}{u}\Bigr)\Bigr) 
\end{align*}
Now, since $f$ is non-negative, we just have to verify that $g(\hat{u})\ge0$ for all possible values of $\hat{u}$, namely $\hat{u}\in[u,2u]$. We notice that $g(\hat{u})$ is an inverted parabola with roots $u$ and $2u$. Thus, we know that $g(u)=g(2u)=0$ and $g(\hat{u})>0$ for $u<\hat{u}<2u$, which finishes the case.

By iteratively applying above procedure to $\mx$, starting with $i=1$ and stopping with $j=T+1$, we obtain a $B$-restricted schedule $\mx'$ that satisfies $\costs(\mx')\le2\costs(\mx)$.
\end{proof}
Safe in the knowledge that there always exists a 2-competitive, $B$-restricted schedule, we are just left with the verification of our new graph. Since we did not change the general construction idea of our graph, this verification turns out to be very similar to that done in Section~\ref{sec:opt_offline_pseudo_lin}. For the proof of the next lemma, the bijection between $B$-restricted schedules and reasonable paths, we need to recall our notion of ``maximum'' nodes $x_t$ in reasonable paths $P$, as described in Definition~\ref{defn:max_path_node}.
\begin{lem}\label{lem:sched_reasn_path_approx_2}
Let $\bm{\mx}$ be the set of all $B$-restricted schedules for $\inp$, and let $\bm{\mathcal{P}}$ be the set of all reasonable paths. The map
\begin{equation*}
	\Phi:\bm{\mathcal{P}}\rightarrow\bm{\mx},\quad P\mapsto (x_1,\dotsc,x_T)
\end{equation*}
is a bijection satisfying $\costs(P)=\costs\bigl(\Phi(P)\bigr)$.
\end{lem}
\begin{proof}
The proof is analogous to the proof of Lemma~\ref{lem:sched_reasn_path_pseudo_lin} considering that we conduct logarithmic instead of incremental switching steps.
\end{proof}
Finally, we arrive at the main result of this section.
\begin{thm}\label{thm:approx_2}
Any shortest, reasonable path $P$ corresponds to a 2-competitive, $B$-restricted schedule $\mx$ for $\inp$ with $\costs(P)=\costs(\mx)$.
\end{thm} 
\begin{proof}
By Lemma~\ref{lem:sched_reasn_path_approx_2}, we have a bijection $\Phi$ between reasonable paths $P$ and $B$-restricted schedules obeying $\costs(P)=\costs\bigl(\Phi(P)\bigr)$. Thus, we have 
\begin{equation*}
	\costs(P)\text{ minimal}\iff \costs\bigl(\Phi(P)\big)\text{ minimal}
\end{equation*}
Now let $P$ be a shortest, reasonable path, and let $\mx^*$ be an optimal schedule for $\inp$. We have to verify that $\costs\bigl(\Phi(P)\bigr)\le 2\costs(\mx^*)$. By Lemma~\ref{lem:transform_schedule_approx_2}, we know that there exists a $B$-restricted schedule $\mx'$ such that $\costs(\mx')\le 2\costs(\mx^*)$. Since $\Phi$ is a bijection, and $P$ is a shortest, reasonable path, we know that $\costs\bigl(\Phi(P)\bigr)\le\costs(\mx')$. Thus, we conclude that 
\begin{equation*}
	\costs(P)=\costs\bigl(\Phi(P)\bigr)\le\costs(\mx')\le 2\costs(\mx^*)
\end{equation*}
and the claim follows.
\end{proof}
Again, it is not difficult to show that also shortest paths which are not reasonable can be transformed to a desired 2-competitive schedule.
\begin{cor}
Any shortest path $P$ from $v_{0,0}$ to $v_{0,T\downarrow}$ can be transformed to a 2-competitive, $B$-restricted schedule $\mx$ for $\inp$ with $\costs(\mx)=\costs(P)$.
\end{cor}
\begin{proof}
Let $P$ be a shortest path from $v_{0,0}$ to $v_{0,T\downarrow}$. By using a small adaption of Proposition~\ref{prop:path_to_reasn_path} that uses logarithmic instead of incremental switching steps, we can transform $P$ to a reasonable path $P'$ with $\costs(P')=\costs(P)$.
In turn, $P'$ corresponds to a 2-competitive, \makebox{$B$-restricted} schedule $\mx$ with $\costs(\mx)=\costs(P')=\costs(P)$ by Theorem~\ref{thm:approx_2}, which finishes the proof.
\end{proof}
In the following, we give an algorithm based on our just verified constructions. Naturally, since we did not change the graph's overall structure and working principle, a small adaption of Algorithm~\ref{alg:opt_offline_pseudo_linear} will serve its purpose. To account for the logarithmic steps in our graph, we introduce an auxiliary function \textproc{nodes}, which calculates the number of servers associated to an given index $i$ in our tables $C$ and $P$. As a matter of implementation convenience, the variable $b$ is defined as $\log_2(\ceil{m})$ instead of $\log_2(\floor{m})$ in the following algorithm.
\begin{algorithm}[H]
  \caption{Linear 2-competitive offline scheduling}
  \label{alg:approx_2_offline_linear}
  \begin{algorithmic}[1]
  \Function{2\_optimal\_offline\_scheduling}{$m,T,\Lambda,\beta,f$}
	  \Let{$(C,P)$}{\Call{shortest\_paths}{$m,T,\Lambda,\beta,f$}}
	  \Let{$\mx$}{\Call{extract\_schedule}{$P,T,m$}}
	  \State \Return{$\mx$}
  \EndFunction
  \Statex
  \Function{node}{$i,m$}
	  \State \Return{$\min\bigl\{m,\floor{2^{i-1}}\bigr\}$}
  \EndFunction
  \algstore{lin2opt}
  \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
  \begin{algorithmic}[1]
  \algrestore{lin2opt}
  \Function{shortest\_paths}{$m,T,\Lambda,\beta,f$}
	\Let{$b$}{$\ceil{\log_2(m)}$}
	\LineComment{Allocate nodes' costs and predecessor tables}
	\Blet{$C[0\dotso b+1,1\dotso T]$ and $P[0\dotso b+1,1\dotso T]$}{new tables}
	\Let{$P[b+1,1]$}{$b+1$ and $C[b+1,1]\gets\costs(0,m,\lambda_1)$}\Comment{Initialize first node in first layer}
	\For{$i \gets b \textrm{ to } 0$}\Comment{Initialize first layer (downward minimization step)}
		\If {$C[i+1,1]<\costs\bigl(0,\Call{node}{i,m},\lambda_1\bigr)$}
			\Let{$P[i,1]$}{$P[i+1,1]$ and $C[i,1]\gets C[i+1,1]$}
		\Else
			\Let{$P[i,1]$}{$i$ and $C[i,1]\gets\costs\bigl(0,\Call{node}{i,m},\lambda_1\bigr)$}
		\EndIf
	\EndFor
	\For{$t \gets 1 \textrm{ to } T-1$}\Comment{Iterative calculate costs and predecessors}
		\For{$i \gets 1 \textrm{ to } b+1$}\Comment{Upward minimization step}
			\If {$C[i-1,t]+\beta\bigl(\Call{node}{i,m}-\Call{node}{i-1,m}\bigr)<C[i,t]$}
			    \Let{$P[i,t]$}{$P[i-1,t]$}
			    \Let{$C[i,t]$}{$C[i-1,t]+\beta\bigl(\Call{node}{i,m}-\Call{node}{i-1,m}\bigr)$}
			\EndIf
		\EndFor
		\Let{$P[b+1,t+1]$}{$b+1$ and $C[b+1,t+1]\gets C[b+1,t]+\opcosts(m,\lambda_{t+1})$}
		\For{$i \gets b \textrm{ to } 0$}\Comment{Downward minimization step}
			\If {$C[i+1,t+1]<C[i,t]+\opcosts\bigl(\Call{node}{i,m},\lambda_{t+1}\bigr)$}
				\Let{$P[i,t+1]$}{$P[i+1,t+1]$ and $C[i,t+1]\gets C[i+1,t+1]$}
			\Else
				\Let{$P[i,t+1]$}{$i$ and $C[i,t+1]\gets C[i,t]+\opcosts\bigl(\Call{node}{i,m},\lambda_{t+1}\bigr)$}
			\EndIf
		\EndFor
	\EndFor
	\State \Return{$(C,P)$}
  \EndFunction
  \Statex
  \Function{extract\_schedule}{$P,T,m$}
	\Blet{$\mx[1\dotso T]$}{a new array}
	\Let{$i$}{$P[0,T]$}\Comment{Get index of best choice for last time slot}
    	\Let{$\mx[T]$}{$\Call{node}{i,m}$}\Comment{Calculate best choice for last time slot}
        \For{$t \gets T-1 \textrm{ to } 1$}\Comment{Iteratively obtain a schedule by using the predecessors}
		\Let{$i$}{$P[i,t]$}
		\Let{$\mx[t]$}{$\Call{node}{i,m}$}	
	\EndFor
	\State \Return{$\mx$}
  \EndFunction
  \end{algorithmic}
\end{algorithm}
The correctness of Algorithm~\ref{alg:approx_2_offline_linear} directly follows from Theorem~\ref{thm:approx_2} and the correctness of the shortest path calculation for directed acyclic graphs (for a proof see~\parencite[Section~24.2]{intro-algo}).

For our runtime analysis, we again take the same assumptions as done for Algorithm~\ref{alg:opt_offline_pseudo_poly}. Subroutine \textproc{shortest\_paths} needs $\Theta\bigl(\log_2(m)\bigr)$ iterations for its initialization and $\Theta\bigl(2T\log_2(m)\bigr)$ steps for the iterative calculation of predecessors and costs\unsure{Repetitive? Same words as in chapter 3}. In addition, \textproc{extract\_schedule} needs $\Theta(T)$ iterations for its schedule retrieval. Hence, we receive a runtime of 
\begin{equation*}
	\Theta\bigl(\log_2(m)+2T\log_2(m)+T\bigr)=\Theta(T\log_2(m))
\end{equation*}
To our great joy, the runtime is linear in the size of the input. Similarly, our memory demand is reduced to linear space $\Theta\bigl(T\log_2(m)\bigr)$, since the algorithm's memory demand is defined by the size of the tables $C$ and $P$, both being of size $\Theta\bigl(T\log_2(m)\bigr)$.

In this section, we saw that our reduction to logarithmic steps allows us to derive a linear algorithm that guarantees 2-competitive results. In our approach, we chose the base two logarithm for our step sizes. It seems like an interesting question, if this approach can be generalized to arbitrary bases, allowing for more precise approximations. This shall be final task of our work.

\section{A $(1+\beps)$-Approximative Linear-Time Algorithm}
TODO text + graph

\begin{defn}
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule and $t\in[T]$. We say that $\mx$ changes its \emph{$y$-state} at time $t$ if $x_t$ lies between a different pair of powers of $y$ than its predecessor, that is $x_t$ satisfies
\begin{equation*}
	x_{t-1}\neq x_t\land \Bigl(x_t\notin\bigl[y^{\ceil{\log_y(x_{t-1})}-1},y^{\ceil{\log_y(x_{t-1})}}\bigr)\lor x_{t-1}=0\Bigr)
\end{equation*}
\end{defn}
Now we are geared up to deal with the main work of this section.
\begin{lem}
Let $\mx$ be a schedule for $\inp$. There exists a $B$-restricted schedule $\mx'$ satisfying \makebox{$\costs(\mx')\le y\costs(\mx)$}.
\end{lem}
\begin{proof}
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule for $\inp$. We need to construct a $B$-restricted schedule $\mx'=(x_1',\dotsc,x_T')$ such that \makebox{$\costs(\mx')\le y\costs(\mx)$}. The construction will be similar to that done in Lemma~\ref{lem:transform_schedule_approx_2}. As a first step, we can again assume that $\mx$ is feasible and never powers down all its servers.  
Next, we show that every period between two $y$-state changes of $\mx$ can be iteratively transformed to a $y$-competitive period in $\mx'$. For this, let $i\in[T]$ and $j+1\in\fromto{2}{T+1}$ with $i<j+1$ be the first unprocessed timeslots at which $\mx$ changes its $y$-state. We define the lower and upper bound of $\mx_{i,j}$ as
\begin{flalign*}
	&&l\coloneqq y^{\ceil{\log_y(x_i)}-1}&&\text{and}&&u\coloneqq\min\bigl\{y^{\ceil{\log_y(x_i)}},m\bigr\}&&&
\end{flalign*}
and the lower and upper bound of $\mx$ at time $j+1$ as
\begin{flalign*}
	&&l'\coloneqq\begin{cases}
		y^{\ceil{\log_y(x_{j+1})}-1}, & \text{if $x_{j+1}\neq 0$}\\
		0, & \text{if $x_{j+1}=0$}
	\end{cases}
&&\text{and}&&u'\coloneqq\min\bigl\{y^{\ceil{\log_y(x_{j+1})}},m\bigr\}&&&
\end{flalign*}
As an invariant of the following transformations, we are going to ensure that $\mx'$ can potentially move to $\floor{u}$ at time $i$ in a $y$-competitive manner, that is we ensure that we can set $x_i'\coloneqq \floor{u}$ with $y$-competitive switching costs -- whether this step will be taken in the end or not. Further, we ensure that $x_t'\ge \floor{u}$ holds for any $i\le t\le j$ after every transformation step. For the initial start-up process (i.e.\ $i=1$), we can simply move to the next power of $y$ larger than $x_1$ contained in $B$, that is we set $x_1'\coloneqq \floor{u} \in B$. Since $x_1'\le yx_1$, we know that $\beta x_1'\le\beta yx_1$; thus, the start-up switching costs of $\mx'$ are $y$-competitive and the invariant initially holds. Moreover, we can (and indeed will have to) use the difference of switching costs $\beta(yx_1-x_1')=\beta(yx_1-\floor{u})$ as a switching cost credit to compensate for subsequent, more expensive switching steps.

Now, let us have a closer look on the possible behaviors of $\mx$ between its $y$-state changes. We will classify the behaviors based on how $\mx$ enters and leaves the interval $[l,u)$. First, we consider the cases where $\mx$ leaves the interval $[l,u)$ by turning off servers, as depicted in Figure~\ref{todo}.
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_down}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_up_down}
\end{subfigure}
\caption{The original schedule comes from above or below, stays between $[l,u)$, and then descends to $[l',u')$. The approximative schedule stays put at $\floor{u}$ for timeslots $i\le t\le j$.}
\end{figure}
Due to our invariant, we know that we can set $x_i'\coloneqq \floor{u}$ with $y$-competitive switching costs. Consequently, setting $x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq \floor{u}\in B$ gives us a strategy with $y$-competitive switching costs between $i$ and $j$. Further, since $x_t\le \floor{u}\le yx_t$ for any $i\le t\le j$, and $f$ is non-negative and monotonically increasing, the operating costs of $\mx'_{i,j}$ can be estimated by
\begin{equation*}
	\opcosts(\mx_{i,j}')\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}\sum\limits_{t=i}^j\Bigl(\floor{u} f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)\Bigr)\le\sum\limits_{t=i}^j\Bigl(yx_tf\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)\Bigr)\le y\sum\limits_{t=i}^j\Bigl(x_tf\Bigl(\frac{\lambda_t}{x_t}\Bigr)\Bigr)\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}y\opcosts(\mx_{i,j})
\end{equation*}
Thus, the operating costs of $\mx_{i,j}'$ are $y$-competitive. Further, since $\floor{u'}<\floor{u}$, we do not need to account for additional switching costs to satisfy our invariant at time $j+1$; however, we want to stress that one cannot tell at this step if we should indeed set $x_{j+1}\coloneqq \floor{u'}$ (c.f. Figure~\ref{todo} and its related case). 
	
Next, we consider the case where $\mx$ rises to $[l,u)$ and then further ascends to $[l',u')$, as illustrated in Figure~\ref{todo}.
\begin{figure}[H]
\centering
\includestandalone[width=0.5\textwidth]{../figures/schedule_behavior_up_up}	
\caption{The original schedule comes from below, stays between $[l,u)$, and then rises to $[l',u')$. The approximative schedule stays put at $u$ for timeslots $i\le t\le j$ and rises to $u'$ at time $j+1$.}
\end{figure}
In this case, we set $x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq \floor{u}\in B$ and $x_{j+1}'\coloneqq \floor{u'}\in B$. Evidently, as in the previous case, the operating costs of $\mx_{i,j}'$ are $y$-competitive. Further, due to our invariant, we can again safely move to $\floor{u}$ with $y$-competitive switching costs. Lastly, we need to ensure that our invariant at time $j+1$ holds. Since $\mx$ has to power on at least $x_{j+1}-x_i$ servers between $i$ and $j+1$, it suffices to show that $\beta(\floor{u'}-\floor{u})\le y\beta(x_{j+1}-x_i)$ holds. Using an amortized analysis, that is using our switching cost credit, we can see that this is indeed the case:
\begin{align*}
	y\beta(x_{j+1}-x_i)+\overbrace{\beta(yx_i-\floor{u})}^{\text{credit}}=\beta(yx_{j+1}-\floor{u})\ge\beta(\floor{u'}-\floor{u})
\end{align*}
where the last inequality follows from $\floor{u'}\le yx_{j+1}$. Thus, our invariant at time $j+1$ is satisfied. Note again that the difference $\beta(yx_{j+1}-\floor{u'})-\beta(\floor{u'}-\floor{u})=\beta(yx_{j+1}-\floor{u'})$ can be used as a switching cost credit for subsequent operations. 
	
Finally, we have to consider the case where $\mx$ descends to $[l,u)$ and then ascends to $[l',u')$. We can solve this issue by considering two different strategies for $\mx_{i,j}'$, namely
\begin{align*}
	\mx_{i,j}^{\floor{u}}&\coloneqq(x_i^{\floor{u}}\coloneqq\floor{u},\dotsc,x_j^{\floor{u}}\coloneqq\floor{u})\\
	\mx_{i,j}^{\floor{\hat{u}}}&\coloneqq(x_i^{\floor{\hat{u}}}\coloneqq\floor{\hat{u}},\dotsc,x_j^{\floor{\hat{u}}}\coloneqq\floor{\hat{u}})
\end{align*}
where $\hat{u}\coloneqq\min\{y^{\log_y(u)+1},m\}\in B$. Due to our invariant and the fact that $\mx$ descends at time $i$, we know that $x_{i-1}'\ge \hat{u}\ge u$, and thus both strategies do no incur switching costs up to time $j$ (but possibly at time $j+1$). Both strategies are illustrated in Figure~\ref{todo}.
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_1}
\caption{Strategy $\mx_{i,j}^u$ stays put at $u$ for $i\le t\le j$\\and then rises to $u'$.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_2}
\caption{Strategy $\mx_{i,j}^{\hat{u}}$ stays put at $\hat{u}$ for $i\le t\le j$ and then rises to $u'$.}
\end{subfigure}
\caption{The original schedule comes from above, stays between $[l,u)$, and then rises to $[l',u')$. The approximative schedule has two different possibilities.}
\end{figure}
We are now going to prove that either the costs of $\mx_{i,j}^{\floor{u}}$ or of $\mx_{i,j}^{\floor{\hat{u}}}$, including possible switching costs to satisfy our invariant at time $j+1$, must be $y$-competitive. 
First, we examine the switching costs of $\mx$ from $i$ up to $j+1$. To do so, we set $d\coloneqq\min\{x_t\mid i\le t\le j\}$ to refer to the smallest number of active servers used by $\mx_{i,j}$. Then, since $\mx$ has to power on at least $x_{j+1}-d$ servers between $i$ and $j+1$, we can give a lower bound for its switching costs:
\begin{equation*}
	\sum\limits_{t=i+1}^{j+1}\swcosts(x_{t-1},x_t)\ge\beta(x_{j+1}-d)
\end{equation*}
The next idea is to split $\mx_{i,j}$ and its associated switching costs at the final switching step into two parts; more precisely, we split $\beta(x_{j+1}-d)$ into $\beta(x_{j+1}-\frac{\floor{\hat{u}}}{y})$ and $\beta(\frac{\floor{\hat{u}}}{y}-d)$. It then suffices to show that either $\mx_{i,j}^u$ or $\mx_{i,j}^{\hat{u}}$ can process the loads $\lambda_i,\dotsc,\lambda_j$ and eventually switch to $\floor{\hat{u}}$ with $y$-competitive costs under the assumption that $\mx_{i,j}$ only switches to $\frac{\floor{\hat{u}}}{y}$ as a first step. This is due to the fact that the remaining switching costs to ascend to $\floor{u'}$ are then $y$-competitive anyway:
\begin{equation*}
	\beta(\floor{u'}-\floor{\hat{u}})\le\beta(yx_{j+1}-\hat{u})=y\beta\Bigl(x_{j+1}-\frac{\floor{\hat{u}}}{y}\Bigr)
\end{equation*}
TODO where we assumed that $\hat{u}=2u$ holds; otherwise, we have $u'=\hat{u}=m$ and the inequality trivially holds. Further, if $\hat{u}=2u$, we can once more use the difference $y\beta(x_{j+1}-\frac{\floor{\hat{u}}}{y})-\beta(\floor{u'}-\floor{\hat{u}})=\beta(yx_{j+1}-\floor{u'})$ as a switching cost credit for subsequent operations. If $\hat{u}\neq2u$, no further credit is needed, since we already reached the limit of servers $m$.

To proceed, we notice that if $\floor{\hat{u}}-\floor{u}\le y(\frac{\floor{\hat{u}}}{y}-d)$ holds, strategy $\mx_{i,j}^u$ has $y$-competitive switching costs:
\begin{equation*}
	\beta(\floor{\hat{u}}-\floor{u})\le y\beta\Bigl(\frac{\floor{\hat{u}}}{y}-d\Bigr)
\end{equation*}
Further, as in the previous cases, the operating costs of $\mx_{i,j}^u$ are $y$-competitive, and thus $\mx_{i,j}^u$ is $y$-competitive if $\floor{\hat{u}}-\floor{u}\le y(\frac{\floor{\hat{u}}}{y}-d)$. Hence, we subsequently assume that 
\begin{align}\label{eq:proof_y_competitive_asm_dist_pos}
	\floor{\hat{u}}-\floor{u}>y\Bigl(\frac{\floor{\hat{u}}}{y}-d\Bigr)&&\text{or equivalently}&&yd-\floor{u}>0
\end{align}
Next, by the law of excluded middle, either $\mx_{i,j}^u$ is $y$-competitive, or it is not $y$-competitive. If it is $y$-competitive, we are done; hence, from now on, we assume that $\mx_{i,j}^{\floor{u}}$ is not $y$-competitive, that is we have 
\begin{equation*}
	\opcosts(\mx_{i,j}^{\floor{u}})+\beta(\floor{\hat{u}}-\floor{u})>y\Bigl(\opcosts(\mx_{i,j})+\beta\Bigl(\frac{\floor{\hat{u}}}{y}-d\Bigr)\Bigr)
\end{equation*}
We then need to show that $\mx_{i,j}^{\floor{\hat{u}}}$ is $y$-competitive.
Rearranging the previous inequality gives us an estimation for $\beta$:
\begin{align*}
	&&\opcosts(\mx_{i,j}^{\floor{u}})+\beta(\floor{\hat{u}}-\floor{u})&>y\Bigl(\opcosts(\mx_{i,j})+\beta\Bigl(\frac{\floor{\hat{u}}}{y}-d\Bigr)\Bigr)
&\\
	&\stackrel{\phantom{yd-\floor{u}>0}}{\iff}&\beta(yd-\floor{u})&>y\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^{\floor{u}})&\\
	&\stackrel{yd-\floor{u}>0}{\iff}&\beta&>\frac{y\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}
\end{align*}
where the last step is justified by Assumption~\eqref{eq:proof_y_competitive_asm_dist_pos}.
This allows us to find a lower bound for the costs of using $\mx_{i,j}$:
\begin{align*}
	\opcosts(\mx_{i,j})+\beta\Bigl(\frac{\floor{\hat{u}}}{y}-d\Bigr)&>\opcosts(\mx_{i,j})+\frac{y\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}\Bigl(\frac{\floor{\hat{u}}}{y}-d\Bigr)\\
	&=\frac{(yd-\floor{u})\opcosts(\mx_{i,j})+(\floor{\hat{u}}-yd)\opcosts(\mx_{i,j})-(\frac{\floor{\hat{u}}}{y}-d)\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}\\
	&=\frac{(\floor{\hat{u}}-\floor{u})\opcosts(\mx_{i,j})-(\frac{\floor{\hat{u}}}{y}-d)\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}
\end{align*}
Next, since we want to prove that $\opcosts(\mx_{i,j}^{\floor{\hat{u}}})$ is $y$-competitive, we have to show that the following cost difference is non-negative:
\begin{align*}
	y\Bigl(\opcosts(\mx_{i,j})+\beta\Bigl(\frac{\floor{\hat{u}}}{y}-d\Bigr)\Bigr)-\opcosts(\mx_{i,j}^{\floor{\hat{u}}})
\end{align*}
We can use our just derived lower bound for $\opcosts(\mx_{i,j})+\beta(u-d)$ to estimate the difference:
\begin{align*}
	&y\Bigl(\opcosts(\mx_{i,j})+\beta\Bigl(\frac{\floor{\hat{u}}}{y}-d\Bigr)\Bigr)-\opcosts(\mx_{i,j}^{\floor{\hat{u}}})\\
	>\quad&y\frac{(\floor{\hat{u}}-\floor{u})\opcosts(\mx_{i,j})-(\frac{\floor{\hat{u}}}{y}-d)\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}-\opcosts(\mx_{i,j}^{\floor{\hat{u}}})
\end{align*}
Thus, it suffices to show that
\begin{align*}
	y\frac{(\floor{\hat{u}}-\floor{u})\opcosts(\mx_{i,j})-(\frac{\floor{\hat{u}}}{y}-d)\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}-\opcosts(\mx_{i,j}^{\floor{\hat{u}}})\ge 0
\end{align*}
which, by Assumption~\eqref{eq:proof_y_competitive_asm_dist_pos}, that is $yd-\floor{u}>0$, can be simplified to
\begin{equation*}
	y(\floor{\hat{u}}-\floor{u})\opcosts(\mx_{i,j})-(\floor{\hat{u}}-yd)\opcosts(\mx_{i,j}^{\floor{u}})-(yd-\floor{u})\opcosts(\mx_{i,j}^{\floor{\hat{u}}})\ge 0
\end{equation*}
To show this inequality, we have to take a closer look on the the schedules' operating costs:
\begin{align*}
	&&&y(\floor{\hat{u}}-\floor{u})\opcosts(\mx_{i,j})-(\floor{\hat{u}}-yd)\opcosts(\mx_{i,j}^{\floor{u}})-(yd-\floor{u})\opcosts(\mx_{i,j}^{\floor{\hat{u}}})\\
	&\stackrel{\eqref{eq:mx_schedule_op_costs}}{=}&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\Bigl(\frac{\lambda_t}{x_t}\Bigr)-(\floor{\hat{u}}-yd)\floor{u}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(yd-\floor{u})\floor{\hat{u}}f\Bigl(\frac{\lambda_t}{\floor{\hat{u}}}\Bigr)\right)
\end{align*}
First, we notice that $\floor{u}\le\floor{\hat{u}}$ and $x_t\le\floor{u}$ for $i\le t\le j$. Together with the fact that $f$ is monotonically increasing, we infer that
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\Bigl(\frac{\lambda_t}{x_t}\Bigr)-(\floor{\hat{u}}-yd)\floor{u}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(yd-\floor{u})\floor{\hat{u}}f\Bigl(\frac{\lambda_t}{\floor{\hat{u}}}\Bigr)\right)\\
	&\ge&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(\floor{\hat{u}}-yd)\floor{u}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(yd-\floor{u})\floor{\hat{u}}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)\right)
\end{align*}
Since $d$ is the smallest number of active servers scheduled by $\mx$, and $f$ is non-negative, we can conclude that
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(\floor{\hat{u}}-yd)\floor{u}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(yd-\floor{u})\floor{\hat{u}}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)\right)\\
	&\ge&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})df\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(\floor{\hat{u}}-yd)\floor{u}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(yd-\floor{u})\floor{\hat{u}}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)\right)
\end{align*}
Hence, to finish the case, it suffices to show that
\begin{align*}
	\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})df\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(\floor{\hat{u}}-yd)\floor{u}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(yd-\floor{u})\floor{\hat{u}}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)\right)\ge 0
\end{align*}
Further rearranging the left hand side gives us
\begin{align*}
	&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})df\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(\floor{\hat{u}}-yd)\floor{u}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)-(yd-\floor{u})\floor{\hat{u}}f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)\right)\\
	=&\sum\limits_{t=i}^j\left(\Bigl(y(\floor{\hat{u}}-\floor{u})d-(\floor{\hat{u}}-yd)\floor{u}-(yd-\floor{u})\floor{\hat{u}}\Bigr)f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)\right)\\
	=&\sum\limits_{t=i}^j\left(\Bigl(yd\floor{\hat{u}}-yd\floor{u}-\floor{u}\floor{\hat{u}}+yd\floor{u}-yd\floor{\hat{u}}+\floor{u}\floor{\hat{u}}\Bigr)f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)\right)\\
	=&\sum\limits_{t=i}^j\left(0\cdot f\Bigl(\frac{\lambda_t}{\floor{u}}\Bigr)\right)=0
\end{align*}
which finishes the case.

By iteratively applying above procedure to $\mx$, starting with $i=1$ and stopping with $j=T+1$, we obtain a $B$-restricted schedule $\mx'$ that satisfies $\costs(\mx')\le y\costs(\mx)$.
\end{proof}


\begin{algorithm}[H]
  \caption{Linear $(1+\beps)$-competitive offline scheduling}
  \label{alg:approx_1_eps_offline_linear}
  \begin{algorithmic}[1]
  \Function{$(1+\beps)$\_optimal\_offline\_scheduling}{$m,T,\Lambda,\beta,f,\beps$}
	  \Let{$(C,P)$}{\Call{shortest\_paths}{$m,T,\Lambda,\beta,f,1+\beps$}}
	  \Let{$\mx$}{\Call{extract\_schedule}{$P,T,m,1+\beps$}}
	  \State \Return{$\mx$}
  \EndFunction
  \Statex
  \Function{node}{$i,m,y$}
	  \State \Return{$\min\bigl\{m,\floor{y^{i-1}}\bigr\}$}
  \EndFunction
  \algstore{lin2opt}
  \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
  \begin{algorithmic}[1]
  \algrestore{lin2opt}
  \Function{shortest\_paths}{$m,T,\Lambda,\beta,f,y$}
	\Let{$b$}{$\ceil{\log_y(m)}$}
	\LineComment{Allocate nodes' costs and predecessor tables}
	\Blet{$C[0\dotso b+1,1\dotso T]$ and $P[0\dotso b+1,1\dotso T]$}{new tables}
	\Let{$P[b+1,1]$}{$b+1$ and $C[b+1,1]\gets\costs(0,m,\lambda_1)$}\Comment{Initialize first node in first layer}
	\For{$i \gets b \textrm{ to } 0$}\Comment{Initialize first layer (downward minimization step)}
		\If {$C[i+1,1]<\costs\bigl(0,\Call{node}{i,m,y},\lambda_1\bigr)$}
			\Let{$P[i,1]$}{$P[i+1,1]$ and $C[i,1]\gets C[i+1,1]$}
		\Else
			\Let{$P[i,1]$}{$i$ and $C[i,1]\gets\costs\bigl(0,\Call{node}{i,m,y},\lambda_1\bigr)$}
		\EndIf
	\EndFor
	\For{$t \gets 1 \textrm{ to } T-1$}\Comment{Iterative calculate costs and predecessors}
		\For{$i \gets 1 \textrm{ to } b+1$}\Comment{Upward minimization step}
			\If {$C[i-1,t]+\beta\bigl(\Call{node}{i,m,y}-\Call{node}{i-1,m,y}\bigr)<C[i,t]$}
			    \Let{$P[i,t]$}{$P[i-1,t]$}
			    \Let{$C[i,t]$}{$C[i-1,t]+\beta\bigl(\Call{node}{i,m,y}-\Call{node}{i-1,m,y}\bigr)$}
			\EndIf
		\EndFor
		\Let{$P[b+1,t+1]$}{$b+1$ and $C[b+1,t+1]\gets C[b+1,t]+\opcosts(m,\lambda_{t+1})$}
		\For{$i \gets b \textrm{ to } 0$}\Comment{Downward minimization step}
			\If {$C[i+1,t+1]<C[i,t]+\opcosts\bigl(\Call{node}{i,m,y},\lambda_{t+1}\bigr)$}
				\Let{$P[i,t+1]$}{$P[i+1,t+1]$ and $C[i,t+1]\gets C[i+1,t+1]$}
			\Else
				\Let{$P[i,t+1]$}{$i$ and $C[i,t+1]\gets C[i,t]+\opcosts\bigl(\Call{node}{i,m,y},\lambda_{t+1}\bigr)$}
			\EndIf
		\EndFor
	\EndFor
	\State \Return{$(C,P)$}
  \EndFunction
  \Statex
  \Function{extract\_schedule}{$P,T,m,y$}
	\Blet{$\mx[1\dotso T]$}{a new array}
	\Let{$i$}{$P[0,T]$}\Comment{Get index of best choice for last time slot}
    	\Let{$\mx[T]$}{$\Call{node}{i,m,y}$}\Comment{Calculate best choice for last time slot}
        \For{$t \gets T-1 \textrm{ to } 1$}\Comment{Iteratively obtain a schedule by using the predecessors}
		\Let{$i$}{$P[i,t]$}
		\Let{$\mx[t]$}{$\Call{node}{i,m,y}$}
	\EndFor
	\State \Return{$\mx$}
  \EndFunction
  \end{algorithmic}
\end{algorithm}

