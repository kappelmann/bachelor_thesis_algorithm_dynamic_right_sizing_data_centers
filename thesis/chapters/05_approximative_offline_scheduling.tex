% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Approximative Offline Scheduling}\label{chap:approx_offline_scheduling}
Heretofore, we have derived two optimal offline algorithms for our scheduling problem. Unfortunately, the algorithms' time complexities are exponential in the input size of the number of servers $m$. Needless to say, we want to reduce this exponential runtime. For this, we must slightly loosen our aspirations, that is we move to approximative methods. 
Further, in the course of the next section, we will see that we need to assume that our convex operating cost function $f$ is non-negative and monotonically increasing; however, this restriction is of no great significance in practice, as will be discussed later.

In this chapter, we will first modify our algorithm derived in Section~\ref{sec:opt_offline_pseudo_lin} to obtain a 2-optimal offline algorithm with linear time complexity. To clarify, given a number $y\in\mathbb{R}$, we say that a scheduling algorithm is \emph{$y$-optimal} if its calculated solution incurs at most $y$ times as much cost as an optimal solution does. Similarly, we say that a schedule/operation is \emph{$y$-approximative} if its cost is at most $y$ times as much as much as the original schedule's/operation's cost.
As a final step of this thesis, we will generalize the \makebox{2-optimal} algorithm to derive an $(1+\beps)$-optimal algorithm with linear time complexity.

\section{A 2-Optimal Linear-Time Algorithm}\label{sec:approx_2_opt}
Recall our algorithm and its corresponding graph $G$ derived in Section~\ref{sec:opt_offline_pseudo_lin}. The algorithm's time complexity of $\Theta(Tm)$ is determined by the number of nodes and edges of $G$. Since we desire to reduce our runtime complexity, we need to reduce the number of nodes and edges in $G$. In particular, we must get rid of the factor $m$. This factor is a consequence of the ``height'' of our graph, i.e.\ the number of nodes in each layer. Therefore, we have to ``thin out'' $G$ by reducing its number of nodes in each layer.

As we saw in Equation~\eqref{eq:inp_size}, the size of our input $\inp$ is given by $\mathcal{O}\bigl(T\log_2(m)+\log_2(\beta)\bigr)$. Consequently, in order to obtain a linear time complexity, we want to reduce the graph's height from $m+1$ to a logarithmic height of $\mathcal{O}\bigl(\log(m)\bigr)$. Given this observation, it seems natural for a computer scientist to choose a logarithmic scale for the number of servers in each layer, to wit, instead of adding a node for each possible number of active servers (i.e.\ $0,1,\dotsc,m$), we only add nodes for logarithmic choices (i.e.\ $0,2^0,2^1,\dotsc,2^{\floor{\log_2(m)}},m$). More formally, given a problem instance $\inp$, we set
\begin{align*}
	b\coloneqq\floor{\log_2(m)}&&\text{and}&&B\coloneqq\{0,2^0,2^1,\dotsc,2^b,m\}
\end{align*}
where $B$ will subsequently represent the set of possible scheduling choices at each time step. Using this set of possible choices, we can then consider the following adaption of our former graph:
\begin{align*}
	V&\coloneqq\bigl\{v_{x,t\downarrow}\mid x\in B,t\in[T]\bigr\}\dotcup\bigl\{v_{x,t\uparrow}\mid x\in B, t\in[T-1]\bigr\}\dotcup\{v_{0,0}\}\\
	E_s&\coloneqq\bigl\{(v_{0,0},v_{x,1\downarrow})\mid x\in B\bigr\}\\
	E_\downarrow&\coloneqq\bigl\{(v_{2^i,t\downarrow},v_{2^{i-1},t\downarrow})\mid i\in[b],t\in[T]\bigr\}\dotcup\bigl\{(v_{2^0,t\downarrow},v_{0,t\downarrow})\mid t\in[T]\bigr\}\\
	&\phantom{{}\coloneqq{}}\dotcup\bigl\{(v_{m,t\downarrow},v_{2^b,t\downarrow})\mid t\in[T]\bigr\}\\
	E_\uparrow&\coloneqq\bigl\{(v_{2^{i-1},t\uparrow},v_{2^i,t\uparrow})\mid i\in[b],t\in[T-1]\bigr\}\dotcup\bigl\{(v_{0,t\uparrow},v_{2^0,t\uparrow})\mid t\in[T-1]\bigr\}\\
	&\phantom{{}\coloneqq{}}\dotcup\bigl\{(v_{2^b,t\uparrow},v_{m,t\uparrow})\mid t\in[T-1]\bigr\}\\
	E_{\downarrow\uparrow}&\coloneqq\bigl\{(v_{x,t\downarrow},v_{x,t\uparrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E_{\uparrow\downarrow}&\coloneqq\bigl\{(v_{x,t\uparrow},v_{x,t+1\downarrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E&\coloneqq E_s\dotcup E_\downarrow\dotcup E_\uparrow\dotcup E_{\downarrow\uparrow}\dotcup E_{\uparrow\downarrow}\\
	c_G(e)&\coloneqq
	\begin{cases}
		\costs(0,x,\lambda_1), & \text{if $e=(v_{0,0},v_{x,1\downarrow})\in E_s$}\\
		\opcosts(x,\lambda_{t+1}), & \text{if $e=(v_{x,t\uparrow},v_{x,t+1\downarrow})\in E_{\uparrow\downarrow}$}\\
		\beta(x'-x), & \text{if $e=(v_{x,t\uparrow},v_{x',t\uparrow})\in E_\uparrow$}\\
		0, & \text{if $e\in(E_\downarrow\dotcup E_{\downarrow\uparrow})$}
	\end{cases}\\
	G&\coloneqq(V,E,c_G)
\end{align*}
If $m$ is a power of two, i.e.\ $m=2^b$, it happens that we add unnecessary loops in~$E_\downarrow$ and~$E_\uparrow$ with cost $0$, which we can simply ignore/discard for our following work.
A graphical representation of $G$ can be found in Figure~\ref{fig:graph_lin_approx_2}.
\begin{figure}[ht]
\includestandalone[width=\textwidth]{../figures/graph_lin_approx_2}
\caption{Graph for a 2-optimal linear-time offline algorithm; the path of the topological sorting is highlighted in red.}
\label{fig:graph_lin_approx_2}
\end{figure}
The nodes' and edges' semantical meaning and the graph's working principle stays similar to that given in Section~\ref{sec:opt_offline_pseudo_lin}. Again, by following the topological sorting, we can work our way through the graph to calculate the shortest paths, ultimately reaching the destination $v_{0,T\downarrow}$. However, since some possible scheduling choices are not representable in this new graph, we may just obtain approximative costs for our nodes. Thus, the shortest path in our graph might not correspond to an optimal schedule, but it will at least correspond to an approximative one. Before we start to establish the graph's approximation guarantee, we first have to conduct some observations. We start by making a convenient definition that helps us to identify schedules that are representable in our graph.
\begin{defn}[Restricted schedules]
Given an input $\inp$ and a set $A\subseteq[m]_0$, we say that a schedule $\mx=(x_1,\dotsc,x_T)$ is \emph{$A$-restricted} if $\mx$ only uses scheduling choices contained in~$A$, that is $\mx$ satisfies the formula $\forall t\in[T]:x_t\in A$.
\end{defn}
Evidently, our graph is able to represent every $B$-restricted schedule. We now examine the incurring operating costs of such a $B$-restricted schedule $\mx'$. Since we are forced to schedule a number of servers contained in $B$, we might not be able to choose an optimal scheduling choice that minimizes the schedule's operating costs. Instead, we may choose the nearest scheduling choice which is contained in $B$. For instance, if the optimal scheduling strategy at some time slot $t$ would be to choose $x_t=3$ servers (which is not a power of two), we may instead have to choose $x_t'=4\in B$ servers for $\mx'$. One might suspect that this strategy would incur at most twice as much operating costs as an optimal schedule does. This, however, is sadly not the case, as one can see in the following example.
\begin{exmpl}
Let $\inp=\bigl(m=4,T=5,\Lambda=(3,3,3,3,3),\beta=0,f\bigr)$ be the input for a problem instance where $f(\lambda)=(\lambda-1)^2$. Since we need at least 3 active servers at any time slot, any $B$-restricted schedule $\mx'=(x_1',\dotsc,x_5')$ forces us to constantly use $x_t'=4$ active machines. An optimal schedule $\mx=(x_1,\dotsc,x_5)$, on the other hand, is able to minimize its cost by constantly scheduling $x_t=3$ servers. Let us compare the costs between $\mx'$ and $\mx$. The schedules' costs are given by
\begin{align*}
	\costs(\mx)&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_tf(\lambda_t/x_t)}^{3(3/3-1)^2}+\overbrace{\swcosts(x_{t-1},x_t)}^{0\max\{\,\cdots\}}\bigr)=5\cdot3\left(\frac{3}{3}-1\right)^2=0\\
	\costs(\mx')&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_t'f(\lambda_t/x_t')}^{4(3/4-1)^2}+\overbrace{\swcosts(x_{t-1}',x_t')}^{0\max\{\,\cdots\}}\bigr)=5\cdot4\left(\frac{3}{4}-1\right)^2=\frac{20}{16} 
\end{align*}
Albeit our approximative schedule uses only one server in addition, the schedule's cost is already inestimably higher than that of an optimal schedule $\mx$, preventing any sensible approximation estimation. Naturally, we may ask ourselves how this explosion of costs is even possible. Evidently, the switching costs are not the root of this explosion since $\beta=0$. Thus, we shall take a closer look on the used operating cost function. The optimal schedule~$\mx$ evenly distributes every load $\lambda_t=3$ to $x_t=3$ servers. Hence, every active server has to process a load of $\lambda_t/x_t=1$ at every time step, incurring costs of $f(1)=0$. On the other hand, the approximative schedule $\mx'$ is able to distribute every load to 4 active machines. Thus, every machine incurs costs of $f(3/4)=\frac{1}{16}$. This observation seems rather surprising: Although every server has to process a smaller load using $\mx'$, the incurring operating costs of each server turn out to be higher. Intuitively, however, we would expect that a less stressed machine would incur less costs. This surprising behavior is due to the fact that our operating cost function $f$ is not monotonically increasing, as one can see in the following figure.
\begin{figure}[H]
\centering
\includestandalone{../figures/non_mono_incr_f_1}
\caption{Example of a non monotonically increasing operating cost function \makebox{$f(\lambda)=(\lambda-1)^2$}, where smaller loads incur higher costs.}
\label{fig:non_mono_incr_f}
\end{figure}
\end{exmpl}
The above example shows us that our graph may not able to deliver a sensible approximation when dealing with general convex operating cost functions $f$. Luckily, this inconvenience can be solved by additionally assuming that $f$ is non-negative and monotonically increasing. To see this, assume that at some time slot $t$ the scheduling choice $x_t$ minimizes the operating costs to process the load $\lambda_t$. Then let $x_t'\in B$ the next scheduling choice representable in~$G$. Since $x_t$ minimizes the operating costs at time slot $t$, we have
\begin{equation*}
	\opcosts(x_t,\lambda_t)\le\opcosts(x_t',\lambda_t)\stackrel{\eqref{eq:mx_schedule_op_costs}}{=}x_t'f(\lambda_t/x_t')
\end{equation*}
Further, since $B$ contains all powers of two up to $m$, we have $x_t\le x_t'\le 2x_t$. If we additionally assume that $f$ is non-negative, we can infer that
\begin{equation*}
	x_t'f(\lambda_t/x_t')\le2x_tf(\lambda_t/x_t')
\end{equation*}
Now, using the fact that $x_t\le x_t'$ and assuming that $f$ is monotonically increasing, we can see that
\begin{equation*}
	2x_tf(\lambda_t/x_t')\le 2x_tf(\lambda_t/x_t)\stackrel{\eqref{eq:mx_schedule_op_costs}}{=}2\opcosts(x_t,\lambda_t)
\end{equation*}
Ultimately, we can combine our observations and conclude
\begin{equation*}
	\opcosts(x_t,\lambda_t)\le\opcosts(x_t',\lambda_t)\le2\opcosts(x_t,\lambda_t)
\end{equation*}
which shows us that our approximative scheduling choice incurs at most twice as much operating costs as an optimal scheduling strategy does. We thus subsequently restrict ourselves to non-negative, monotonically increasing convex cost functions. This evidently reduces the theoretical generality of our initial approach, but it does not interfere with practical applicability. On the one hand, negative cost functions would semantically allow to ``generate profit by consuming energy'', which seems unreasonable in practice. On the other hand, if we have a non monotonically increasing convex cost function, we can simply add artificial loads to our machines to reduce our costs in given circumstances. For instance, in our previous example, we could assign every machine a load of $1$ instead of $\frac{3}{4}$ to reduce the approximative schedule's cost. This trick, which is exemplarily outlined in Figure~\ref{fig:transform_to_mono_incr}, allows us to transform any arbitrary convex cost function to a monotonically increasing one.
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.5\textwidth}
\includestandalone[width=\textwidth]{./../figures/non_mono_incr_f_2}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.5\textwidth}
\includestandalone[width=\textwidth]{../figures/mono_incr_f}
\end{subfigure}
\caption{The non monotonically increasing convex function $f$ can be transformed to the monotonically increasing convex function $f'$ by adding artificial loads $\lambda^+$ to assignments $\lambda\in[0,0.5)$ such that we obtain a new assignment $\lambda'\coloneqq\lambda+\lambda^+=0.5$.}
\label{fig:transform_to_mono_incr}
\end{figure}
Next, we examine the incurring switching costs of a $B$-restricted schedule $\mx'$. Again, given an optimal scheduling choice $x_t$, we use the idea to choose the nearest scheduling choice $x_t'$ that is contained in $B$ to construct $\mx'$. Once more, one might hope that $\mx'$ would incur at most twice as much switching costs as an optimal schedule. Needless to say, the next example dashes this hope.
\begin{exmpl}\label{exmpl:oscillating_schedule}
Let $\inp=\bigl(m=16,T=5,\Lambda=(9,7,9,7,9),\beta=1,f\bigr)$ be the input for a problem instance where $f(\lambda)=0$. Our possible scheduling choices are then given by $B=\{0,1,2,4,8,16\}$, and one optimal schedule is given by $\mx=(9,7,9,7,9)$. The $B$-restricted schedule corresponding to $\mx$ is then given by $\mx'=(16,8,16,8,16)$. The schedules' costs amount to
\begin{align*}
	\costs(\mx)&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_tf(\lambda_t/x_t)}^{x_t\cdot 0}+\overbrace{\swcosts(x_{t-1},x_t)}^{\max\{0,x_t-x_{t-1}\}}\bigr)=9+0+2+0+2=13\\
	\costs(\mx')&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_t'f(\lambda_t/x_t')}^{x_t'\cdot 0}+\overbrace{\swcosts(x_{t-1}',x_t')}^{\max\{0,x_t'-x_{t-1}'\}}\bigr)=16+0+8+0+8=32
\end{align*}
which shows that $\mx'$ is not 2-approximative. Of course, we are again curious about how this cost explosion is possible. Since we set $f(\lambda)=0$, our servers do not incur operating costs, which means that the cost explosion must be due to the increased switching costs of $\mx'$. The problem in this case is the oscillating behavior of $\mx$ around a power of two (namely $8=2^3$), as one can see in the following figure. 
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{./../figures/switching_costs_not_2_opt_1}
	\caption{\underline{Optimal schedule $\mx$:} Note how the schedule oscillates around 8 (a power of two).}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{../figures/switching_costs_not_2_opt_2}
	\caption{\underline{Approximative schedule $\mx'$:} The approximative schedule is restricted to powers of two.}
\end{subfigure}
\caption{Comparison between an optimal schedule and its approximative counterpart. The approximative schedule incurs more than twice as much switching costs.}
\label{fig:adaption-schedule}
\end{figure}
\end{exmpl}
So, is this the end of our hunt for a 2-optimal algorithm? No, certainly not! Although our naive approach was to no avail, there is indeed a better $B$-restricted schedule for our example. Instead of following the optimal schedule's oscillation, we can simply use a schedule that stays put during these oscillating steps, namely the schedule $\mx'=(16,16,16,16,16)$ with cost $\costs(\mx')=16$. Obviously, this seems like a rather trivial example since we set $f(\lambda)=0$, and hence we do not need to worry about the new schedule's operating costs. However, it indeed turns out that making the right choice between following the optimal schedule's oscillation and staying put will always allow us to acquire a 2-optimal solution. This observation will be a key part of the next lemma's proof. 

In the proof, we are going to divide a schedule $\mx$ into periods at which its plot crosses a power of two. We then show that every such period can be transformed to a period in a $B$-restricted schedule $\mx'$ such that the transformed period incurs at most twice as much costs. In order to formalize how to exactly split our schedules, we make a handy definition.
\begin{defn}[2-state changes]
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule and $t\in[T+1]$. We say that $\mx$ changes its \emph{2-state} at time $t$ if $\mx$ satisfies the formula
\begin{equation*}
	x_{t-1}\neq x_t\land \left(x_{t-1}=0 \lor x_t\notin\left[2^{\floor{\log_2(x_{t-1})}},2^{\floor{\log_2(x_{t-1})}+1}\right)\right)
\end{equation*}
\end{defn}
For example, the schedule $\mx=(5,4,8,7,8,15,10,16)$ changes its 2-state at times \makebox{$t\in\{1,3,4,5,8,9\}$}. As one can see in Figure~\ref{fig:schedule_2_states}, we can say that $\mx$ changes its 2-state if its plot ascends and touches a next higher power of two or descends and leaves the current pair of powers of two. Using this notion of 2-state changes, we are geared up to deal with the next lemma -- the main work of this section.
\begin{figure}[H]
\centering
\includestandalone[width=0.7\textwidth]{../figures/schedule_2_states}
\caption{Plot of the schedule $\mx=(5,4,8,7,8,15,10,16)$ and its 2-state changes}
\label{fig:schedule_2_states}
\end{figure}
\begin{lem}\label{lem:transform_schedule_approx_2}
Let $\mx$ be a schedule for $\inp$. There exists a $B$-restricted schedule $\mx'$ satisfying \makebox{$\costs(\mx')\le2\costs(\mx)$}.
\end{lem}
\begin{proof}
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule for $\inp$. We need to construct a $B$-restricted schedule $\mx'=(x_1',\dotsc,x_T')$ such that \makebox{$\costs(\mx')\le2\costs(\mx)$}.
First, if $\mx$ is not feasible, we have $\costs(\mx)=\infty$, and thus any arbitrary $B$-restricted schedule $\mx'$ satisfies $\costs(\mx')\le2\costs(\mx)$; hence, assume that $\mx$ is feasible. Next, we notice that if $\mx$ shuts down all servers at some time slot $t\in[T]$ (i.e.\ $x_t=0$), we can split $\mx$ into two subschedules \makebox{$\mx_1\coloneqq(x_1,\dotsc,x_{t-1})$} and \makebox{$\mx_2\coloneqq(x_{t+1},\dotsc,x_T)$}. It then suffices to prove the claim for $\mx_1$ and $\mx_2$, since we can then construct the 2-approximative schedule by setting $x_t'\coloneqq 0$ and $\mx'\coloneqq(\mx_1',x_t',\mx_2')$. Thus, by recursively applying this method, we can reduce our proof to a list of subschedules $\mx_1,\dotsc,\mx_N$ that never shut down all servers. Consequently, without loss of generality, we subsequently assume that $\mx$ never powers down all servers, that is $x_t>0$ for all $t\in[T]$. 
	
Next, we show that we can iteratively construct $\mx'$ by transforming every period between two 2-state changes of $\mx$ to a 2-approximative period in $\mx'$. 
To prove that our transformations will be 2-approximative, we have to conduct an amortized analysis for the switching costs of $\mx'$ using the \emph{accounting method}. The basic idea of the accounting method is to overcharge some operations and to save the excess charge as a \emph{credit}, which can be used to compensate for subsequent, more expensive operations. An introduction about the accounting method can be found in~\parencite[Section~17.2]{intro-algo}. To see the necessity of such an amortized analysis, and to get a basic idea about its working principle, consider the schedule $\mx=(7,9)$ and its approximative counterpart $\mx'=(8,16)$. Although the total switching costs of $\mx'$ are 2-approximative, the individual switching steps of $\mx'$ are not, since $\mx'$ has to turn on $8$ machines at time $t=2$ while $\mx$ only turns on $2$ machines. However, at time $t=1$, $\mx'$ only turns on $8$ servers while it would be allowed to turn on $2\cdot7=14$. We can thus overcharge the first switching operation and use the excess charge $14-8=6$ as a credit to compensate for the second switching step, which exactly misses these $6$ credit points.

For our iterative construction, let $i$ and $j+1$ with \makebox{$i,j\in[T]$} and $i\le j$ be the first unprocessed time slots at which $\mx$ changes its \makebox{2-state}. To conveniently refer to the schedules' periods between $i$ and $j$, we define the subschedules \makebox{$\mx_{i,j}\coloneqq(x_i,\dotsc,x_j)$} and $\mx'_{i,j}\coloneqq(x_i',\dotsc,x_j')$. We then have to show that $\mx_{i,j}$ can be transformed to $\mx_{i,j}'$ with 2-approximative costs. Note that the periods are consecutive, i.e.\ after processing the period $[i,j]$, the next pair of indices $i',j'$ will be chosen such that $j+1=i'$. To conduct the transformations, we will need to refer to the lower and upper bound of~$\mx_{i,j}$, namely
\begin{flalign*}
	&&l\coloneqq2^{\floor{\log_2(x_i)}}&&\text{and}&&u\coloneqq\min\bigl\{2l,m\bigr\}&&&
\end{flalign*}
as well as to the lower and upper bound of $\mx$ at time $j+1$:
\begin{flalign*}
	&&l'\coloneqq\begin{cases}
		2^{\floor{\log_2(x_{j+1})}}, & \text{if $x_{j+1}\neq 0$}\\
		0, & \text{if $x_{j+1}=0$}
	\end{cases}
&&\text{and}&&u'\coloneqq\min\bigl\{2l',m\bigr\}&&&
\end{flalign*}
Note that $u,u'\in B$ and that $x_t\le u\le 2x_t$ holds for any $i\le t\le j$ since $\mx$ does not change its 2-state between $i$ and $j$.

As an \emph{invariant} of the following transformations, we are going to ensure that $\mx'$ can potentially move to $u$ at time $i$ in a 2-approximative manner, i.e.\ we ensure that~$\mx'$ incurs at most twice as much switching costs as $\mx$ if we set $x_i'\coloneqq u$ -- whether this step will be taken in the end or not. Further, we are going to ensure that $x_t'\ge u$ holds for any $i\le t\le j$ after every transformation step. This guarantees that $\mx'$ will be feasible, since $x_t\le u$. Moreover, we show that the credit of our amortized analysis will be greater than or equal to $\beta(2x_{j+1}-u')$ if $u'\neq m$ and $\mx$ exits~$[l,u)$ by ascending to $[l',u')$ at time $j+1$. Since the periods are consecutive, this equivalently means that the credit will be greater than or equal to $\beta(2x_i-u)$ if $u\neq m$ and $\mx$ enters~$[l,u)$ at time~$i$ from below.
	
First, we have to check that our invariant initially holds. For the initial start-up process \makebox{(i.e.\ $i=1$)}, we can simply move to the next power of 2 larger than $x_1$ contained in $B$, that is we set $x_1'\coloneqq u\in B$. Since we know that $x_1'\le2x_1$, we can conclude that $\beta x_1'\le\beta 2x_1$, which shows that $\mx'$ has 2-approximative start-up switching costs. Moreover, we can use the difference of switching costs $\beta(2x_1-x_1')=\beta(2x_1-u)$ for the credit of our amortized analysis. Thus, the invariant initially holds. 

Now, let us have a closer look on the possible behaviors of $\mx$ between its 2-state changes. The behaviors can be classified based on how $\mx$ enters and leaves the interval $[l,u)$. We have to consider four different cases:
\begin{enumerate}[label=(\alph*)]
	\item $\mx$ enters $[l,u)$ from below and then descends to $[l',u')$.\label{itm:schedule_behavior_up_down}
	\item $\mx$ enters $[l,u)$ from above and then descends to $[l',u')$.\label{itm:schedule_behavior_down_down}
	\item $\mx$ enters $[l,u)$ from below and then ascends to $[l',u')$.\label{itm:schedule_behavior_up_up} 
	\item $\mx$ enters $[l,u)$ from above and then ascends to $[l',u')$.\label{itm:schedule_behavior_down_up} 
\end{enumerate}
To finish the proof, we show that any case can be transformed to a 2-approximative period in $\mx'$ while ensuring that our invariant will not be violated. Additionally, we have to verify that the credit of our amortized analysis stays non-negative.
	
We begin with cases~\ref{itm:schedule_behavior_up_down} and~\ref{itm:schedule_behavior_down_down}, which are both depicted in Figure~\ref{fig:schedule_behavior_down}.
\begin{figure}[ht]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_up_down}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_down}
\end{subfigure}
\caption{The original schedule comes from below (case~\ref{itm:schedule_behavior_up_down}) or above (case~\ref{itm:schedule_behavior_down_down}), stays between $[l,u)$, and then descends to $[l',u')$. The approximative schedule stays put at $u$ for time slots $i\le t\le j$.}
\label{fig:schedule_behavior_down}
\end{figure}
Due to our invariant, we know that we can set $x_i'\coloneqq u$ with 2-approximative switching costs. Consequently, setting $x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq u\in B$ gives us a strategy with 2-approximative switching costs between $i$ and $j$. Further, since $x_t\le u\le2x_t$ for any $i\le t\le j$, and $f$ is non-negative and monotonically increasing, the operating costs of $\mx'_{i,j}$ can be estimated by
\begin{equation*}
	\opcosts\bigl(\mx_{i,j}'\bigr)\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}\sum\limits_{t=i}^j\left(uf\left(\frac{\lambda_t}{u}\right)\right)\le\sum\limits_{t=i}^j\left(2x_tf\left(\frac{\lambda_t}{u}\right)\right)\le2\sum\limits_{t=i}^j\left(x_tf\left(\frac{\lambda_t}{x_t}\right)\right)\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}2\opcosts\bigl(\mx_{i,j}\bigr)
\end{equation*}
Thus, the operating costs of $\mx_{i,j}'$ are 2-approximative. Further, since $u'<u$, we do not need to account for additional switching costs to satisfy our invariant at time $j+1$; however, we want to stress that one cannot tell at this step if we should indeed set $x_{j+1}\coloneqq u'$ (c.f. Figure~\ref{fig:schedule_behavior_down_up} and its related case). Lastly, we note that the credit of our amortized analysis stays untouched in both cases, i.e.\ the credit stays non-negative.
	
Next, we consider case~\ref{itm:schedule_behavior_up_up}, which is illustrated in Figure~\ref{fig:schedule_behavior_up_up}.
\begin{figure}[ht]
\centering
\includestandalone[width=0.5\textwidth]{../figures/schedule_behavior_up_up}	
\caption{The original schedule (case~\ref{itm:schedule_behavior_up_up}) comes from below, stays between $[l,u)$, and then ascends to $[l',u')$. The approximative schedule stays put at $u$ for time slots $i\le t\le j$.}
\label{fig:schedule_behavior_up_up}
\end{figure}
Again, due to our invariant, we know that setting $x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq u\in B$ gives us a strategy with \makebox{2-approximative} switching costs. Moreover, as in the previous case, the operating costs of~$\mx_{i,j}'$ are \makebox{2-approximative}. To verify the invariant at time $j+1$, we have to show that $\mx'$ can potentially move to $u'$ at time $j+1$ with 2-approximative costs, and, since $\mx$ ascends at time $j+1$, we also need to account for the new credit required by the invariant. To compensate for these costs, we notice that $\mx$ has to power on at least $x_{j+1}-x_i$ servers between $i$ and $j+1$. Further, as $\mx$ ascends at time $i$, we can use the old credit guaranteed by our invariant to compensate for the costs. Putting it all together, we have
\begin{align*}
	&2\overbrace{\beta(x_{j+1}-x_i)}^{\text{sw.\ costs }\mx}+\overbrace{\beta(2x_i-u)}^{\text{old credit}}-\overbrace{\beta(u'-u)}^{\text{sw.\ costs }\mx'}-\overbrace{\beta(2x_{j+1}-u')}^{\text{new credit}}\\
	=\quad&\beta(2x_{j+1}-2x_i+2x_i-u-u'+u-2x_{j+1}+u')=0
\end{align*}
that is, the cost difference is non-negative, and thus our invariant at time $j+1$ is satisfied.
	
Finally, we have to consider case~\ref{itm:schedule_behavior_down_up}. As we have seen in Example~\ref{exmpl:oscillating_schedule}, this case turns out to be slightly more complicated, since simply following an oscillating behavior of $\mx$ can lead to a cost explosion for $\mx'$. Nevertheless, we can solve this issue by considering two different strategies for $\mx_{i,j}'$, namely
\begin{flalign*}
	&&\mx_{i,j}^u&\coloneqq\bigl(x_i^u,\dotsc,x_j^u\bigr),&&\text{where}&x_t^u\coloneqq u,\,\text{for } i\le t\le j&&&&\\
	&&\text{and}\quad\mx_{i,j}^{\hat{u}}&\coloneqq\bigl(x_i^{\hat{u}},\dotsc,x_j^{\hat{u}}\bigr),&&\text{where}&x_t^{\hat{u}}\coloneqq \hat{u},\,\text{for } i\le t\le j&&&&
\end{flalign*}
with $\hat{u}\coloneqq\min\{2u,m\}\in B$. Both strategies are illustrated in Figure~\ref{fig:schedule_behavior_down_up}.
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_1}
\caption{Strategy $\mx_{i,j}^u$ stays put at $u$ for $i\le t\le j$.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_2}
\caption{Strategy $\mx_{i,j}^{\hat{u}}$ stays put at $\hat{u}$ for $i\le t\le j$.}
\end{subfigure}
\caption{The original schedule (case~\ref{itm:schedule_behavior_down_up}) comes from above, stays between $[l,u)$, and then rises to $[l',u')$. The approximative schedule has two different possibilities.}
\label{fig:schedule_behavior_down_up}
\end{figure}
Due to our invariant and the fact that $\mx$ descends at time $i$, we know that $x_{i-1}'\ge \hat{u}\ge u$, and thus both strategies do no incur switching costs up to time $j$ -- we can simply move down from $x_{i-1}'$ to $\hat{u}$ or $u$.
We are now going to prove that either the cost of $\mx_{i,j}^u$ or of $\mx_{i,j}^{\hat{u}}$, including costs to satisfy our invariant at time $j+1$, must be 2-approximative. 
First, we examine the switching costs of $\mx$ from $i$ up to $j+1$. To do so, we set $d\coloneqq\min\{x_t\mid i\le t\le j\}$ to refer to the smallest number of active servers used by $\mx_{i,j}$. Then, since $\mx$ has to power on at least $x_{j+1}-d$ servers between $i$ and $j+1$, we can give a lower bound for its switching costs:
\begin{equation*}
	\sum\limits_{t=i+1}^{j+1}\swcosts(x_{t-1},x_t)\ge\beta(x_{j+1}-d)
\end{equation*}
The next idea is to split $\mx_{i,j}$ and its associated switching costs at the final switching step into two parts; more precisely, we split $\beta(x_{j+1}-d)$ into $\beta(x_{j+1}-u)$ and $\beta(u-d)$. It then suffices to show that either $\mx_{i,j}^u$ or $\mx_{i,j}^{\hat{u}}$ can process the loads $\lambda_i,\dotsc,\lambda_j$ and eventually switch to $\hat{u}$ with 2-approximative costs under the assumption that $\mx$ only moves to $u$ as a first step. This is because the remaining switching costs $\beta(x_{j+1}-u)$ of $\mx$ already suffice to compensate for the remaining switching costs $\beta(u'-\hat{u})$ of $\mx'$ and the costs to satisfy the invariant:
\begin{align*}
	2\beta(x_{j+1}-u)-\beta(u'-\hat{u})-\overbrace{\beta(2x_{j+1}-u')}^{\text{new credit}}&=\beta(2x_{j+1}-2u-u'+\hat{u}-2x_{j+1}+u')\\
	&=\beta(\hat{u}-2u)=0
\end{align*}
where we assumed that $\hat{u}=2u$ holds; otherwise, we have $u'=\hat{u}=m$ and thus do not need to account for the new credit to satisfy our invariant:
\begin{align*}
	2\beta(x_{j+1}-u)-\beta(u'-\hat{u})=2\beta(x_{j+1}-u)-\beta(m-m)=2\beta(x_{j+1}-u)\ge 0
\end{align*}
To proceed, we notice that if $\hat{u}-u\le2(u-d)$ holds, strategy $\mx_{i,j}^u$ has 2-approximative switching costs:
\begin{equation*}
	\beta(\hat{u}-u)\le2\beta(u-d)
\end{equation*}
Further, as in the previous cases, the operating costs of $\mx_{i,j}^u$ are 2-approximative, and thus~$\mx_{i,j}^u$ is 2-approximative if $\hat{u}-u\le2(u-d)$ holds. Hence, we subsequently assume that 
\begin{align}\label{eq:proof_2_approximative_asm_dist_pos}
	\hat{u}-u>2(u-d)&&\text{or equivalently}&&\hat{u}-3u+2d>0
\end{align}
Next, by the law of excluded middle, the schedule $\mx_{i,j}^u$ is either 2-approximative, or it is not \makebox{2-approximative}. If it is 2-approximative, we are done; hence, from now on, we assume that $\mx_{i,j}^u$ is not 2-approximative, that is
\begin{equation*}
	\opcosts\bigl(\mx_{i,j}^u\bigr)+\beta(\hat{u}-u)>2\left(\opcosts\bigl(\mx_{i,j}\bigr)+\beta(u-d)\right)
\end{equation*}
We then have to show that $\mx_{i,j}^{\hat{u}}$ is 2-approximative.
First note that $\mx_{i,j}^u$ uses at most twice as many active servers as $\mx_{i,j}$, and therefore $\mx_{i,j}^u$ incurs at most twice as much operating costs as $\mx_{i,j}$. Consequently, the switching costs of $\mx_{i,j}^u$ must significantly outweigh those of $\mx_{i,j}$. Hence, it seems interesting to get an estimation for $\beta$. Rearranging the previous inequality gives us
\begin{align*}
	&&\opcosts\bigl(\mx_{i,j}^u\bigr)+\beta(\hat{u}-u)&>2\left(\opcosts\bigl(\mx_{i,j}\bigr)+\beta(u-d)\right)&\\
	&\stackrel{\phantom{\hat{u}-3u+2d)>0}}{\iff}&\beta(\hat{u}-3u+2d)&>2\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^u\bigr)&\\
	&\stackrel{\hat{u}-3u+2d>0}{\iff}&\beta&>\frac{2\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^u\bigr)}{\hat{u}-3u+2d}
\end{align*}
where the last step is justified by Assumption~\eqref{eq:proof_2_approximative_asm_dist_pos}.
This allows us to find a lower bound for the cost of $\mx_{i,j}$:
\begin{align*}
	\opcosts\bigl(\mx_{i,j}\bigr)+\beta(u-d)&\ge\opcosts\bigl(\mx_{i,j}\bigr)+\frac{2\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^u\bigr)}{\hat{u}-3u+2d}(u-d)\\
	&=\frac{(\hat{u}-3u+2d)\opcosts\bigl(\mx_{i,j}\bigr)+2(u-d)\opcosts\bigl(\mx_{i,j}\bigr)-(u-d)\opcosts\bigl(\mx_{i,j}^u\bigr)}{\hat{u}-3u+2d}\\
	&=\frac{(\hat{u}-u)\opcosts\bigl(\mx_{i,j}\bigr)-(u-d)\opcosts\bigl(\mx_{i,j}^u\bigr)}{\hat{u}-3u+2d}
\end{align*}
Next, since we want to prove that $\mx_{i,j}^{\hat{u}}$ is 2-approximative, we have to show that the following cost difference is non-negative:
\begin{align*}
	2\left(\opcosts\bigl(\mx_{i,j}\bigr)+\beta(u-d)\right)-\opcosts\bigl(\mx_{i,j}^{\hat{u}}\bigr)
\end{align*}
Recall that $\mx_{i,j}^{\hat{u}}$ does not incur switching costs to move to $\hat{u}$, and thus we only have to consider its operating costs.
We can now use our just derived lower bound for \makebox{$\opcosts\bigl(\mx_{i,j}\bigr)+\beta(u-d)$} to estimate the difference:
\begin{align*}
	2\left(\opcosts\bigl(\mx_{i,j}\bigr)+\beta(u-d)\right)-\opcosts\bigl(\mx_{i,j}^{\hat{u}}\bigr)\ge2\,\frac{(\hat{u}-u)\opcosts\bigl(\mx_{i,j}\bigr)-(u-d)\opcosts\bigl(\mx_{i,j}^u\bigr)}{\hat{u}-3u+2d}-\opcosts\bigl(\mx_{i,j}^{\hat{u}}\bigr)
\end{align*}
Thus, it suffices to show that
\begin{align*}
	2\,\frac{(\hat{u}-u)\opcosts\bigl(\mx_{i,j}\bigr)-(u-d)\opcosts\bigl(\mx_{i,j}^u\bigr)}{\hat{u}-3u+2d}-\opcosts\bigl(\mx_{i,j}^{\hat{u}}\bigr)\ge 0
\end{align*}
which, using Assumption~\eqref{eq:proof_2_approximative_asm_dist_pos}, that is $\hat{u}-3u+2d>0$, can be simplified to
\begin{equation*}
	2(\hat{u}-u)\opcosts\bigl(\mx_{i,j}\bigr)-2(u-d)\opcosts\bigl(\mx_{i,j}^u\bigr)-(\hat{u}-3u+2d)\opcosts\bigl(\mx_{i,j}^{\hat{u}}\bigr)\ge 0
\end{equation*}
To show this inequality, we have to take a closer look on the schedules' operating costs:
\begin{align*}
	&&&2(\hat{u}-u)\opcosts\bigl(\mx_{i,j}\bigr)-2(u-d)\opcosts\bigl(\mx_{i,j}^u\bigr)-(\hat{u}-3u+2d)\opcosts\bigl(\mx_{i,j}^{\hat{u}}\bigr)\\
	&\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)x_tf\left(\frac{\lambda_t}{x_t}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{\hat{u}}\right)\right)
\end{align*}
First, we notice that $u\le\hat{u}$ and $x_t<u$ for $i\le t\le j$. Together with the fact that $f$ is monotonically increasing, we infer that
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)x_tf\left(\frac{\lambda_t}{x_t}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{\hat{u}}\right)\right)\\
	&\ge&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)x_tf\left(\frac{\lambda_t}{u}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{u}\right)\right)
\end{align*}
Since $d$ is the smallest number of active servers scheduled by $\mx$, and $f$ is non-negative, we can conclude that
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)x_tf\left(\frac{\lambda_t}{u}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{u}\right)\right)\\
	&\ge&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)df\left(\frac{\lambda_t}{u}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{u}\right)\right)
\end{align*}
Hence, to finish the case, it suffices to show that
\begin{equation*}
	\sum\limits_{t=i}^j\left(2(\hat{u}-u)df\left(\frac{\lambda_t}{u}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{u}\right)\right)\ge 0
\end{equation*}
Further rearranging the left hand side gives us
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)df\left(\frac{\lambda_t}{u}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{u}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(\Bigl(2(\hat{u}-u)d-2(u-d)u-(\hat{u}-3u+2d)\hat{u}\Bigr)f\left(\frac{\lambda_t}{u}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(\Bigl(2d\hat{u}-2du-2u^2+2du-\hat{u}^2+3u\hat{u}-2d\hat{u}\Bigr)f\left(\frac{\lambda_t}{u}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(\left(-\hat{u}^2+3u\hat{u}-2u^2\right)f\left(\frac{\lambda_t}{u}\right)\right)=\sum\limits_{t=i}^j\left(\underbrace{-(u-\hat{u})(2u-\hat{u})}_{g(\hat{u})}f\left(\frac{\lambda_t}{u}\right)\right) 
\end{align*}
Now let $g(\hat{u})\coloneqq-(u-\hat{u})(2u-\hat{u})$. Since $f$ is non-negative, it suffices to show that $g(\hat{u})\ge0$ for all possible values of $\hat{u}$, namely $\hat{u}\in[u,2u]$. We notice that $g(\cdot)$ is an inverted parabola with roots $u$ and $2u$. Thus, we know that $g(u)=g(2u)=0$ and $g(\hat{u})>0$ for $u<\hat{u}<2u$, which finishes the case.

By iteratively applying above procedure to $\mx$, starting with $i=1$ and stopping with $j=T+1$, we obtain a $B$-restricted schedule $\mx'$ that satisfies $\costs(\mx')\le2\costs(\mx)$.
\end{proof}
Safe in the knowledge that there always exists a 2-approximative $B$-restricted schedule $\mx'$ for any schedule $\mx$, we are just left with the verification of our modified graph. Since we did not change the graph's general construction idea, this verification turns out to be very similar to that done in Section~\ref{sec:opt_offline_pseudo_lin}. For the proof of the next lemma, which establishes the bijection between $B$-restricted schedules and reasonable paths, we need to recall our notion of ``maximum'' nodes $x_t$ in reasonable paths $P$, as described in Definition~\ref{defn:max_path_node}.
\begin{lem}\label{lem:sched_reasn_path_approx_2}
Let $\bm{\mx}$ be the set of all $B$-restricted schedules for $\inp$, and let $\bm{\mathcal{P}}$ be the set of all reasonable paths. The map
\begin{equation*}
	\Phi:\bm{\mathcal{P}}\rightarrow\bm{\mx},\quad P\mapsto (x_1,\dotsc,x_T)
\end{equation*}
is a bijection satisfying $\costs(P)=\costs\bigl(\Phi(P)\bigr)$.
\end{lem}
\begin{proof}
The proof is analogous to the proof of Lemma~\ref{lem:sched_reasn_path_pseudo_lin} considering that we conduct logarithmic instead of incremental switching steps.
\end{proof}
Finally, we arrive at the main result of this section.
\begin{thm}\label{thm:approx_2}
Any shortest reasonable path $P$ corresponds to a 2-optimal, $B$-restricted schedule $\mx$ for $\inp$ with $\costs(P)=\costs(\mx)$.
\end{thm} 
\begin{proof}
By Lemma~\ref{lem:sched_reasn_path_approx_2}, we have a bijection $\Phi$ between reasonable paths $P$ and $B$-restricted schedules obeying $\costs(P)=\costs\bigl(\Phi(P)\bigr)$. Thus, we have 
\begin{equation*}
	\costs(P)\text{ minimal}\iff \costs\bigl(\Phi(P)\big)\text{ minimal}
\end{equation*}
Now let $P$ be a shortest reasonable path, and let $\mx^*$ be an optimal schedule for $\inp$. We have to verify that $\costs\bigl(\Phi(P)\bigr)\le 2\costs(\mx^*)$. By Lemma~\ref{lem:transform_schedule_approx_2}, we know that there exists a $B$-restricted schedule $\mx'$ such that $\costs(\mx')\le 2\costs(\mx^*)$. Since $\Phi$ is a bijection, and $P$ is a shortest reasonable path, we know that $\costs\bigl(\Phi(P)\bigr)\le\costs(\mx')$. Thus, we conclude that 
\begin{equation*}
	\costs(P)=\costs\bigl(\Phi(P)\bigr)\le\costs(\mx')\le 2\costs(\mx^*)
\end{equation*}
and the claim follows.
\end{proof}
Again, it is not difficult to show that also shortest paths which are not reasonable can be transformed to a desired 2-optimal schedule.
\begin{cor}\label{cor:approx_2_sched_short_path}
Any shortest path $P$ from $v_{0,0}$ to $v_{0,T\downarrow}$ can be transformed to a \makebox{2-optimal}, $B$-restricted schedule $\mx$ for $\inp$ with $\costs(P)=\costs(\mx)$.
\end{cor}
\begin{proof}
Let $P$ be a shortest path from $v_{0,0}$ to $v_{0,T\downarrow}$. By using a small adaption of Proposition~\ref{prop:path_to_reasn_path} that uses logarithmic instead of incremental switching steps, we can transform $P$ to a reasonable path $P'$ with $\costs(P')=\costs(P)$.
In turn, $P'$ corresponds to a 2-optimal, \makebox{$B$-restricted} schedule $\mx$ with $\costs(\mx)=\costs(P')=\costs(P)$ by Theorem~\ref{thm:approx_2}, which finishes the proof.
\end{proof}
To finish this section, we give an algorithm based on our verified constructions. Naturally, since we did not change the graph's overall structure and working principle, a small adaption of Algorithm~\ref{alg:opt_offline_pseudo_linear} will serve its purpose. To account for the logarithmic steps in our graph, we introduce an auxiliary function \textproc{nodes}, which calculates the number of servers associated to an given index $i$ in our tables $C$ and $S$. As a matter of implementation convenience, the variable $b$ is defined as $\ceil{\log_2(m)}$ instead of $\floor{\log_2(m)}$ in Algorithm~\ref{alg:approx_2_offline_linear}.
The correctness of the algorithm directly follows from Theorem~\ref{thm:approx_2} and the correctness of the shortest path calculation for directed acyclic graphs (for a proof see~\parencite[Section~24.2]{intro-algo}).\unsure{always cite this?}

For our runtime analysis, we again take the same assumptions as done for Algorithm~\ref{alg:opt_offline_pseudo_poly}. Subroutine \textproc{shortest\_paths} requires $\Theta\bigl(\log_2(m)\bigr)$ steps for its initialization and \makebox{$\Theta\bigl(2T\log_2(m)\bigr)$} steps for the iterative calculation of the selections and costs\unsure{Repetitive? Same words as in chapter 4}. Further, \textproc{extract\_schedule} needs $\Theta(T)$ iterations for its schedule retrieval. Hence, we receive a time complexity of
\begin{equation*}
	\Theta\bigl(\log_2(m)+2T\log_2(m)+T\bigr)=\Theta(T\log_2(m))
\end{equation*}
To our great joy, the runtime is linear in the size of the input. Similarly, our memory demand is reduced to linear space $\Theta\bigl(T\log_2(m)\bigr)$ since the size of the tables $C$ and $S$ shrank to $\Theta\bigl(T\log_2(m)\bigr)$.

We saw in this section that our reduction to logarithmic steps allows us to derive a 2-optimal linear-time algorithm. In our approach, we chose the base two logarithm for the graph's step sizes. It seems like an interesting question whether this approach can be generalized to arbitrary bases, allowing for more precise approximations. The answer of this question shall be final task of this thesis.
\newpage
\begin{algorithm}[H]
  \caption{2-optimal linear-time offline scheduling}
  \label{alg:approx_2_offline_linear}
  \begin{algorithmic}[1]
  \Function{2\_optimal\_offline\_scheduling}{$m,T,\Lambda,\beta,f$}
	  \Let{$(C,S)$}{\Call{shortest\_paths}{$m,T,\Lambda,\beta,f$}}
	  \Let{$\mx$}{\Call{extract\_schedule}{$S,T,m$}}
	  \State \Return{$\mx$}
  \EndFunction
  \Statex
  \Function{node}{$i,m$}
	  \State \Return{$\min\bigl\{m,\floor{2^{i-1}}\bigr\}$}
  \EndFunction
  \Statex
  \Function{shortest\_paths}{$m,T,\Lambda,\beta,f$}
	\Let{$b$}{$\ceil{\log_2(m)}$}
	\Blet{$C[0\dotso b+1,1\dotso T]$ and $S[0\dotso b+1,1\dotso T]$}{new tables}
	\Statex \Comment{Allocate cost and selection tables}
	\Let{$S[b+1,1]$}{$b+1$ and $C[b+1,1]\gets\costs(0,m,\lambda_1)$}\Comment{Initialize first node in first layer}
	\For{$i \gets b \textrm{ to } 0$}\Comment{Initialize first layer (downward minimization step)}
		\If {$C[i+1,1]<\costs\bigl(0,\Call{node}{i,m},\lambda_1\bigr)$}
			\Let{$S[i,1]$}{$S[i+1,1]$ and $C[i,1]\gets C[i+1,1]$}
		\Else
			\Let{$S[i,1]$}{$i$ and $C[i,1]\gets\costs\bigl(0,\Call{node}{i,m},\lambda_1\bigr)$}
		\EndIf
	\EndFor
	\For{$t \gets 1 \textrm{ to } T-1$}\Comment{Iterative calculate costs and selections}
		\For{$i \gets 1 \textrm{ to } b+1$}\Comment{Upward minimization step}
			\If {$C[i-1,t]+\beta\bigl(\Call{node}{i,m}-\Call{node}{i-1,m}\bigr)<C[i,t]$}
			    \Let{$S[i,t]$}{$S[i-1,t]$}
			    \Let{$C[i,t]$}{$C[i-1,t]+\beta\bigl(\Call{node}{i,m}-\Call{node}{i-1,m}\bigr)$}
			\EndIf
		\EndFor
		\Let{$S[b+1,t+1]$}{$b+1$ and $C[b+1,t+1]\gets C[b+1,t]+\opcosts(m,\lambda_{t+1})$}
		\For{$i \gets b \textrm{ to } 0$}\Comment{Downward minimization step}
			\If {$C[i+1,t+1]<C[i,t]+\opcosts\bigl(\Call{node}{i,m},\lambda_{t+1}\bigr)$}
				\Let{$S[i,t+1]$}{$S[i+1,t+1]$ and $C[i,t+1]\gets C[i+1,t+1]$}
			\Else
				\Let{$S[i,t+1]$}{$i$ and $C[i,t+1]\gets C[i,t]+\opcosts\bigl(\Call{node}{i,m},\lambda_{t+1}\bigr)$}
			\EndIf
		\EndFor
	\EndFor
	\State \Return{$(C,S)$}
  \EndFunction
  \Statex
  \Function{extract\_schedule}{$S,T,m$}
	\Blet{$\mx[1\dotso T]$}{a new array}
	\Let{$i$}{$S[0,T]$}\Comment{Get index of best selection for last time slot}
    	\Let{$\mx[T]$}{$\Call{node}{i,m}$}\Comment{Calculate best selection for last time slot}
        \For{$t \gets T-1 \textrm{ to } 1$}\Comment{Iteratively obtain schedule from selection table}
		\Let{$i$}{$S[i,t]$}
		\Let{$\mx[t]$}{$\Call{node}{i,m}$}	
	\EndFor
	\State \Return{$\mx$}
  \EndFunction
  \end{algorithmic}
\end{algorithm}

\section{A $(1+\beps)$-Optimal Linear-Time Algorithm}
In this section, we will generalize our algorithm from Section~\ref{sec:approx_2_opt} such that it will be able to approximate an optimal solution with arbitrary precision. More precisely, for any user-provided positive number $\beps\in\mathbb{R}_{>0}$, our final algorithm will be able to find a \makebox{$(1+\beps)$-optimal} schedule in linear time.
For the rest of this section, we denote the algorithm's desired precision as $y$, that is we set
\begin{equation*}
	y\coloneqq 1+\beps\in\mathbb{R}_{>1}
\end{equation*}
Previously, we chose the base two logarithm for the step sizes of our graph and derived an 2-optimal algorithm. Consequently, one might conjecture that choosing the base $y$ for the graph's logarithmic step sizes will result in a $y$-optimal algorithm. But to make this conjecture a theorem, it requires some more work than merely changing the base from 2 to $y$.

Recall the graph's structure from Section~\ref{sec:approx_2_opt}. For any power of two smaller than~$m$, we added a node in our graph. As a result, the set of possible scheduling choices was given by $B=\{0,2^0,2^1,\dotsc,2^{\floor{\log_2(m)}},m\}$. Now consider an analogous attempt for arbitrary bases~$y$, that is $B=\{0,y^0,y^1,\dotsc,y^{\floor{\log_y(m)}},m\}$. Since~$y$ is not necessarily integer, an arbitrary power of~$y$ can also be non-integer. Consequently, scheduling a number picked from the new set~$B$ potentially fails as one cannot schedule a non-integer number of servers in practice. We thus have to adapt the new set. To do so, we simply round the powers of~$y$ to their closest integers. More formally, given an input $\inp$, we set
\begin{align*}
	b&\coloneqq\floor{\log_y(m)}\\
	\text{and}\qquad B&\coloneqq\left\{0,1,\floor{y^1},\ceil{y^1},\floor{y^2},\ceil{y^2},\dotsc,\floor{y^b},\ceil{y^b},m\right\}
\end{align*}
Using this new set of scheduling choices, we consider the following modification of our graph:
\begin{align*}
	V&\coloneqq\bigl\{v_{x,t\downarrow}\mid x\in B,t\in[T]\bigr\}\dotcup\bigl\{v_{x,t\uparrow}\mid x\in B, t\in[T-1]\bigr\}\dotcup\{v_{0,0}\}\\
	E_s&\coloneqq\bigl\{(v_{0,0},v_{x,1\downarrow})\mid x\in B\bigr\}\\
	E_\downarrow&\coloneqq\bigl\{(v_{\ceil{y^i},t\downarrow},v_{\floor{y^{i}},t\downarrow})\mid i\in[b],t\in[T]\bigr\}\cup\bigl\{(v_{\floor{y^i},t\downarrow},v_{\ceil{y^{i-1}},t\downarrow})\mid i\in[b],t\in[T]\bigr\}\\
	&\phantom{{}\coloneqq{}}\cup\bigl\{(v_{y^0,t\downarrow},v_{0,t\downarrow})\mid t\in[T]\bigr\}\cup\bigl\{(v_{m,t\downarrow},v_{\ceil{y^b},t\downarrow})\mid t\in[T]\bigr\}\\
	E_\uparrow&\coloneqq\bigl\{(v_{\floor{y^i},t\uparrow},v_{\ceil{y^i},t\uparrow})\mid i\in[b],t\in[T-1]\bigr\}\cup\bigl\{(v_{\ceil{y^{i-1}},t\uparrow},v_{\floor{y^i},t\uparrow})\mid i\in[b],t\in[T-1]\bigr\}\\
	&\phantom{{}\coloneqq{}}\cup\bigl\{(v_{0,t\uparrow},v_{y^0,t\uparrow})\mid t\in[T-1]\bigr\}\cup\bigl\{(v_{\ceil{y^b},t\uparrow},v_{m,t\uparrow})\mid t\in[T-1]\bigr\}\\
	E_{\downarrow\uparrow}&\coloneqq\bigl\{(v_{x,t\downarrow},v_{x,t\uparrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E_{\uparrow\downarrow}&\coloneqq\bigl\{(v_{x,t\uparrow},v_{x,t+1\downarrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E&\coloneqq E_s\dotcup E_\downarrow\dotcup E_\uparrow\dotcup E_{\downarrow\uparrow}\dotcup E_{\uparrow\downarrow}\displaybreak[0]\\
	c_G(e)&\coloneqq
	\begin{cases}
		\costs(0,x,\lambda_1), & \text{if $e=(v_{0,0},v_{x,1\downarrow})\in E_s$}\\
		\opcosts(x,\lambda_{t+1}), & \text{if $e=(v_{x,t\uparrow},v_{x,t+1\downarrow})\in E_{\uparrow\downarrow}$}\\
		\beta(x'-x), & \text{if $e=(v_{x,t\uparrow},v_{x',t\uparrow})\in E_\uparrow$}\\
		0, & \text{if $e\in(E_\downarrow\dotcup E_{\downarrow\uparrow})$}
	\end{cases}\\
	G&\coloneqq(V,E,c_G)
\end{align*}
Unnecessary loops can again simply be ignored. A graphical representation of $G$ can be found in Figure~\ref{fig:graph_lin_approx_y}.
\begin{figure}[H]
\includestandalone[width=\textwidth]{../figures/graph_lin_approx_y}
\caption{Graph for a $y$-optimal linear-time offline algorithm; the path of the topological sorting is highlighted in red.}
\label{fig:graph_lin_approx_y}
\end{figure}
There is not much to say about the graph's working principle, since it stays the same as for our construction in Section~\ref{sec:approx_2_opt}. Instead, we dive right into proving process. To begin, we have to adapt Lemma~\ref{lem:transform_schedule_approx_2}, for which we have to generalize our definition of 2-state changes.
\begin{defn}[y-state changes]\label{defn:y_state_changes}
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule and $t\in[T+1]$. We say that $\mx$ changes its \emph{$y$-state} at time $t$ if $\mx$ satisfies the formula
\begin{equation*}
	x_{t-1}\neq x_t\land \left(x_{t-1}=0 \lor x_t\notin\left[y^{\floor{\log_y(x_{t-1})}},y^{\floor{\log_y(x_{t-1})}+1}\right)\right)
\end{equation*}
\end{defn}
The proof of the next lemma will to a great extent be very similar to that of Lemma~\ref{lem:transform_schedule_approx_2}. However, as the interval boundaries of our $y$-state changes are not integer anymore, we will have to proceed more cautious and consider some more cases.
\begin{lem}
Let $\mx$ be a schedule for $\inp$. There exists a $B$-restricted schedule $\mx'$ satisfying \makebox{$\costs(\mx')\le y\costs(\mx)$}.
\end{lem}
\begin{proof}
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule for $\inp$. We need to construct a $B$-restricted schedule $\mx'=(x_1',\dotsc,x_T')$ such that \makebox{$\costs(\mx')\le y\costs(\mx)$}. By following the same reasoning as done for Lemma~\ref{lem:transform_schedule_approx_2}, we can again assume that $\mx$ is feasible and never shuts down all servers. We then have to show that every period between two $y$-state changes of $\mx$ can be iteratively transformed to a $y$-approximative period in $\mx'$. 
The proof will again require an amortized analysis for the switching costs of $\mx'$ using the accounting method. For our iterative transformation, let $i$ and $j+1$ with \makebox{$i,j\in[T]$} and $i\le j$ be the first unprocessed time slots at which $\mx$ changes its \makebox{$y$-state}. We define the lower and upper bound of $\mx_{i,j}$ as
\begin{flalign*}
	&&l\coloneqq y^{\floor{\log_y(x_i)}}&&\text{and}&&u\coloneqq\min\bigl\{yl,m\bigr\}&&&
\end{flalign*}
and the lower and upper bound of $\mx$ at time $j+1$ as
\begin{flalign*}
	&&l'\coloneqq\begin{cases}
		y^{\floor{\log_y(x_{j+1})}}, & \text{if $x_{j+1}\neq 0$}\\
		0, & \text{if $x_{j+1}=0$}
	\end{cases}
&&\text{and}&&u'\coloneqq\min\bigl\{yl',m\bigr\}&&&
\end{flalign*}
In the proof of Lemma~\ref{lem:transform_schedule_approx_2}, we knew that, for example, $u$ is contained in $B$, and thus we were able to simply use $u$ servers for $\mx'$. This time, however, we have to appropriately round our choices, e.g.\ use $\floor{u}\in B$ or $\ceil{u}\in B$ instead of $u$ servers.  
Note that $\mx$ does not change its $y$-state between $i$ and $j$, which means that $l\le x_t\le u$ holds for $i\le t\le j$. Since~$x_t$ is integer, $\ceil{l}$ is the smallest integer contained in~$[l,u]$, and $\floor{u}$ is the largest integer contained in~$[l,u]$, we can even refine our bounds to $\ceil{l}\le x_t\le\floor{u}$. Further, by using the fact that $u\le yl$, we can infer that $\floor{u}\le yx_t$ holds for any $i\le t\le j$.

We adapt the invariant used in Lemma~\ref{lem:transform_schedule_approx_2} in that we are going to ensure that $\mx'$ can potentially move to $\floor{u}$ at time $i$ in a $y$-approximative manner, i.e.\ we ensure that~$\mx'$ incurs at most $y$ times as much switching costs as $\mx$ if we set $x_i'\coloneqq \floor{u}$. Further, we are going to ensure that $x_t'\ge \floor{u}$ holds for any $i\le t\le j$ after every transformation step. Since $x_t\le \floor{u}$, this guarantees that $\mx'$ will be feasible.
Moreover, we show that our credit will be greater than or equal to $\beta(yx_{j+1}-\floor{u'})$ if $\mx$ exits $[l,u)$ by ascending to $[l',u')$ at time $j+1$. Since the periods are consecutive, this equivalently means that the credit will be greater than or equal to $\beta(yx_i-\floor{u})$ if $\mx$ enters $[l,u)$ at time~$i$ from below.

For the initial start-up process (i.e.\ $i=1$), we can simply set $x_1'\coloneqq \floor{u} \in B$. Since we know that $x_1'\le yx_1$, we can conclude that $\beta x_1'\le\beta yx_1$. Further, we can use the difference of switching costs $\beta(yx_1-x_1')=\beta(yx_1-\floor{u})$ for the credit of our amortized analysis. Thus, the invariant initially holds.

Now recall the possible behaviors of $\mx$ between its $y$-state changes:
\begin{enumerate}[label=(\alph*)]
	\item $\mx$ enters $[l,u)$ from below and then descends to $[l',u')$.\label{itm:schedule_behavior_up_down_y}
	\item $\mx$ enters $[l,u)$ from above and then descends to $[l',u')$.\label{itm:schedule_behavior_down_down_y}
	\item $\mx$ enters $[l,u)$ from below and then ascends to $[l',u')$.\label{itm:schedule_behavior_up_up_y} 
	\item $\mx$ enters $[l,u)$ from above and then ascends to $[l',u')$.\label{itm:schedule_behavior_down_up_y} 
\end{enumerate}
We need to show that any case can be transformed to a $y$-approximative period in $\mx'$ while ensuring that our invariant will not be violated. Additionally, we have to verify that the credit of our amortized analysis stays non-negative.

We start with case~\ref{itm:schedule_behavior_up_down_y}. Due to our invariant, we know that we can set $x_i'\coloneqq \floor{u}$ with $y$-approximative switching costs. Consequently, setting $x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq \floor{u}\in B$ gives us a strategy with $y$-approximative switching costs between $i$ and $j$. Further, since $x_t\le \floor{u}\le yx_t$ for any $i\le t\le j$, and $f$ is non-negative and monotonically increasing, the operating costs of $\mx'_{i,j}$ can be estimated by
\begin{align*}
	\opcosts\bigl(\mx_{i,j}'\bigr)&\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}\sum\limits_{t=i}^j\left(\floor{u} f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\le\sum\limits_{t=i}^j\left(yx_tf\left(\frac{\lambda_t}{\floor{u}}\right)\right)\le y\sum\limits_{t=i}^j\left(x_tf\left(\frac{\lambda_t}{x_t}\right)\right)\\
	&\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}y\opcosts\bigl(\mx_{i,j}\bigr)
\end{align*}
Thus, the operating costs of $\mx_{i,j}'$ are $y$-approximative. Moreover, since $\floor{u'}<\floor{u}$, we do not need to account for additional switching costs to satisfy our invariant at time~$j+1$. Lastly, the credit of our amortized analysis stays untouched, i.e.\ the credit stays at $\beta(yx_i-\floor{u})$. We will have to use this credit for case~\ref{itm:schedule_behavior_down_up_y}.

Next, we consider case~\ref{itm:schedule_behavior_down_down_y}. Since $\mx$ enters $[l,u)$ from above, we have the possibility to choose $\ceil{u}$ or $\floor{u}$ servers for $\mx'_{i,t}$ at time $i$ without incurring switching costs. We thus consider two different strategies for $\mx_{i,j}'$:
\begin{flalign*}
	&&\mx_{i,j}^{\floor{u}}&\coloneqq\left(x_i^{\floor{u}},\dotsc,x_j^{\floor{u}}\right),&\text{where}&&x_t^{\floor{u}}&\coloneqq \floor{u},\,\text{for } i\le t\le j,&&\\
	&&\text{and}\quad\mx_{i,j}^{\ceil{u}}&\coloneqq\left(x_i^{\ceil{u}},\dotsc,x_j^{\ceil{u}}\right),&\text{where}&&x_t^{\ceil{u}}&\coloneqq\ceil{u},\,\text{for } i\le t\le j&&
\end{flalign*}
Since $\floor{u'}<\floor{u}\le\ceil{u}$, both strategies do not incur switching costs to satisfy the invariant at time $j+1$. We decide to choose strategy $\mx_{i,j}^{\ceil{u}}$ if $y\ceil{l}>\ceil{u}$, and strategy $\mx_{i,j}^{\ceil{u}}$ otherwise. Let us check that these choices are indeed $y$-approximative. If $y\ceil{l}>\ceil{u}$, we know that $yx_t>\ceil{u}$ for $i\le t\le j$, which allows us to estimate the cost of $\mx_{i,j}^{\ceil{u}}$ by
\begin{align*}
	\opcosts\bigl(\mx_{i,j}^{\ceil{u}}\bigr)&\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}\sum\limits_{t=i}^j\left(\ceil{u} f\left(\frac{\lambda_t}{\ceil{u}}\right)\right)\le\sum\limits_{t=i}^j\left(yx_tf\left(\frac{\lambda_t}{\ceil{u}}\right)\right)\le y\sum\limits_{t=i}^j\left(x_tf\left(\frac{\lambda_t}{x_t}\right)\right)\\
	&\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}y\opcosts\bigl(\mx_{i,j}\bigr)
\end{align*}
which shows that $\mx_{i,j}^{\ceil{u}}$ is $y$-approximative. Next, if $y\ceil{l}\le\ceil{u}$, we choose strategy $\mx_{i,j}^{\floor{u}}$, whose cost, as we have already seen in case~\ref{itm:schedule_behavior_up_down}, is $y$-approximative.

For case~\ref{itm:schedule_behavior_up_up_y}, we know that setting \makebox{$x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq \floor{u}\in B$} gives us a strategy with $y$-approximative switching costs due to our invariant. Moreover, as in the previous cases, the operating costs of $\mx_{i,j}'$ are $y$-approximative. 
To verify our invariant at time~$j+1$, we have to show that we can account for the switching costs to move to $u'$ and for the new credit required by the invariant. For this, we use the fact that $\mx$ has to power on at least $x_{j+1}-x_i$ servers between~$i$ and~$j+1$, and that we can use our old credit to compensate for our costs, since we entered $[l,u)$ from below:
\begin{align*}
	&y\overbrace{\beta(x_{j+1}-x_i)}^{\text{sw.\ costs }\mx}+\overbrace{\beta(yx_i-\floor{u})}^{\text{old credit}}-\overbrace{\beta(\floor{u'}-\floor{u})}^{\text{sw.\ costs }\mx'}-\overbrace{\beta(yx_{j+1}-\floor{u'})}^{\text{new credit}}\\
	=\quad&\beta(yx_{j+1}-yx_i+yx_i-\floor{u}-\floor{u'}+\floor{u}-yx_{j+1}+\floor{u'})=0
\end{align*}
that is, the cost difference is non-negative, and thus our invariant at time $j+1$ is satisfied.
	
Finally, we have to study case~\ref{itm:schedule_behavior_down_up_y}, for which we consider three different strategies, namely
\begin{flalign*}
	&&\mx_{i,j}^{\floor{u}}&\coloneqq\left(x_i^{\floor{u}},\dotsc,x_j^{\floor{u}}\right),&&\text{where}&x_t^{\floor{u}}&\coloneqq \floor{u},\,\text{for } i\le t\le j,&&&\\
	&&\mx_{i,j}^{\floor{\hat{u}}}&\coloneqq\left(x_i^{\floor{\hat{u}}},\dotsc,x_j^{\floor{\hat{u}}}\right),&&\text{where}&x_t^{\floor{\hat{u}}}&\coloneqq \floor{\hat{u}},\,\text{for } i\le t\le j,&&&\\
	&&\text{and}\quad\mx_{i,j}^{\ceil{\hat{u}}}&\coloneqq\left(x_i^{\ceil{\hat{u}}},\dotsc,x_j^{\ceil{\hat{u}}}\right),&&\text{where}&x_t^{\ceil{\hat{u}}}&\coloneqq \ceil{\hat{u}},\,\text{for } i\le t\le j&&&
\end{flalign*}
with $\hat{u}\coloneqq\min\{yu,m\}$. Notice that due to our invariant and the fact that $\mx$ descends at time $i$, strategies $\mx_{i,j}^{\floor{u}}$ and $\mx_{i,j}^{\floor{\hat{u}}}$ do no incur switching costs up to time $j$, since $x_{i-1}'\ge\floor{\hat{u}}\ge\floor{u}$. We are now going to prove that the cost of at least one of the three schedules, including costs to satisfy our invariant at time $j+1$, must be $y$-approximative. 
First, we define $d\coloneqq\min\{x_t\mid i\le t\le j\}$ to be the smallest number of active servers used by $\mx_{i,j}$. We can use $d$ to give a lower bound for the switching costs of $\mx$ between $i$ and~$j+1$:
\begin{equation*}
	\sum\limits_{t=i+1}^{j+1}\swcosts(x_{t-1},x_t)\ge\beta(x_{j+1}-d)
\end{equation*}
Next, we have to conduct another case distinction; namely, either $yd\le\floor{\hat{u}}$ or $yd>\floor{\hat{u}}$. We begin with the former, for which we proceed analogous as in the proof of Lemma~\ref{lem:transform_schedule_approx_2}. We again split the switching costs of $\mx_{i,j}$ (including its final switching step) into two parts. This time, we split $\beta(x_{j+1}-d)$ into $\beta\bigl(x_{j+1}-\floor{\hat{u}}/y\bigr)$ and $\beta\bigl(\hat{u}/y-d\bigr)$. It now suffices to show that either $\mx_{i,j}^{\floor{u}}$ or $\mx_{i,j}^{\floor{\hat{u}}}$ -- strategy $\mx_{i,j}^{\ceil{\hat{u}}}$ will not be needed in this case -- can process the loads $\lambda_i,\dotsc,\lambda_j$ and eventually switch to $\floor{\hat{u}}$ with $y$-approximative costs under the assumption that $\mx_{i,j}$ only incurs~$\beta\bigl(\floor{\hat{u}}/y-d\bigr)$ switching costs in a first step. 
This is because the remaining switching costs $\beta\bigl(x_{j+1}-\floor{\hat{u}}/y\bigr)$ of $\mx$ already suffice to compensate for the remaining switching costs $\beta(\floor{u'}-\floor{\hat{u}})$ of $\mx'$ and the costs to satisfy the invariant:
\begin{align*}
	&y\overbrace{\beta(x_{j+1}-\floor{\hat{u}}/y)}^{\text{remaining sw.\ costs }\mx}-\overbrace{\beta(\floor{u'}-\floor{\hat{u}})}^{\text{rem.\ sw.\ costs }\mx'}-\overbrace{\beta(yx_{j+1}-\floor{u'})}^{\text{new credit}}\\
	=\quad&\beta(yx_{j+1}-\floor{\hat{u}}-\floor{u'}+\floor{\hat{u}}-yx_{j+1}+\floor{u'})=0
\end{align*}
To proceed, we notice that if $yd=\floor{u}$ holds, strategy $\mx_{i,j}^u$ has $y$-approximative switching costs:
\begin{equation*}
	\beta(\floor{\hat{u}}-\floor{u})=\beta(\floor{\hat{u}}-yd)=y\beta\left(\frac{\floor{\hat{u}}}{y}-d\right)
\end{equation*}
Further, as in the previous cases, the operating costs of $\mx_{i,j}^{\floor{u}}$ are $y$-approximative, and thus~$\mx_{i,j}^{\floor{u}}$ is $y$-approximative if $yd=\floor{u}$. Since $yd<\floor{u}$ is impossible, we can safely assume that $yd>\floor{u}$. Moreover, we can assume that $\mx_{i,j}^{\floor{u}}$ is not $y$-approximative; otherwise, we are already done. Hence, suppose that 
\begin{equation*}
	\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)+\beta(\floor{\hat{u}}-\floor{u})>y\left(\opcosts\bigl(\mx_{i,j}\bigr)+\beta\left(\frac{\floor{\hat{u}}}{y}-d\right)\right)
\end{equation*}
We then have to show that $\mx_{i,j}^{\floor{\hat{u}}}$ is $y$-approximative.
Rearranging the previous inequality gives us an estimation for $\beta$:
\begin{align*}
	&&\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)+\beta(\floor{\hat{u}}-\floor{u})&>y\left(\opcosts\bigl(\mx_{i,j}\bigr)+\beta\left(\frac{\floor{\hat{u}}}{y}-d\right)\right)
&\\
	&\stackrel{\phantom{yd-\floor{u}>0}}{\iff}&\beta(yd-\floor{u})&>y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)&\\
	&\stackrel{yd-\floor{u}>0}{\iff}&\beta&>\frac{y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)}{yd-\floor{u}}
\end{align*}
where the last step is justified by assumption $yd>\floor{u}$. Now, by additionally using our assumption $yd\le\floor{\hat{u}}$, which equivalently means $\frac{\floor{\hat{u}}}{y}-d\ge0$, we can find a lower bound for the cost of~$\mx_{i,j}$:
\begin{equation*}
	\opcosts\bigl(\mx_{i,j})+\beta\biggl(\underbrace{\frac{\floor{\hat{u}}}{y}-d}_{\ge 0}\biggr)\ge\opcosts\bigl(\mx_{i,j}\bigr)+\frac{y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)}{yd-\floor{u}}\left(\frac{\floor{\hat{u}}}{y}-d\right)
\end{equation*}
which can be simplified to
\begin{align*}
	&\opcosts\bigl(\mx_{i,j}\bigr)+\frac{y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)}{yd-\floor{u}}\left(\frac{\floor{\hat{u}}}{y}-d\right)\\
	=\quad&\frac{(yd-\floor{u})\opcosts\bigl(\mx_{i,j}\bigr)+(\floor{\hat{u}}-yd)\opcosts\bigl(\mx_{i,j}\bigr)-\left(\frac{\floor{\hat{u}}}{y}-d\right)\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)}{yd-\floor{u}}\\
	=\quad&\frac{(\floor{\hat{u}}-\floor{u})\opcosts\bigl(\mx_{i,j}\bigr)-\left(\frac{\floor{\hat{u}}}{y}-d\right)\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)}{yd-\floor{u}}
\end{align*}
Next, since we want to prove that $\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)$ is $y$-approximative, we have to show that the following cost difference is non-negative:
\begin{align*}
	y\left(\opcosts\bigl(\mx_{i,j}\bigr)+\beta\left(\frac{\floor{\hat{u}}}{y}-d\right)\right)-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)
\end{align*}
Recall that $\mx_{i,j}^{\floor{\hat{u}}}$ does not incur switching costs to move to $\floor{\hat{u}}$. We can now use our just derived lower bound for $\opcosts\bigl(\mx_{i,j}\bigr)+\beta\bigl(\floor{\hat{u}}/y-d\bigr)$ to estimate the difference:
\begin{align*}
	&y\left(\opcosts\bigl(\mx_{i,j}\bigr)+\beta\left(\frac{\floor{\hat{u}}}{y}-d\right)\right)-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)\\
	\ge\quad&y\, \frac{(\floor{\hat{u}}-\floor{u})\opcosts\bigl(\mx_{i,j}\bigr)-\left(\frac{\floor{\hat{u}}}{y}-d\right)\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)}{yd-\floor{u}}-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)
\end{align*}
Thus, it suffices to show that
\begin{align*}
	y\,\frac{(\floor{\hat{u}}-\floor{u})\opcosts\bigl(\mx_{i,j}\bigr)-\left(\frac{\floor{\hat{u}}}{y}-d\right)\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)}{yd-\floor{u}}-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)\ge 0
\end{align*}
which can be simplified to
\begin{equation*}
	y(\floor{\hat{u}}-\floor{u})\opcosts\bigl(\mx_{i,j}\bigr)-(\floor{\hat{u}}-yd)\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)-(yd-\floor{u})\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)\ge 0
\end{equation*}
To show this inequality, we have to take a closer look on the the schedules' operating costs:
\begin{align*}
	&&&y(\floor{\hat{u}}-\floor{u})\opcosts\bigl(\mx_{i,j}\bigr)-(\floor{\hat{u}}-yd)\opcosts\bigl(\mx_{i,j}^{\floor{u}}\bigr)-(yd-\floor{u})\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)\\
	&\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\left(\frac{\lambda_t}{x_t}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\right)
\end{align*}
First, we recall that $\floor{u}\le\floor{\hat{u}}$ and $x_t\le\floor{u}$ for $i\le t\le j$. Together with the fact that $f$ is monotonically increasing, we infer that
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\left(\frac{\lambda_t}{x_t}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\right)\\
	&\ge&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\left(\frac{\lambda_t}{\floor{u}}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{u}}\right)\right)
\end{align*}
Since $d$ is the smallest number of active servers scheduled by $\mx$, and $f$ is non-negative, we can conclude that
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\left(\frac{\lambda_t}{\floor{u}}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\\
	&\ge&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})df\left(\frac{\lambda_t}{\floor{u}}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{u}}\right)\right)
\end{align*}
Hence, to finish the case, it suffices to show that
\begin{align*}
	\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})df\left(\frac{\lambda_t}{\floor{u}}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\ge 0
\end{align*}
Further rearranging the left hand side gives us
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})df\left(\frac{\lambda_t}{\floor{u}}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(\Bigl(y(\floor{\hat{u}}-\floor{u})d-(\floor{\hat{u}}-yd)\floor{u}-(yd-\floor{u})\floor{\hat{u}}\Bigr)f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(\Bigl(yd\floor{\hat{u}}-yd\floor{u}-\floor{u}\floor{\hat{u}}+yd\floor{u}-yd\floor{\hat{u}}+\floor{u}\floor{\hat{u}}\Bigr)f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(0\cdot f\left(\frac{\lambda_t}{\floor{u}}\right)\right)=0
\end{align*}
which finishes the case with assumption $yd\le\floor{\hat{u}}$. We are left with case $yd>\floor{\hat{u}}$. For this case, we have to analyze the behavior of $\mx$ more precisely. To do so, let $h<i$ be the closest time before $i$ at which $\mx$ changes its $y$-state. There are three possible behaviors of $\mx$ that can lead to case~\ref{itm:schedule_behavior_down_up_y}.
\begin{enumerate}[label=(\roman*)]
	\item $\mx$ ascends to $[u,\hat{u})$ at time $h$ and then descends to $[l,u)$ at time $i$.\label{itm:schedule_behavior_down_up_y_subcase_1}
	\item $\mx$ descends to $[u,\hat{u})$ at time $h$ and then descends to $[l,u)$ at time $i$.\label{itm:schedule_behavior_down_up_y_subcase_2}
	\item $\mx$ descends to $[l,u)$ at time $i$ from a region above $[u,\hat{u})$.\label{itm:schedule_behavior_down_up_y_subcase_3}
\end{enumerate}
All three possibilities are illustrated in Figure~\ref{fig:schedule_behavior_down_up_y}. For each case, we have to show that at least one of our three strategies satisfies our conditions, to wit, it incurs $y$-approximative costs including the costs to satisfy the invariant at time $j+1$.
\begin{figure}[H]
\renewcommand\thesubfigure{\roman{subfigure}}
\captionsetup[subfigure]{labelformat=parens}
\begin{subfigure}[b]{0.31\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_y_1}
\caption{$\mx$ ascends to $[u,\hat{u})$ and then descends to $[l,u)$.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.31\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_y_2}
\caption{$\mx$ descends to $[u,\hat{u})$ and then descends to $[l,u)$.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.31\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_y_3}
\caption{$\mx$ descends to $[l,u)$ from a region above $[u,\hat{u})$.}
\end{subfigure}
\caption{The three different possible behaviors of $\mx$ to reach case~\ref{itm:schedule_behavior_down_up_y}}
\label{fig:schedule_behavior_down_up_y}
\end{figure}
We begin with case~\ref{itm:schedule_behavior_down_up_y_subcase_1}. Since $\mx$ ascends to $[u,\hat{u})$ at time~$h$ and descends to $[l,u)$ at time~$i$, we know from case~\ref{itm:schedule_behavior_up_down_y} that we have some remaining credit $\beta(yx_h-\floor{\hat{u}})$. Since $x_h\ge\ceil{u}$, this credit amounts to at least $\beta(y\ceil{u}-\floor{\hat{u}})$. We use this credit to show that strategy~$\mx_{i,j}^{\floor{\hat{u}}}$ is $y$-approximative, that is we have to verify that the following cost difference is non-negative:
\begin{align*}
	&y\bigl(\overbrace{\opcosts\bigl(\mx_{i,j}\bigr)+\beta(x_{j+1}-d)}^{\text{costs of }\mx}\bigr)+\overbrace{\beta(y\ceil{u}-\floor{\hat{u}})}^{\text{old credit}}-\bigl(\overbrace{\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)+\beta(\floor{u'}-\floor{\hat{u}})}^{\text{costs of }\mx_{i,j}^{\floor{\hat{u}}}}\bigr)\\
	-&\underbrace{\beta(yx_{j+1}-\floor{u'})}_{\text{new credit}}
\end{align*}
This difference can be simplified to
\begin{align*}
	&&&y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)+\beta\bigl(yx_{j+1}-yd+y\ceil{u}-\floor{\hat{u}}-\floor{u'}+\floor{\hat{u}}-yx_{j+1}+\floor{u'}\bigr)\\
	&\stackrel{\phantom{\eqref{eq:mx_schedule_op_costs_complete}}}{=}&&y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)+\underbrace{\beta\bigl(y\ceil{u}-yd\bigr)}_{\ge 0}\ge y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)
\end{align*}
where $y\ceil{u}-yd\ge 0$ follows from $d\le \floor{u}\le \ceil{u}$. Thus, it suffices to show that
\begin{align}
	y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)&\ge 0\label{eq:op_cost_floor_u_hat_y_approx}
\end{align}
Further modifying the left hand side as done in previous cases reveals
\begin{align*}
	y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)&\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}\sum\limits_{t=i}^{j}\left(yx_tf\left(\frac{\lambda_t}{x_t}\right)-\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\right)\\
	&\stackrel{\phantom{\eqref{eq:mx_schedule_op_costs_complete}}}{\ge}\sum\limits_{t=i}^{j}\left(ydf\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)-\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\right)\\
	&\stackrel{\phantom{\eqref{eq:mx_schedule_op_costs_complete}}}{=}\sum\limits_{t=i}^{j}\biggl((\underbrace{yd-\floor{\hat{u}}}_{>0})f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\biggr)\ge 0
\end{align*}
where $yd-\floor{\hat{u}}>0$ follows from assumption $yd>\floor{\hat{u}}$. Hence, strategy $\mx_{i,j}^{\floor{\hat{u}}}$ is $y$-approxi\-mative.

Lastly, we consider cases~\ref{itm:schedule_behavior_down_up_y_subcase_2} and~\ref{itm:schedule_behavior_down_up_y_subcase_3}. Suppose for these cases that $\mx_{i,j}^{\floor{\hat{u}}}$ is not $y$-approxi\-mative, that is
\begin{align*}
	y\bigl(\overbrace{\opcosts\bigl(\mx_{i,j}\bigr)+\beta(x_{j+1}-d)}^{\text{costs of }\mx}\bigr)&-\bigl(\overbrace{\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)+\beta(\floor{u'}-\floor{\hat{u}})}^{\text{costs of }\mx_{i,j}^{\floor{\hat{u}}}}\bigr)-\overbrace{\beta(yx_{j+1}-\floor{u'})}^{\text{new credit}}<0
\end{align*}
We already know from Inequality~\eqref{eq:op_cost_floor_u_hat_y_approx} that the operating costs of $\mx_{i,j}^{\floor{\hat{u}}}$ are $y$-approximative. However, as the total difference of costs is still negative, it seems interesting to get an estimation for $\beta$ by rearranging the previous inequality:
\begin{align*}
	&&y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)&<-\beta\bigl(yx_{j+1}-yd-\floor{u'}+\floor{\hat{u}}-yx_{j+1}+\floor{u'}\bigr)\\
	\iff&&y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)&<-\beta\bigl(-yd+\floor{\hat{u}}\bigr)\\
	\iff&&y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)&<\beta\bigl(yd-\floor{\hat{u}}\bigr)
\end{align*}
We plug in the definition of our operating cost function, and conduct our usual estimations to derive
\begin{align}
	&&y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\floor{\hat{u}}}\bigr)&<\beta\bigl(yd-\floor{\hat{u}}\bigr)\nonumber\\
	\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{\iff}&&\sum\limits_{t=i}^{j}\biggl(yx_tf\left(\frac{\lambda_t}{x_t}\right)-\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\biggr)&<\beta\bigl(yd-\floor{\hat{u}}\bigr)\nonumber\\
	\implies&&\sum\limits_{t=i}^{j}\biggl(ydf\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)-\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\biggr)&<\beta\bigl(yd-\floor{\hat{u}}\bigr)\nonumber\\
	\iff&&\sum\limits_{t=i}^{j}\biggl((yd-\floor{\hat{u}})f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\biggr)&<\beta\bigl(yd-\floor{\hat{u}}\bigr)\nonumber\\
	\stackrel{yd>\floor{\hat{u}}}{\iff}&&\sum\limits_{t=i}^{j}f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)&<\beta\label{eq:beta_greater_sum_op_floor_u_hat}
\end{align}
We are now going to prove that $\mx_{i,j}^{\ceil{\hat{u}}}$ is $y$-approximative. The just derived lower bound for~$\beta$ will be needed at the very end of this endeavor.
First note that $\mx$ descends at time~$h$ to $[u,\hat{u})$ in case~\ref{itm:schedule_behavior_down_up_y_subcase_2} and at time~$i$ in case~\ref{itm:schedule_behavior_down_up_y_subcase_3}. Hence, we know that $\hat{u}=yu$; otherwise, we have $\hat{u}=m<yu$, and $\mx$ would consequently not be able to descend at time~$h$ or~$i$, respectively. Next, we show that $\mx_{i,j}^{\ceil{\hat{u}}}$ does not incur switching costs to move to $\ceil{\hat{u}}$ at time $i$. This is evidently true for case~\ref{itm:schedule_behavior_down_up_y_subcase_3} as $\mx$ descends at time $i$ from a region above $[u,\hat{u})$. Case~\ref{itm:schedule_behavior_down_up_y_subcase_2} requires some more effort. We first infer that $\hat{u}\neq\floor{\hat{u}}$; in other words, $\hat{u}$ is not integer. If $\hat{u}$ would be integer, we could derive from our assumption that $yd>\floor{\hat{u}}=\floor{yu}=yu$, which leads to a contradiction since $d\le\floor{u}$. 
Further, we can can infer that $u\neq\floor{u}$; otherwise, we know that $d\le u-1$ because $d$ is integer and must be smaller than $u$ (by our Definition~\ref{defn:y_state_changes} of $y$-state changes), which again leads to a contradiction:
\begin{flalign*}
	&&yd&>\floor{\hat{u}}&\stackrel{u-1\ge d}{\implies}&&y(u-1)&>\floor{\hat{u}}&\stackrel{\hat{u}=yu}{\iff}&&y(u-1)&>\floor{yu}&&\\
	&&&&\iff&&yu-\floor{yu}&>y\quad\lightning
\end{flalign*}
The contradiction follows from $1>yu-\floor{yu}$ and $y>1$. We can use these observations to derive
\begin{flalign*}
	&&yd&>\floor{\hat{u}}&\stackrel{\hat{u}\neq\floor{\hat{u}}}{\iff}&&yd&>\ceil{\hat{u}}-1&\stackrel{\floor{u}\ge d}{\implies}&&y\floor{u}&>\ceil{\hat{u}}-1&&\\
	\stackrel{u\neq\floor{u}}{\iff}&&y(\ceil{u}-1)&>\ceil{\hat{u}}-1&\iff&&y\ceil{u}&>\ceil{\hat{u}}+y-1&\stackrel{y>1}{\implies}&&y\ceil{u}&>\ceil{\hat{u}}&&
\end{flalign*}
Now recall that in case~\ref{itm:schedule_behavior_down_up_y_subcase_2} the schedule~$\mx$ descended at time $h$ to $[u,\hat{u})$ and further descends at time $i$. This means that we considered case~\ref{itm:schedule_behavior_down_down_y} for the transformation of~$\mx$ between~$h$ and $i-1$. But since $u$ is the lower bound and $\hat{u}$ the upper bound of~$\mx$ between~$h$ and $i-1$, and we proved that $y\ceil{u}>\ceil{\hat{u}}$, we know from case~\ref{itm:schedule_behavior_down_down_y} that $x_{i-1}'=\ceil{\hat{u}}$. Consequently,~$\mx_{i,j}^{\ceil{\hat{u}}}$ does not require switching costs to move to $\ceil{\hat{u}}$ at time $i$. Thus, to finish the proof, it suffices to show that the following cost difference is non-negative:
\begin{align*}
	&y\bigl(\overbrace{\opcosts\bigl(\mx_{i,j}\bigr)+\beta(x_{j+1}-d)}^{\text{costs of }\mx}\bigr)-\bigl(\overbrace{\opcosts\bigl(\mx_{i,j}^{\ceil{\hat{u}}}\bigr)+\beta(\floor{u'}-\ceil{\hat{u}})}^{\text{costs of }\mx_{i,j}^{\ceil{\hat{u}}}}\bigr)-\overbrace{\beta(yx_{j+1}-\floor{u'})}^{\text{new credit}}\\
	\stackrel{\phantom{\eqref{eq:mx_schedule_op_costs_complete}}}{=}\quad&y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\ceil{\hat{u}}}\bigr)+\beta(yx_{j+1}-yd-\floor{u'}+\ceil{\hat{u}}-yx_{j+1}+\floor{u'})\\
	\stackrel{\phantom{\eqref{eq:mx_schedule_op_costs_complete}}}{=}\quad&y\opcosts\bigl(\mx_{i,j}\bigr)-\opcosts\bigl(\mx_{i,j}^{\ceil{\hat{u}}}\bigr)+\beta(\ceil{\hat{u}}-yd)\\
	\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}\quad&\sum\limits_{t=i}^{j}\left(yx_tf\left(\frac{\lambda_t}{x_t}\right)-\ceil{\hat{u}}f\left(\frac{\lambda_t}{\ceil{\hat{u}}}\right)\right)+\beta(\ceil{\hat{u}}-yd)
\end{align*}
We again apply the usual estimations and rewritings to derive
\begin{flalign*}
	&&&&&&&\sum\limits_{t=i}^{j}\left(yx_tf\left(\frac{\lambda_t}{x_t}\right)-\ceil{\hat{u}}f\left(\frac{\lambda_t}{\ceil{\hat{u}}}\right)\right)+\beta(\ceil{\hat{u}}-yd)&&&&&\\
	&&&&\ge&&&\sum\limits_{t=i}^{j}\left(ydf\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)-\ceil{\hat{u}}f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\right)+\beta(\ceil{\hat{u}}-yd)&&&&&\\
	&&&&=&&&\sum\limits_{t=i}^{j}\left((yd-\ceil{\hat{u}})f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\right)+\beta(\ceil{\hat{u}}-yd)&&&&&\\
	&&&&=&&&\sum\limits_{t=i}^{j}\left(-(\ceil{\hat{u}}-yd)f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\right)+\beta(\ceil{\hat{u}}-yd)&&&&&\\
	&&&&=&&&\Biggl(\underbrace{\beta-\sum\limits_{t=i}^{j}f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)}_{>0}\Biggr)(\underbrace{\ceil{\hat{u}}-yd}_{\ge 0})\ge 0&&&&&
\end{flalign*}
where we used Inequality~\eqref{eq:beta_greater_sum_op_floor_u_hat} as well as $d\le u$ and $\hat{u}=yu$ for the last step. This finishes case~\ref{itm:schedule_behavior_down_up_y}.

By iteratively applying above procedure to $\mx$, starting with $i=1$ and stopping with $j=T+1$, we obtain a $B$-restricted schedule $\mx'$ that satisfies $\costs(\mx')\le y\costs(\mx)$.
\end{proof}
The remaining verification of our graph proceeds similar to Section~\ref{sec:approx_2_opt}.
\begin{lem}\label{lem:sched_reasn_path_approx_y}
Let $\bm{\mx}$ be the set of all $B$-restricted schedules for $\inp$, and let $\bm{\mathcal{P}}$ be the set of all reasonable paths. The map
\begin{equation*}
	\Phi:\bm{\mathcal{P}}\rightarrow\bm{\mx},\quad P\mapsto (x_1,\dotsc,x_T)
\end{equation*}
is a bijection satisfying $\costs(P)=\costs\bigl(\Phi(P)\bigr)$.
\end{lem}
\begin{thm}\label{thm:approx_y}
Any shortest reasonable path $P$ corresponds to a $y$-optimal, $B$-restricted schedule $\mx$ for $\inp$ with $\costs(P)=\costs(\mx)$.
\end{thm} 
\begin{cor}
Any shortest path $P$ from $v_{0,0}$ to $v_{0,T\downarrow}$ can be transformed to a \makebox{$y$-optimal}, $B$-restricted schedule $\mx$ for $\inp$ with $\costs(P)=\costs(\mx)$.
\end{cor}
\begin{proof}
The proofs are, once the necessary changes have been made (i.e.\ substituting $2$ with~$y$), the same as the ones of Lemma~\ref{lem:sched_reasn_path_approx_2}, Theorem~\ref{thm:approx_2}, and Corollary~\ref{cor:approx_2_sched_short_path}.
\end{proof}
To no surprise, the final algorithm will be almost identical to the algorithm of our \makebox{2-approximation}. The only major thing that requires a change is the auxiliary function \textproc{nodes}, which has to account for the new set of possible scheduling choices $B$. Our runtime analysis consequently also stays very similar. Subroutine \textproc{shortest\_paths} requires $\Theta\bigl(2\log_y(m)\bigr)$ steps for its initialization and \makebox{$\Theta\bigl(2T\cdot2\log_y(m)\bigr)$} steps for the iterative calculation of the selections and costs. Further, \textproc{extract\_schedule} needs $\Theta(T)$ iterations for its schedule retrieval. Hence, we receive a time complexity of
\begin{equation*}
	\Theta\bigl(2\log_y(m)+2T\cdot2\log_y(m)+T\bigr)=\Theta(T\log_y(m))
\end{equation*}
that is, the runtime is linear in the size of the input. Similarly, our memory demand amounts to $\Theta\bigl(T\log_y(m)\bigr)$ since the size of the tables $C$ and $S$ is given by $\Theta\bigl(2T\log_y(m)\bigr)$.

\newpage
\begin{algorithm}[H]
  \caption{$(1+\beps)$-optimal linear-time offline scheduling}
  \label{alg:approx_y_offline_linear}
  \begin{algorithmic}[1]
  \Function{$(1+\beps)$\_optimal\_offline\_scheduling}{$m,T,\Lambda,\beta,f,\beps$}
	  \Let{$(C,S)$}{\Call{shortest\_paths}{$m,T,\Lambda,\beta,f,1+\beps$}}
	  \Let{$\mx$}{\Call{extract\_schedule}{$S,T,m,1+\beps$}}
	  \State \Return{$\mx$}
  \EndFunction
  \Statex
  \Function{node}{$i,m,y$}
	\If {$i\bmod2=0$}$\ $\Return{$\min\bigl\{m,\floor{y^{\floor{\frac{i-1}{2}}}}\bigr\}$}
	\Else$\ $\Return{$\min\bigl\{m,\ceil{y^{\floor{\frac{i-1}{2}}}}\bigr\}$}
	\EndIf
  \EndFunction
  \Statex
  \Function{shortest\_paths}{$m,T,\Lambda,\beta,f,y$}
	\Let{$b$}{$\floor{\log_y(m)}+1$}
	\Blet{$C[0\dotso 2b+1,1\dotso T]$ and $S[0\dotso 2b+1,1\dotso T]$}{new tables}
	\Let{$S[2b+1,1]$}{$2b+1$ and $C[2b+1,1]\gets\costs(0,m,\lambda_1)$}\Comment{Initialize first node}
	\For{$i \gets 2b \textrm{ to } 0$}\Comment{Initialize first layer (downward minimization step)}
		\If {$C[i+1,1]<\costs\bigl(0,\Call{node}{i,m,y},\lambda_1\bigr)$}
			\Let{$S[i,1]$}{$S[i+1,1]$ and $C[i,1]\gets C[i+1,1]$}
		\Else
			\Let{$S[i,1]$}{$i$ and $C[i,1]\gets\costs\bigl(0,\Call{node}{i,m,y},\lambda_1\bigr)$}
		\EndIf
	\EndFor
	\For{$t \gets 1 \textrm{ to } T-1$}\Comment{Iterative calculate costs and selections}
		\For{$i \gets 1 \textrm{ to } 2b+1$}\Comment{Upward minimization step}
			\If {$C[i-1,t]+\beta\bigl(\Call{node}{i,m,y}-\Call{node}{i-1,m,y}\bigr)<C[i,t]$}
			    \Let{$S[i,t]$}{$S[i-1,t]$}
			    \Let{$C[i,t]$}{$C[i-1,t]+\beta\bigl(\Call{node}{i,m,y}-\Call{node}{i-1,m,y}\bigr)$}
			\EndIf
		\EndFor
		\Let{$S[2b+1,t+1]$}{$2b+1$ and $C[2b+1,t+1]\gets C[2b+1,t]+\opcosts(m,\lambda_{t+1})$}
		\For{$i \gets 2b \textrm{ to } 0$}\Comment{Downward minimization step}
			\If {$C[i+1,t+1]<C[i,t]+\opcosts\bigl(\Call{node}{i,m,y},\lambda_{t+1}\bigr)$}
				\Let{$S[i,t+1]$}{$S[i+1,t+1]$ and $C[i,t+1]\gets C[i+1,t+1]$}
			\Else
				\Let{$S[i,t+1]$}{$i$ and $C[i,t+1]\gets C[i,t]+\opcosts\bigl(\Call{node}{i,m,y},\lambda_{t+1}\bigr)$}
			\EndIf
		\EndFor
	\EndFor
	\State \Return{$(C,S)$}
  \EndFunction
  \Statex
  \Function{extract\_schedule}{$S,T,m,y$}
	\Blet{$\mx[1\dotso T]$}{a new array}
	\Let{$i$}{$S[0,T]$}\Comment{Get index of best selection for last time slot}
    	\Let{$\mx[T]$}{$\Call{node}{i,m,y}$}\Comment{Calculate best selection for last time slot}
        \For{$t \gets T-1 \textrm{ to } 1$}\Comment{Iteratively obtain schedule from selection table}
		\Let{$i$}{$S[i,t]$}
		\Let{$\mx[t]$}{$\Call{node}{i,m,y}$}	
	\EndFor
	\State \Return{$\mx$}
  \EndFunction
  \end{algorithmic}
\end{algorithm}
