% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Approximative Offline Scheduling}\label{chap:approx_offline_scheduling}
Heretofore, we have derived two optimal offline algorithms for our scheduling problem. Unfortunately, the algorithms' time complexities are exponential in the input size of the number of servers $m$. Needless to say, we want to reduce this exponential runtime. For this, we must slightly loosen our aspirations, that is we move to approximative methods. 
Further, in the course of the next section, we will see that we need to assume that our convex operating cost function $f$ is non-negative and monotonically increasing; however, this restriction is of no great significance in practice, as will be discussed later.

In this chapter, we will first modify our algorithm derived in Section~\ref{sec:opt_offline_pseudo_lin} to obtain a 2-optimal offline algorithm with linear time complexity. To clarify, given a number $y\in\mathbb{R}$, we say that a scheduling algorithm is \emph{$y$-optimal} if its calculated solution incurs at most $y$ times as much cost as an optimal solution does. Similarly, we say that a schedule/operation is \emph{$y$-approximative} if its cost is at most $y$ times as much as much as the original schedule's/operation's cost. \unsure{definition $y$-approx/opt.\ okay?}
As a final step of this thesis, we will generalize the 2-optimal algorithm to derive an $(1+\beps)$-optimal algorithm with TODO time complexity.

\section{A 2-Optimal Linear-Time Algorithm}
Recall our algorithm and its corresponding graph $G$ derived in Section~\ref{sec:opt_offline_pseudo_lin}. The algorithm's time complexity of $\Theta(Tm)$ is determined by the number of nodes and edges of $G$. Since we desire to reduce our runtime complexity, we need to reduce the number of nodes and edges in $G$. In particular, we must get rid of the factor $m$. This factor is a consequence of the ``height'' of our graph, i.e.\ the number of nodes in each layer. Therefore, we have to ``thin out'' $G$ by reducing its number of nodes in each layer.

As we saw in Equation~\eqref{eq:inp_size}, the size of our input $\inp$ is given by $\mathcal{O}\bigl(T\log_2(m)+\log_2(\beta)\bigr)$. Consequently, in order to obtain a linear time complexity, we want to reduce the graph's height from $m+1$ to a logarithmic height of $\mathcal{O}\bigl(\log(m)\bigr)$. Given this observation, it seems natural for a computer scientist to choose a logarithmic scale for the number of servers in each layer, to wit, instead of adding a node for each possible number of active servers (i.e.\ $0,1,\dotsc,m$), we only add nodes for logarithmic choices (i.e.\ $0,2^0,2^1,\dotsc,2^{\floor{\log_2(m)}},m$). More formally, given a problem instance $\inp$, we set
\begin{align*}
	b&\coloneqq\floor{\log_2(m)}\\
	B&\coloneqq\{0,2^0,2^1,\dotsc,2^b,m\}
\end{align*}
where $B$ will subsequently represent the set of possible scheduling choices at each time slot. Using this set of possible choices, we can then consider the following adaption of our former graph:
\begin{align*}
	V&\coloneqq\bigl\{v_{x,t\downarrow}\mid x\in B,t\in[T]\bigr\}\dotcup\bigl\{v_{x,t\uparrow}\mid x\in B, t\in[T-1]\bigr\}\dotcup\{v_{0,0}\}\\
	E_s&\coloneqq\bigl\{(v_{0,0},v_{x,1\downarrow})\mid x\in B\bigr\}\\
	E_\downarrow&\coloneqq\bigl\{(v_{2^i,t\downarrow},v_{2^{i-1},t\downarrow})\mid i\in[b],t\in[T]\bigr\}\dotcup\bigl\{(v_{2^0,t\downarrow},v_{0,t\downarrow})\mid t\in[T]\bigr\}\dotcup\\
	&\phantom{{}\coloneqq{}}\bigl\{(v_{m,t\downarrow},v_{2^b,t\downarrow})\mid t\in[T]\bigr\}\\
	E_\uparrow&\coloneqq\bigl\{(v_{2^{i-1},t\uparrow},v_{2^i,t\uparrow})\mid i\in[b],t\in[T-1]\bigr\}\dotcup\bigl\{(v_{0,t\uparrow},v_{2^0,t\uparrow})\mid t\in[T-1]\bigr\}\dotcup\\
	&\phantom{{}\coloneqq{}}\bigl\{(v_{2^b,t\uparrow},v_{m,t\uparrow})\mid t\in[T-1]\bigr\}\\
	E_{\downarrow\uparrow}&\coloneqq\bigl\{(v_{x,t\downarrow},v_{x,t\uparrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E_{\uparrow\downarrow}&\coloneqq\bigl\{(v_{x,t\uparrow},v_{x,t+1\downarrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E&\coloneqq E_s\dotcup E_\downarrow\dotcup E_\uparrow\dotcup E_{\downarrow\uparrow}\dotcup E_{\uparrow\downarrow}\\
	c_G(e)&\coloneqq
	\begin{cases}
		\costs(0,x,\lambda_1), & \text{if $e=(v_{0,0},v_{x,1\downarrow})\in E_s$}\\
		\opcosts(x,\lambda_{t+1}), & \text{if $e=(v_{x,t\uparrow},v_{x,t+1\downarrow})\in E_{\uparrow\downarrow}$}\\
		(x'-x)\beta, & \text{if $e=(v_{x,t\uparrow},v_{x',t\uparrow})\in E_\uparrow$}\\
		0, & \text{if $e\in(E_\downarrow\dotcup E_{\downarrow\uparrow})$}
	\end{cases}\\
	G&\coloneqq(V,E,c_G)
\end{align*}
If $m$ is a power of two, i.e.\ $m=2^b$, it happens that we add unnecessary loops in~$E_\downarrow$ and~$E_\uparrow$ with cost $0$, which we can simply ignore for our following works.
A graphical representation of $G$ can be found in the following figure.
\begin{figure}[H]
\includestandalone[width=\textwidth]{../figures/graph_lin_approx_2}
\caption{Graph for a 2-optimal linear-time offline algorithm; the path of the topological sorting is highlighted in red. Note that $\beta(2^i-2^{i-1})=\beta2^{i-1}$.}
\label{fig:graph_lin_approx_2}
\end{figure}
The nodes' and edges' semantical meaning and the graph's working principle stays similar to that given in Section~\ref{sec:opt_offline_pseudo_lin}. Again, by following the colored path of the topological sorting, we can work our way through the graph to calculate the shortest paths, ultimately reaching the destination $v_{0,T\downarrow}$. However, since some possible scheduling choices are not representable in this new graph, we may just obtain approximative costs for our nodes. Thus, the shortest path in our graph might not correspond to an optimal schedule, but it will at least correspond to an approximative one. Before we start to establish the graph's approximation guarantee, we first have to conduct some observations. We start by making a convenient definition that helps us to identify schedules that are representable in our graph.
\begin{defn}[Restricted schedules]
Given an input $\inp$ and a set $A\subseteq[m]_0$, we say that a schedule $\mx=(x_1,\dotsc,x_T)$ is \emph{$A$-restricted} if $\mx$ only uses scheduling choices contained in~$A$, that is $\mx$ satisfies the formula $\forall t\in[T]:x_t\in A$.
\end{defn}
Evidently, our graph is able to represent every $B$-restricted schedule. We now examine the incurring operating costs of such a $B$-restricted schedule $\mx'$. Since we are forced to schedule a number of servers contained in $B$, we might not be able to choose an optimal scheduling choice that minimizes the schedule's operating costs. Instead, we may choose the nearest scheduling choice which is contained in $B$. For instance, if the optimal scheduling strategy at some timeslot $t$ would be to choose $x_t=3$ servers (which is not a power of two), we may instead have to choose $x_t'=4\in B$ servers for $\mx'$. One might suspect that this strategy would incur at most twice as much operating costs as an optimal schedule. This, however, is sadly not the case, as one can see in the following example.
\begin{exmpl}
Let $\inp=\bigl(m=4,T=5,\Lambda=(3,3,3,3,3),\beta=0,f\bigr)$ be the input for a problem instance where $f(\lambda)=(\lambda-1)^2$. Since we need at least 3 active servers at any timeslot, any $B$-restricted schedule $\mx'=(x_1',\dotsc,x_5')$ forces us to constantly use $x_t'=4$ active machines. An optimal schedule $\mx=(x_1,\dotsc,x_5)$, on the other hand, is able to minimize its cost by constantly scheduling $x_t=3$ servers. Let us compare the costs between $\mx'$ and $\mx$. The schedules' costs are given by
\begin{align*}
	\costs(\mx)&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_tf(\lambda_t/x_t)}^{3(3/3-1)^2}+\overbrace{\swcosts(x_{t-1},x_t)}^{0\max\{\,\cdots\}}\bigr)=5\cdot3\left(\frac{3}{3}-1\right)^2=0\\
	\costs(\mx')&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_t'f(\lambda_t/x_t')}^{4(3/4-1)^2}+\overbrace{\swcosts(x_{t-1}',x_t')}^{0\max\{\,\cdots\}}\bigr)=5\cdot4\left(\frac{3}{4}-1\right)^2=\frac{20}{16} 
\end{align*}
Albeit our approximative schedule uses only one server in addition, the schedule's cost is already inestimably higher than that of an optimal schedule $\mx$, preventing any sensible approximation estimation. Naturally, we may ask ourselves how this explosion of costs is even possible. Evidently, the switching costs are not the root of this explosion since $\beta=0$. Thus, we shall take a closer look on the used operating cost function. The optimal schedule $\mx$ evenly distributes every load $\lambda_t=3$ to $x_t=3$ servers. Hence, every active server has to process a load of $\lambda_t/x_t=1$ at every time step, incurring costs of $f(1)=0$. On the other hand, the approximative schedule $\mx'$ is able to distribute every load to 4 active machines. Thus, every machine incurs costs of $f(3/4)=\frac{1}{16}$. This observation seems rather surprising: Although every server has to process a smaller load using $\mx'$, the incurring operating costs of each server turn out to be higher. Intuitively, however, we would expect that a less stressed machine would incur less costs. This surprising behavior is due to the fact that our operating cost function $f$ is not monotonically increasing, as one can see in the following figure.
\begin{figure}[H]
\centering
\includestandalone{../figures/non_mono_incr_f_1}
\caption{Example of a non monotonically increasing operating cost function \makebox{$f(\lambda)=(\lambda-1)^2$}, where smaller loads incur higher costs.}
\label{fig:non_mono_incr_f}
\end{figure}
\end{exmpl}
The above example shows us that our graph may not able to deliver a sensible approximation when dealing with general convex operating cost functions $f$. Luckily, this inconvenience can be solved by additionally assuming that $f$ is non-negative and monotonically increasing. To see this, assume that at some timeslot $t$ the scheduling choice $x_t$ minimizes the operating costs to process the load $\lambda_t$. Then let $x_t'\in B$ the next scheduling choice representable in~$G$. Since $x_t$ minimizes the operating costs at timeslot $t$, we have
\begin{equation*}
	\opcosts(x_t,\lambda_t)\le\opcosts(x_t',\lambda_t)\stackrel{\eqref{eq:mx_schedule_op_costs}}{=}x_t'f(\lambda_t/x_t')
\end{equation*}
Further, since $B$ contains all powers of two up to $m$, we have $x_t\le x_t'\le 2x_t$. If we additionally assume that $f$ is non-negative, we can infer that
\begin{equation*}
	x_t'f(\lambda_t/x_t')\le2x_tf(\lambda_t/x_t')
\end{equation*}
Now, using the fact that $x_t\le x_t'$ and assuming that $f$ is monotonically increasing, we can see that
\begin{equation*}
	2x_tf(\lambda_t/x_t')\le 2x_tf(\lambda_t/x_t)\stackrel{\eqref{eq:mx_schedule_op_costs}}{=}2\opcosts(x_t,\lambda_t)
\end{equation*}
Ultimately, we can combine our observations and conclude
\begin{equation*}
	\opcosts(x_t,\lambda_t)\le\opcosts(x_t',\lambda_t)\le2\opcosts(x_t,\lambda_t)
\end{equation*}
which shows us that our approximative scheduling choice incurs at most twice as much operating costs as an optimal scheduling strategy. We thus subsequently restrict ourselves to non-negative, monotonically increasing convex cost functions. This evidently reduces the theoretical generality of our initial approach, but it does not interfere with practical applicability. On the one hand, negative cost functions would semantically allow to ``generate profit by consuming energy'', which seems unreasonable in practice. On the other hand, if we have a non monotonically increasing convex cost function, we can simply add artifical loads to our machines to reduce our costs in given circumstances. For instance, in our previous example, we could assign every machine a load of $1$ instead of $\frac{3}{4}$ to reduce the approximative schedule's cost. This trick, which is exemplarily outlined in Figure~\ref{fig:transform_to_mono_incr}, allows us to transform any arbitrary convex cost function to a monotonically increasing one.
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.5\textwidth}
\includestandalone[width=\textwidth]{./../figures/non_mono_incr_f_2}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.5\textwidth}
\includestandalone[width=\textwidth]{../figures/mono_incr_f}
\end{subfigure}
\caption{The non monotonically increasing convex function $f$ can be transformed to the monotonically increasing convex function $f'$ by adding artifical loads $\lambda^+$ to assignments $\lambda\in[0,0.5)$ such that we obtain a new assignment $\lambda'\coloneqq\lambda+\lambda^+=0.5$.}
\label{fig:transform_to_mono_incr}
\end{figure}
Next, we examine the incurring switching costs of a $B$-restricted schedule $\mx'$. Again, given an optimal scheduling choice $x_t$, we use the idea to choose the nearest scheduling choice $x_t'$ that is contained in $B$ to construct $\mx'$. Once more, one might hope that $\mx'$ would incur at most twice as much switching costs as an optimal schedule. Needless to say, the next example dashes this hope.
\begin{exmpl}\label{exmpl:oscillating_schedule}
Let $\inp=\bigl(m=16,T=5,\Lambda=(9,7,9,7,9),\beta=1,f\bigr)$ be the input for a problem instance where $f(\lambda)=0$. Our possible scheduling choices are then given by $B=\{0,1,2,4,8,16\}$, and one optimal schedule is given by $\mx=(9,7,9,7,9)$. The $B$-restricted schedule corresponding to $\mx$ is then given by $\mx'=(16,8,16,8,16)$. The schedules' costs amount to
\begin{align*}
	\costs(\mx)&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_tf(\lambda_t/x_t)}^{x_t\cdot 0}+\overbrace{\swcosts(x_{t-1},x_t)}^{\max\{0,x_t-x_{t-1}\}}\bigr)=9+0+2+0+2=13\\
	\costs(\mx')&\stackrel{\eqref{eq:mx_schedule_total_costs}}{=}\sum\limits_{t=1}^{5}\bigl(\overbrace{x_t'f(\lambda_t/x_t')}^{x_t'\cdot 0}+\overbrace{\swcosts(x_{t-1}',x_t')}^{\max\{0,x_t'-x_{t-1}'\}}\bigr)=16+0+8+0+8=32
\end{align*}
which shows that $\mx'$ is not 2-approximative. Of course, we are again curious about how this cost explosion is possible. Since we set $f(\lambda)=0$, our servers do not incur operating costs, which means that the cost explosion must be due to the increased switching costs of $\mx'$. The problem in this case is the oscillating behavior of $\mx$ around a power of two (namely $8=2^3$), as one can see in the following figure. 
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{./../figures/switching_costs_not_2_opt_1}
	\caption{\underline{Optimal schedule $\mx$:} Note how the schedule oscillates around 8 (a power of two).}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{../figures/switching_costs_not_2_opt_2}
	\caption{\underline{Approximative schedule $\mx'$:} The approximative schedule is restricted to powers of two.}
\end{subfigure}
\caption{Comparison between an optimal schedule and its approximative counterpart. The approximative schedule incurs more than twice as much switching costs.}
\label{fig:adaption-schedule}
\end{figure}
\end{exmpl}
So, is this the end of our hunt for a 2-optimal algorithm? No, certainly not! Although our naive approach was to no avail, there is indeed a better $B$-restricted schedule for our example. Instead of following the optimal schedule's oscillation, we can simply use a schedule that stays put during these oscillating steps, namely the schedule $\mx'=(16,16,16,16,16)$ with cost $\costs(\mx')=16$. Obviously, this seems like a rather trivial example since we set $f(\lambda)=0$, and hence we do not need to worry about the new schedule's operating costs. However, it indeed turns out that making the right choice between following the optimal schedule's oscillation and staying put will always allow us to acquire a 2-optimal solution. This observation will be a key part of the next lemma's proof. 

In the proof, we are going to divide a schedule $\mx$ into periods at which its plot crosses a power of two. We then show that every such period can be transformed to a period in a $B$-restricted schedule $\mx'$ such that the transformed period incurs at most twice as much costs. In order to formalize how to exactly split our schedules into such periods, we make a handy definition.
\begin{defn}[2-state changes]
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule and $t\in[T+1]$. We say that $\mx$ changes its \emph{2-state} at time $t$ if $\mx$ satisfies the formula
\begin{equation*}
	x_{t-1}\neq x_t\land \left(x_{t-1}=0 \lor x_t\notin\left[2^{\floor{\log_2(x_{t-1})}},2^{\floor{\log_2(x_{t-1})}+1}\right)\right)
\end{equation*}
\end{defn}
For example, the schedule $\mx=(5,4,8,7,8,15,10,16)$ changes its 2-state at times \makebox{$t\in\{1,3,4,5,8,9\}$}. As one can see in Figure~\ref{fig:schedule_2_states}, we can say that $\mx$ changes its 2-state if its plot ascends and touches a next higher power of two or descends and leaves the current pair of powers of two. Using this notion of 2-state changes, we are geared up to deal with the next lemma -- the main work of this section.
\begin{figure}[H]
\centering
\includestandalone[width=0.7\textwidth]{../figures/schedule_2_states}
\caption{Plot of the schedule $\mx=(5,4,8,7,8,15,10,16)$ and its 2-state changes}
\label{fig:schedule_2_states}
\end{figure}
\begin{lem}\label{lem:transform_schedule_approx_2}
Let $\mx$ be a schedule for $\inp$. There exists a $B$-restricted schedule $\mx'$ satisfying \makebox{$\costs(\mx')\le2\costs(\mx)$}.
\end{lem}
\begin{proof}
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule for $\inp$. We need to construct a $B$-restricted schedule $\mx'=(x_1',\dotsc,x_T')$ such that \makebox{$\costs(\mx')\le2\costs(\mx)$}. \unsure{Is the lemma's structure (e.g.\ the use of the credit) clearer than before?}
First, if $\mx$ is not feasible, we have $\costs(\mx)=\infty$, and thus any arbitrary $B$-restricted schedule $\mx'$ satisfies $\costs(\mx')\le2\costs(\mx)$; hence, assume that $\mx$ is feasible. Next, we notice that if $\mx$ shuts down all its servers at some timeslot $t\in[T]$ (i.e.\ $x_t=0$), we can split $\mx$ into two subschedules \makebox{$\mx_1\coloneqq(x_1,\dotsc,x_{t-1})$} and \makebox{$\mx_2\coloneqq(x_{t+1},\dotsc,x_T)$}. It then suffices to prove the claim for $\mx_1$ and $\mx_2$, since we can then construct the 2-approximative schedule by setting $x_t'\coloneqq 0$ and $\mx'\coloneqq(\mx_1',x_t',\mx_2')$. Thus, by recursively applying this method, we can reduce our proof to a list of subschedules $\mx_1,\dotsc,\mx_N$ that never shut down all servers. Consequently, without loss of generality, we subsequently assume that $\mx$ never powers down all its servers, that is $x_t>0$ for all $t\in[T]$. 
	
Next, we show that we can iteratively construct $\mx'$ by transforming every period between two 2-state changes of $\mx$ to a 2-approximative period in $\mx'$. 
To prove that our transformations will be 2-approximative, we have to conduct an amortized analysis for the switching costs of $\mx'$ using the \emph{accounting method}. The basic idea of the accounting method is to overcharge some operations and to save the excess charge as a \emph{credit}, which can be used to compensate for subsequent, more expensive operations. An introduction about the accounting method can be found in~\parencite[Section~17.2]{intro-algo}. To see the necessity of such an amortized analysis, and to get a basic idea about its working principle, consider the schedule $\mx=(7,9)$ and its approximative counterpart $\mx'=(8,16)$. Although the total switching costs of $\mx'$ are 2-approximative, the individual switching steps of $\mx'$ are not, since $\mx'$ has to turn on $8$ machines at time $t=2$ while $\mx$ only turns on $2$ machines. However, at time $t=1$, $\mx'$ only turns on $8$ servers while it would be allowed to turn on $2\cdot7=14$. We can thus overcharge the first switching operation and use the excess charge $14-8=6$ as a credit to compensate for the second switching step, which -- to no suprise -- exactly misses $6$ machines.
	
For our iterative construction, let $i$ and $j+1$ with \makebox{$i,j\in[T]$} and $i\le j$ be the first unprocessed timeslots at which $\mx$ changes its \makebox{2-state}. To conveniently refer to the schedules' periods between $i$ and $j$, we define the subschedules \makebox{$\mx_{i,j}\coloneqq(x_i,\dotsc,x_j)$} and $\mx'_{i,j}\coloneqq(x_i',\dotsc,x_j')$. We then have to show that $\mx_{i,j}$ can be transformed to $\mx_{i,j}'$ with 2-approximative costs. Note that the periods are consecutive, i.e.\ after processing the period $[i,j]$, the next pair of indices $i',j'$ will be chosen such that $j+1=i'$. To conduct the transformations, we will need to refer to the lower and upper bound of~$\mx_{i,j}$, namely
\begin{flalign*}
	&&l\coloneqq2^{\floor{\log_2(x_i)}}&&\text{and}&&u\coloneqq\min\bigl\{2l,m\bigr\}&&&
\end{flalign*}
as well as to the lower and upper bound of $\mx$ at time $j+1$:
\begin{flalign*}
	&&l'\coloneqq\begin{cases}
		2^{\floor{\log_2(x_{j+1})}}, & \text{if $x_{j+1}\neq 0$}\\
		0, & \text{if $x_{j+1}=0$}
	\end{cases}
&&\text{and}&&u'\coloneqq\min\bigl\{2l',m\bigr\}&&&
\end{flalign*}
Note that $u,u'\in B$ and that $x_t\le u\le 2x_t$ holds for any $i\le t\le j$ since $\mx$ does not change its 2-state between $i$ and $j$.

As an \emph{invariant} of the following transformations, we are going to ensure that $\mx'$ can potentially move to $u$ at time $i$ in a 2-approximative manner, i.e.\ we ensure that~$\mx'$ incurs at most twice as much switching costs as $\mx$ if we set $x_i'\coloneqq u$ -- whether this step will be taken in the end or not. Further, we are going to ensure that $x_t'\ge u$ holds for any $i\le t\le j$ after every transformation step. Since $x_t\le u$, this guarantees that $\mx'$ will be feasible.
	
First, we have to check that our invariant initially holds. For the initial start-up process \makebox{(i.e.\ $i=1$)}, we can simply move to the next power of 2 larger than $x_1$ contained in $B$, that is we set $x_1'\coloneqq u\in B$. Since we know that $x_1'\le2x_1$, we can conclude that $\beta x_1'\le\beta 2x_1$, which shows that $\mx'$ has 2-approximative start-up switching costs. Thus, the invariant initially holds. Moreover, we can use the difference of switching costs $\beta(2x_1-x_1')=\beta(2x_1-u)$ as a credit for our amortized analysis.

Now, let us have a closer look on the possible behaviors of $\mx$ between its 2-state changes. The behaviors can be classified based on how $\mx$ enters and leaves the interval $[l,u)$. We have to consider four different cases:
\begin{enumerate}[label=(\alph*)]
	\item $\mx$ enters $[l,u)$ from below and then descends to $[l',u')$.\label{itm:schedule_behavior_up_down}
	\item $\mx$ enters $[l,u)$ from above and then descends to $[l',u')$.\label{itm:schedule_behavior_down_down}
	\item $\mx$ enters $[l,u)$ from below and then ascends to $[l',u')$.\label{itm:schedule_behavior_up_up} 
	\item $\mx$ enters $[l,u)$ from above and then ascends to $[l',u')$.\label{itm:schedule_behavior_down_up} 
\end{enumerate}
To finish the proof, we need to show that any case can be transformed to a 2-approximative period in $\mx'$ while ensuring that our invariant will hold at the beginning of the next period, i.e.\ that we can set $x_{j+1}\coloneqq u'$ with 2-approximative switching costs. Additionally, we have to verify that the credit of our amortized analysis stays non-negative. In particular, we will show that our credit will be greater than or equal to $\beta(2x_{j+1}-u')$ if $\mx$ exits $[l,u)$ by ascending to $[l',u')$ at time $j+1$. Since the periods are consecutive, this equivalently means that the credit will be greater than or equal to $\beta(2x_i-u)$ if $\mx$ enters $[l,u)$ at time~$i$ from below. Note that we already showed that this claim holds for our initial start-up process.
	
We begin with cases~\ref{itm:schedule_behavior_up_down} and~\ref{itm:schedule_behavior_down_down}, which are both depicted in Figure~\ref{fig:schedule_behavior_down}.
\begin{figure}[ht]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_up_down}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_down}
\end{subfigure}
\caption{The original schedule comes from below (case~\ref{itm:schedule_behavior_up_down}) or above (case~\ref{itm:schedule_behavior_down_down}), stays between $[l,u)$, and then descends to $[l',u')$. The approximative schedule stays put at $u$ for timeslots $i\le t\le j$.}
\label{fig:schedule_behavior_down}
\end{figure}
Due to our invariant, we know that we can set $x_i'\coloneqq u$ with 2-approximative switching costs. Consequently, setting $x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq u\in B$ gives us a strategy with 2-approximative switching costs between $i$ and $j$. Further, since $x_t\le u\le2x_t$ for any $i\le t\le j$, and $f$ is non-negative and monotonically increasing, the operating costs of $\mx'_{i,j}$ can be estimated by
\begin{equation*}
	\opcosts(\mx_{i,j}')\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}\sum\limits_{t=i}^j\left(uf\left(\frac{\lambda_t}{u}\right)\right)\le\sum\limits_{t=i}^j\left(2x_tf\left(\frac{\lambda_t}{u}\right)\right)\le2\sum\limits_{t=i}^j\left(x_tf\left(\frac{\lambda_t}{x_t}\right)\right)\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}2\opcosts(\mx_{i,j})
\end{equation*}
Thus, the operating costs of $\mx_{i,j}'$ are 2-approximative. Further, since $u'<u$, we do not need to account for additional switching costs to satisfy our invariant at time $j+1$; however, we want to stress that one cannot tell at this step if we should indeed set $x_{j+1}\coloneqq u'$ (c.f. Figure~\ref{fig:schedule_behavior_down_up} and its related case). Lastly, we note that the credit of our amortized analysis stays untouched in both cases, i.e.\ the credit stays non-negative.
	
Next, we consider case~\ref{itm:schedule_behavior_up_up}, which is illustrated in Figure~\ref{fig:schedule_behavior_up_up}.
\begin{figure}[ht]
\centering
\includestandalone[width=0.5\textwidth]{../figures/schedule_behavior_up_up}	
\caption{The original schedule (case~\ref{itm:schedule_behavior_up_up}) comes from below, stays between $[l,u)$, and then ascends to $[l',u')$. The approximative schedule stays put at $u$ for timeslots $i\le t\le j$.}
\label{fig:schedule_behavior_up_up}
\end{figure}
Again, due to our invariant, we know that setting $x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq u\in B$ gives us a strategy with \makebox{2-approximative} switching costs. Moreover, as in the previous case, the operating costs of~$\mx_{i,j}'$ are \makebox{2-approximative}. In order to verify that our invariant holds at time $j+1$, we notice that $\mx$ has to power on at least $x_{j+1}-x_i$ servers between $i$ and $j+1$. Thus, it suffices to show that $\beta(u'-u)\le2\beta(x_{j+1}-x_i)$ holds; however, it is clear that this must not be the case (just take $u'=8,u=4,x_{j+1}=4,x_i=3$). Nevertheless, since we entered $[l,u)$ from below, we know that our credit is greater than or equal to $\beta(2x_i-u)$. Hence, we can use our credit to compensate for our costs:
\begin{align*}
	2\beta(x_{j+1}-x_i)+\overbrace{\beta(2x_i-u)}^{\text{credit}}=\beta(2x_{j+1}-u)\ge\beta(u'-u)
\end{align*}
where the last inequality follows from $u'\le2x_{j+1}$. Thus, our invariant at time $j+1$ is satisfied. Further, we can use the difference $\beta(2x_{j+1}-u)-\beta(u'-u)=\beta(2x_{j+1}-u')$ as our new credit for subsequent operations. 
	
Finally, we have to consider case~\ref{itm:schedule_behavior_down_up}. As we have seen in Example~\ref{exmpl:oscillating_schedule}, this case turns out to be slightly more complicated, since simply following an oscillating behavior of $\mx$ can lead to a cost explosion for $\mx'$. Nevertheless, we can solve this issue by considering two different strategies for $\mx_{i,j}'$, namely
\begin{flalign*}
	&&\mx_{i,j}^u&\coloneqq\bigl(x_i^u,\dotsc,x_j^u\bigr),&&\text{where}&x_t^u\coloneqq u,\,\text{for } i\le t\le j&&&&\\
	&&\text{and}\quad\mx_{i,j}^{\hat{u}}&\coloneqq\bigl(x_i^{\hat{u}},\dotsc,x_j^{\hat{u}}\bigr),&&\text{where}&x_t^{\hat{u}}\coloneqq \hat{u},\,\text{for } i\le t\le j&&&&
\end{flalign*}
with $\hat{u}\coloneqq\min\{2u,m\}\in B$. Both strategies are illustrated in Figure~\ref{fig:schedule_behavior_down_up}.
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_1}
\caption{Strategy $\mx_{i,j}^u$ stays put at $u$ for $i\le t\le j$.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\includestandalone[width=\textwidth]{../figures/schedule_behavior_down_up_2}
\caption{Strategy $\mx_{i,j}^{\hat{u}}$ stays put at $\hat{u}$ for $i\le t\le j$.}
\end{subfigure}
\caption{The original schedule (case~\ref{itm:schedule_behavior_down_up}) comes from above, stays between $[l,u)$, and then rises to $[l',u')$. The approximative schedule has two different possibilities.}
\label{fig:schedule_behavior_down_up}
\end{figure}
Due to our invariant and the fact that $\mx$ descends at time $i$, we know that $x_{i-1}'\ge \hat{u}\ge u$, and thus both strategies do no incur switching costs up to time $j$ (but possibly at time $j+1$). 
We are now going to prove that either the cost of $\mx_{i,j}^u$ or of $\mx_{i,j}^{\hat{u}}$, including possible switching costs to satisfy our invariant at time $j+1$, must be 2-approximative. 
First, we examine the switching costs of $\mx$ from $i$ up to $j+1$. To do so, we set $d\coloneqq\min\{x_t\mid i\le t\le j\}$ to refer to the smallest number of active servers used by $\mx_{i,j}$. Then, since $\mx$ has to power on at least $x_{j+1}-d$ servers between $i$ and $j+1$, we can give a lower bound for its switching costs:
\begin{equation*}
	\sum\limits_{t=i+1}^{j+1}\swcosts(x_{t-1},x_t)\ge\beta(x_{j+1}-d)
\end{equation*}
The next idea is to split $\mx_{i,j}$ and its associated switching costs at the final switching step into two parts; more precisely, we split $\beta(x_{j+1}-d)$ into $\beta(x_{j+1}-u)$ and $\beta(u-d)$. It then suffices to show that either $\mx_{i,j}^u$ or $\mx_{i,j}^{\hat{u}}$ can process the loads $\lambda_i,\dotsc,\lambda_j$ and eventually switch to $\hat{u}$ with 2-approximative costs under the assumption that $\mx_{i,j}$ only switches to $u$ as a first step. This is due to the fact that the remaining switching costs to ascend from $\hat{u}$ to $u'$ are then 2-approximative anyway:
\begin{equation*}
	\beta(u'-\hat{u})\le\beta(2x_{j+1}-\hat{u})=\beta(2x_{j+1}-2u)=2\beta(x_{j+1}-u)
\end{equation*}
where we assumed that $\hat{u}=2u$ holds; otherwise, we have $u'=\hat{u}=m$ and the inequality trivially holds. Further, if $\hat{u}=2u$, we can again use the difference of costs \makebox{$2\beta(x_{j+1}-u)-\beta(u'-\hat{u})=\beta(2x_{j+1}-u')$} as a credit for subsequent operations. If $\hat{u}=m$, no further credit is needed, since we already reached the limit of servers $m$.

To proceed, we notice that if $\hat{u}-u\le2(u-d)$ holds, strategy $\mx_{i,j}^u$ has 2-approximative switching costs:
\begin{equation*}
	\beta(\hat{u}-u)\le2\beta(u-d)
\end{equation*}
Further, as in the previous cases, the operating costs of $\mx_{i,j}^u$ are 2-approximative, and thus~$\mx_{i,j}^u$ is 2-approximative if $\hat{u}-u\le2(u-d)$ holds. Hence, we subsequently assume that 
\begin{align}\label{eq:proof_2_competitive_asm_dist_pos}
	\hat{u}-u>2(u-d)&&\text{or equivalently}&&\hat{u}-3u+2d>0
\end{align}
Next, by the law of excluded middle, the schedule $\mx_{i,j}^u$ is either 2-approximative, or it is not \makebox{2-approximative}. If it is 2-approximative, we are done; hence, from now on, we assume that $\mx_{i,j}^u$ is not 2-approximative, that is
\begin{equation*}
	\opcosts(\mx_{i,j}^u)+\beta(\hat{u}-u)>2\bigl(\opcosts(\mx_{i,j})+\beta(u-d)\bigr)
\end{equation*}
We then have to show that $\mx_{i,j}^{\hat{u}}$ is 2-approximative.
First note that $\mx_{i,j}^u$ uses at most twice as many active servers as $\mx_{i,j}$, and therefore $\mx_{i,j}^u$ incurs at most twice as much operating costs as $\mx_{i,j}$. Consequently, the switching costs of $\mx_{i,j}^u$ must significantly outweigh those of $\mx_{i,j}$. Hence, it seems interesting to get an estimation for $\beta$. Rearranging the previous inequality gives us
\begin{align*}
	&&\opcosts(\mx_{i,j}^u)+\beta(\hat{u}-u)&>2\bigl(\opcosts(\mx_{i,j})+\beta(u-d)\bigr)&\\
	&\stackrel{\phantom{\hat{u}-3u+2d)>0}}{\iff}&\beta(\hat{u}-3u+2d)&>2\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^u)&\\
	&\stackrel{\hat{u}-3u+2d>0}{\iff}&\beta&>\frac{2\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}
\end{align*}
where the last step is justified by Assumption~\eqref{eq:proof_2_competitive_asm_dist_pos}.
This allows us to find a lower bound for the cost of $\mx_{i,j}$:
\begin{align*}
	\opcosts(\mx_{i,j})+\beta(u-d)&>\opcosts(\mx_{i,j})+\frac{2\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}(u-d)\\
	&=\frac{(\hat{u}-3u+2d)\opcosts(\mx_{i,j})+2(u-d)\opcosts(\mx_{i,j})-(u-d)\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}\\
	&=\frac{(\hat{u}-u)\opcosts(\mx_{i,j})-(u-d)\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}
\end{align*}
Next, since we want to prove that $\mx_{i,j}^{\hat{u}}$ is 2-approximative, we have to show that the following cost difference is non-negative:
\begin{align*}
	2\bigl(\opcosts(\mx_{i,j})+\beta(u-d)\bigr)-\opcosts(\mx_{i,j}^{\hat{u}})
\end{align*}
Recall that $\mx_{i,j}^{\hat{u}}$ does not incur switching costs to move to $\hat{u}$, and thus we only have to consider its operating costs.
We can now use our just derived lower bound for \makebox{$\opcosts(\mx_{i,j})+\beta(u-d)$} to estimate the difference:
\begin{align*}
	2\bigl(\opcosts(\mx_{i,j})+\beta(u-d)\bigr)-\opcosts(\mx_{i,j}^{\hat{u}})>2\,\frac{(\hat{u}-u)\opcosts(\mx_{i,j})-(u-d)\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}-\opcosts(\mx_{i,j}^{\hat{u}})
\end{align*}
Thus, it suffices to show that
\begin{align*}
	2\,\frac{(\hat{u}-u)\opcosts(\mx_{i,j})-(u-d)\opcosts(\mx_{i,j}^u)}{\hat{u}-3u+2d}-\opcosts(\mx_{i,j}^{\hat{u}})\ge 0
\end{align*}
which, using Assumption~\eqref{eq:proof_2_competitive_asm_dist_pos}, that is $\hat{u}-3u+2d>0$, can be simplified to
\begin{equation*}
	2(\hat{u}-u)\opcosts(\mx_{i,j})-2(u-d)\opcosts(\mx_{i,j}^u)-(\hat{u}-3u+2d)\opcosts(\mx_{i,j}^{\hat{u}})\ge 0
\end{equation*}
To show this inequality, we have to take a closer look on the schedules' operating costs:
\begin{align*}
	&&&2(\hat{u}-u)\opcosts(\mx_{i,j})-2(u-d)\opcosts(\mx_{i,j}^u)-(\hat{u}-3u+2d)\opcosts(\mx_{i,j}^{\hat{u}})\\
	&\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)x_tf\left(\frac{\lambda_t}{x_t}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{\hat{u}}\right)\right)
\end{align*}
First, we notice that $u\le\hat{u}$ and $x_t<u$ for $i\le t\le j$. Together with the fact that $f$ is monotonically increasing, we infer that
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)x_tf\left(\frac{\lambda_t}{x_t}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{\hat{u}}\right)\right)\\
	&\ge&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)x_tf\left(\frac{\lambda_t}{u}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{u}\right)\right)
\end{align*}
Since $d$ is the smallest number of active servers scheduled by $\mx$, and $f$ is non-negative, we can conclude that
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)x_tf\left(\frac{\lambda_t}{u}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{u}\right)\right)\\
	&\ge&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)df\left(\frac{\lambda_t}{u}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{u}\right)\right)
\end{align*}
Hence, to finish the case, it suffices to show that
\begin{equation*}
	\sum\limits_{t=i}^j\left(2(\hat{u}-u)df\left(\frac{\lambda_t}{u}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{u}\right)\right)\ge 0
\end{equation*}
Further rearranging the left hand side gives us
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(2(\hat{u}-u)df\left(\frac{\lambda_t}{u}\right)-2(u-d)uf\left(\frac{\lambda_t}{u}\right)-(\hat{u}-3u+2d)\hat{u}f\left(\frac{\lambda_t}{u}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(\Bigl(2(\hat{u}-u)d-2(u-d)u-(\hat{u}-3u+2d)\hat{u}\Bigr)f\left(\frac{\lambda_t}{u}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(\Bigl(2d\hat{u}-2du-2u^2+2du-\hat{u}^2+3u\hat{u}-2d\hat{u}\Bigr)f\left(\frac{\lambda_t}{u}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(\left(-\hat{u}^2+3u\hat{u}-2u^2\right)f\left(\frac{\lambda_t}{u}\right)\right)=\sum\limits_{t=i}^j\left(\underbrace{-(u-\hat{u})(2u-\hat{u})}_{g(\hat{u})}f\left(\frac{\lambda_t}{u}\right)\right) 
\end{align*}
Now let $g(\hat{u})\coloneqq-(u-\hat{u})(2u-\hat{u})$. Since $f$ is non-negative, it suffices to show that $g(\hat{u})\ge0$ for all possible values of $\hat{u}$, namely $\hat{u}\in[u,2u]$. We notice that $g(\cdot)$ is an inverted parabola with roots $u$ and $2u$. Thus, we know that $g(u)=g(2u)=0$ and $g(\hat{u})>0$ for $u<\hat{u}<2u$, which finishes the case.

By iteratively applying above procedure to $\mx$, starting with $i=1$ and stopping with $j=T+1$, we obtain a $B$-restricted schedule $\mx'$ that satisfies $\costs(\mx')\le2\costs(\mx)$.
\end{proof}
Safe in the knowledge that there always exists a 2-approximative $B$-restricted schedule $\mx'$ for any schedule $\mx$, we are just left with the verification of our modified graph. Since we did not change the graph's general construction idea, this verification turns out to be very similar to that done in Section~\ref{sec:opt_offline_pseudo_lin}. For the proof of the next lemma, which establishes the bijection between $B$-restricted schedules and reasonable paths, we need to recall our notion of ``maximum'' nodes $x_t$ in reasonable paths $P$, as described in Definition~\ref{defn:max_path_node}.
\begin{lem}\label{lem:sched_reasn_path_approx_2}
Let $\bm{\mx}$ be the set of all $B$-restricted schedules for $\inp$, and let $\bm{\mathcal{P}}$ be the set of all reasonable paths. The map
\begin{equation*}
	\Phi:\bm{\mathcal{P}}\rightarrow\bm{\mx},\quad P\mapsto (x_1,\dotsc,x_T)
\end{equation*}
is a bijection satisfying $\costs(P)=\costs\bigl(\Phi(P)\bigr)$.
\end{lem}
\begin{proof}
The proof is analogous to the proof of Lemma~\ref{lem:sched_reasn_path_pseudo_lin} considering that we conduct logarithmic instead of incremental switching steps.
\end{proof}
Finally, we arrive at the main result of this section.
\begin{thm}\label{thm:approx_2}
Any shortest reasonable path $P$ corresponds to a 2-optimal, $B$-restricted schedule $\mx$ for $\inp$ with $\costs(P)=\costs(\mx)$.
\end{thm} 
\begin{proof}
By Lemma~\ref{lem:sched_reasn_path_approx_2}, we have a bijection $\Phi$ between reasonable paths $P$ and $B$-restricted schedules obeying $\costs(P)=\costs\bigl(\Phi(P)\bigr)$. Thus, we have 
\begin{equation*}
	\costs(P)\text{ minimal}\iff \costs\bigl(\Phi(P)\big)\text{ minimal}
\end{equation*}
Now let $P$ be a shortest reasonable path, and let $\mx^*$ be an optimal schedule for $\inp$. We have to verify that $\costs\bigl(\Phi(P)\bigr)\le 2\costs(\mx^*)$. By Lemma~\ref{lem:transform_schedule_approx_2}, we know that there exists a $B$-restricted schedule $\mx'$ such that $\costs(\mx')\le 2\costs(\mx^*)$. Since $\Phi$ is a bijection, and $P$ is a shortest reasonable path, we know that $\costs\bigl(\Phi(P)\bigr)\le\costs(\mx')$. Thus, we conclude that 
\begin{equation*}
	\costs(P)=\costs\bigl(\Phi(P)\bigr)\le\costs(\mx')\le 2\costs(\mx^*)
\end{equation*}
and the claim follows.
\end{proof}
Again, it is not difficult to show that also shortest paths which are not reasonable can be transformed to a desired 2-optimal schedule.
\begin{cor}
Any shortest path $P$ from $v_{0,0}$ to $v_{0,T\downarrow}$ can be transformed to a \makebox{2-optimal}, $B$-restricted schedule $\mx$ for $\inp$ with $\costs(P)=\costs(\mx)$.
\end{cor}
\begin{proof}
Let $P$ be a shortest path from $v_{0,0}$ to $v_{0,T\downarrow}$. By using a small adaption of Proposition~\ref{prop:path_to_reasn_path} that uses logarithmic instead of incremental switching steps, we can transform $P$ to a reasonable path $P'$ with $\costs(P')=\costs(P)$.
In turn, $P'$ corresponds to a 2-optimal, \makebox{$B$-restricted} schedule $\mx$ with $\costs(\mx)=\costs(P')=\costs(P)$ by Theorem~\ref{thm:approx_2}, which finishes the proof.
\end{proof}
To finish this section, we give an algorithm based on verified constructions. Naturally, since we did not change the graph's overall structure and working principle, a small adaption of Algorithm~\ref{alg:opt_offline_pseudo_linear} will serve its purpose. To account for the logarithmic steps in our graph, we introduce an auxiliary function \textproc{nodes}, which calculates the number of servers associated to an given index $i$ in our tables $C$ and $S$. As a matter of implementation convenience, the variable $b$ is defined as $\ceil{\log_2(m)}$ instead of $\floor{\log_2(m)}$ in Algorithm~\ref{alg:approx_2_offline_linear}.
The correctness of the algorithm directly follows from Theorem~\ref{thm:approx_2} and the correctness of the shortest path calculation for directed acyclic graphs (for a proof see~\parencite[Section~24.2]{intro-algo}).\unsure{always cite this?}

For our runtime analysis, we again take the same assumptions as done for Algorithm~\ref{alg:opt_offline_pseudo_poly}. Subroutine \textproc{shortest\_paths} requires $\Theta\bigl(\log_2(m)\bigr)$ steps for its initialization and \makebox{$\Theta\bigl(2T\log_2(m)\bigr)$} steps for the iterative calculation of the selections and costs\unsure{Repetitive? Same words as in chapter 4}. Further, \textproc{extract\_schedule} needs $\Theta(T)$ iterations for its schedule retrieval. Hence, we receive a time complexity of
\begin{equation*}
	\Theta\bigl(\log_2(m)+2T\log_2(m)+T\bigr)=\Theta(T\log_2(m))
\end{equation*}
To our great joy, the runtime is linear in the size of the input. Similarly, our memory demand is reduced to linear space $\Theta\bigl(T\log_2(m)\bigr)$, since the size of the tables $C$ and $S$ shrinked to $\Theta\bigl(T\log_2(m)\bigr)$.

We saw in this section that our reduction to logarithmic steps allows us to derive a 2-optimal linear algorithm. In our approach, we chose the base two logarithm for our step sizes. It seems like an interesting question whether this approach can be generalized to arbitrary bases, allowing for more precise approximations. The answer of this question shall be final task of this thesis.
\begin{algorithm}[H]
  \caption{2-optimal linear-time offline scheduling}
  \label{alg:approx_2_offline_linear}
  \begin{algorithmic}[1]
  \Function{2\_optimal\_offline\_scheduling}{$m,T,\Lambda,\beta,f$}
	  \Let{$(C,S)$}{\Call{shortest\_paths}{$m,T,\Lambda,\beta,f$}}
	  \Let{$\mx$}{\Call{extract\_schedule}{$S,T,m$}}
	  \State \Return{$\mx$}
  \EndFunction
  \Statex
  \Function{node}{$i,m$}
	  \State \Return{$\min\bigl\{m,\floor{2^{i-1}}\bigr\}$}
  \EndFunction
  \Statex
  \Function{shortest\_paths}{$m,T,\Lambda,\beta,f$}
	\Let{$b$}{$\ceil{\log_2(m)}$}
	\Blet{$C[0\dotso b+1,1\dotso T]$ and $S[0\dotso b+1,1\dotso T]$}{new tables}
	\Statex \Comment{Allocate cost and selection tables}
	\Let{$S[b+1,1]$}{$b+1$ and $C[b+1,1]\gets\costs(0,m,\lambda_1)$}\Comment{Initialize first node in first layer}
	\For{$i \gets b \textrm{ to } 0$}\Comment{Initialize first layer (downward minimization step)}
		\If {$C[i+1,1]<\costs\bigl(0,\Call{node}{i,m},\lambda_1\bigr)$}
			\Let{$S[i,1]$}{$S[i+1,1]$ and $C[i,1]\gets C[i+1,1]$}
		\Else
			\Let{$S[i,1]$}{$i$ and $C[i,1]\gets\costs\bigl(0,\Call{node}{i,m},\lambda_1\bigr)$}
		\EndIf
	\EndFor
	\For{$t \gets 1 \textrm{ to } T-1$}\Comment{Iterative calculate costs and selections}
		\For{$i \gets 1 \textrm{ to } b+1$}\Comment{Upward minimization step}
			\If {$C[i-1,t]+\beta\bigl(\Call{node}{i,m}-\Call{node}{i-1,m}\bigr)<C[i,t]$}
			    \Let{$S[i,t]$}{$S[i-1,t]$}
			    \Let{$C[i,t]$}{$C[i-1,t]+\beta\bigl(\Call{node}{i,m}-\Call{node}{i-1,m}\bigr)$}
			\EndIf
		\EndFor
		\Let{$S[b+1,t+1]$}{$b+1$ and $C[b+1,t+1]\gets C[b+1,t]+\opcosts(m,\lambda_{t+1})$}
		\For{$i \gets b \textrm{ to } 0$}\Comment{Downward minimization step}
			\If {$C[i+1,t+1]<C[i,t]+\opcosts\bigl(\Call{node}{i,m},\lambda_{t+1}\bigr)$}
				\Let{$S[i,t+1]$}{$S[i+1,t+1]$ and $C[i,t+1]\gets C[i+1,t+1]$}
			\Else
				\Let{$S[i,t+1]$}{$i$ and $C[i,t+1]\gets C[i,t]+\opcosts\bigl(\Call{node}{i,m},\lambda_{t+1}\bigr)$}
			\EndIf
		\EndFor
	\EndFor
	\State \Return{$(C,S)$}
  \EndFunction
  \Statex
  \Function{extract\_schedule}{$S,T,m$}
	\Blet{$\mx[1\dotso T]$}{a new array}
	\Let{$i$}{$S[0,T]$}\Comment{Get index of best selection for last time slot}
    	\Let{$\mx[T]$}{$\Call{node}{i,m}$}\Comment{Calculate best selection for last time slot}
        \For{$t \gets T-1 \textrm{ to } 1$}\Comment{Iteratively obtain schedule from selection table}
		\Let{$i$}{$S[i,t]$}
		\Let{$\mx[t]$}{$\Call{node}{i,m}$}	
	\EndFor
	\State \Return{$\mx$}
  \EndFunction
  \end{algorithmic}
\end{algorithm}

\section{A $(1+\beps)$-Optimal Linear-Time Algorithm}
TODO text
\begin{align*}
	y&\coloneqq1+\beps\\
	b&\coloneqq\floor{\log_y(m)}\\
	B&\coloneqq\left\{0,\floor{y^0},\floor{y^1},\dotsc,\floor{y^b},m\right\}
\end{align*}
where $B$ represents the set of possible scheduling choices at each time slot. We can then consider the following adaption of our graph:
\begin{align*}
	V&\coloneqq\bigl\{v_{x,t\downarrow}\mid x\in B,t\in[T]\bigr\}\dotcup\bigl\{v_{x,t\uparrow}\mid x\in B, t\in[T-1]\bigr\}\dotcup\{v_{0,0}\}\\
	E_s&\coloneqq\bigl\{(v_{0,0},v_{x,1\downarrow})\mid x\in B\bigr\}\\
	E_\downarrow&\coloneqq\bigl\{(v_{\floor{y^i},t\downarrow},v_{\floor{y^{i-1}},t\downarrow})\mid i\in[b],t\in[T]\bigr\}\dotcup\bigl\{(v_{\floor{y^0},t\downarrow},v_{0,t\downarrow})\mid t\in[T]\bigr\}\dotcup\\
	&\phantom{{}\coloneqq{}}\bigl\{(v_{m,t\downarrow},v_{\floor{y^b},t\downarrow})\mid t\in[T]\bigr\}\\
	E_\uparrow&\coloneqq\bigl\{(v_{\floor{y^{i-1}},t\uparrow},v_{\floor{y^i},t\uparrow})\mid i\in[b],t\in[T-1]\bigr\}\dotcup\bigl\{(v_{0,t\uparrow},v_{\floor{y^0},t\uparrow})\mid t\in[T-1]\bigr\}\dotcup\\
	&\phantom{{}\coloneqq{}}\bigl\{(v_{\floor{y^b},t\uparrow},v_{m,t\uparrow})\mid t\in[T-1]\bigr\}\\
	E_{\downarrow\uparrow}&\coloneqq\bigl\{(v_{x,t\downarrow},v_{x,t\uparrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E_{\uparrow\downarrow}&\coloneqq\bigl\{(v_{x,t\uparrow},v_{x,t+1\downarrow})\mid x\in B,t\in[T-1]\bigr\}\\
	E&\coloneqq E_s\dotcup E_\downarrow\dotcup E_\uparrow\dotcup E_{\downarrow\uparrow}\dotcup E_{\uparrow\downarrow}\\
	c_G(e)&\coloneqq
	\begin{cases}
		\costs(0,x,\lambda_1), & \text{if $e=(v_{0,0},v_{x,1\downarrow})\in E_s$}\\
		\opcosts(x,\lambda_{t+1}), & \text{if $e=(v_{x,t\uparrow},v_{x,t+1\downarrow})\in E_{\uparrow\downarrow}$}\\
		(x'-x)\beta, & \text{if $e=(v_{x,t\uparrow},v_{x',t\uparrow})\in E_\uparrow$}\\
		0, & \text{if $e\in(E_\downarrow\dotcup E_{\downarrow\uparrow})$}
	\end{cases}\\
	G&\coloneqq(V,E,c_G)
\end{align*}
Unnecessary loops can again simply be ignored for our following works.
A graphical representation of $G$ can be found in Figure~\ref{fig:graph_lin_approx_y}.
\begin{figure}[H]
\includestandalone[width=\textwidth]{../figures/graph_lin_approx_y}
\caption{Graph for a $y$-optimal linear-time offline algorithm; the path of the topological sorting is highlighted in red.}
\label{fig:graph_lin_approx_y}
\end{figure}
TODO text
\begin{defn}[y-state changes]
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule and $t\in[T+1]$. We say that $\mx$ changes its \emph{$y$-state} at time $t$ if $\mx$ satisfies the formula
\begin{equation*}
	x_{t-1}\neq x_t\land \left(x_{t-1}=0 \lor x_t\notin\left[y^{\floor{\log_y(x_{t-1})}},y^{\floor{\log_y(x_{t-1})}+1}\right)\right)
\end{equation*}
\end{defn}
The proof of the next lemma will be very similar to that done in Lemma~\ref{lem:transform_schedule_approx_2}.
\begin{lem}
Let $\mx$ be a schedule for $\inp$. There exists a $B$-restricted schedule $\mx'$ satisfying \makebox{$\costs(\mx')\le y\costs(\mx)$}.
\end{lem}
\begin{proof}
Let $\mx=(x_1,\dotsc,x_T)$ be a schedule for $\inp$. We need to construct a $B$-restricted schedule $\mx'=(x_1',\dotsc,x_T')$ such that \makebox{$\costs(\mx')\le y\costs(\mx)$}. By following the same reasoning as done for Lemma~\ref{lem:transform_schedule_approx_2}, we can again assume that $\mx$ is feasible and never shuts down all its servers. We then have to show that every period between two $y$-state changes of $\mx$ can be iteratively transformed to a $y$-approximative period in $\mx'$. 
The proof will again require an amortized analysis for the switching costs of $\mx'$ using the accounting method. For our iterative transformation, let $i$ and $j+1$ with \makebox{$i,j\in[T]$} and $i\le j$ be the first unprocessed timeslots at which $\mx$ changes its \makebox{$y$-state}. We define the lower and upper bound of $\mx_{i,j}$ as
\begin{flalign*}
	&&l\coloneqq y^{\floor{\log_y(x_i)}}&&\text{and}&&u\coloneqq\min\bigl\{yl,m\bigr\}&&&
\end{flalign*}
and the lower and upper bound of $\mx$ at time $j+1$ as
\begin{flalign*}
	&&l'\coloneqq\begin{cases}
		y^{\floor{\log_y(x_{j+1})}}, & \text{if $x_{j+1}\neq 0$}\\
		0, & \text{if $x_{j+1}=0$}
	\end{cases}
&&\text{and}&&u'\coloneqq\min\bigl\{yl',m\bigr\}&&&
\end{flalign*}
In the proof for our 2-approximation, we knew that $u$ and $u'$ must be integer, and thus we were able to simply assign $u$ or $u'$ servers for $\mx'$. However, as $y$ is not necessarily integer anymore, $u$ and $u'$ can also be non-integer, and thus $\mx'$ may not be able to schedule~$u$ or~$u'$ servers. Nevertheless, we can solve this issue by using $\floor{u}\in B$ and $\floor{u'}\in B$ servers instead.
Note that $\mx$ does not change its $y$-state between $i$ and $j$, which means that $l\le x_t\le u$ holds for $i\le t\le j$. Since $x_t$ is integer and $\floor{u}$ is the largest integer contained in~$[l,u]$, we can even refine our upper bound to $x_t\le\floor{u}$. Further, by using the fact that $u\le yl$, we can conclude that $x_t\le\floor{u}\le yx_t$ holds for any $i\le t\le j$.

We adapt the invariant used in Lemma~\ref{lem:transform_schedule_approx_2} in that we are going to ensure that $\mx'$ can potentially move to $\floor{u}$ at time $i$ in a $y$-approximative manner, i.e.\ we ensure that~$\mx'$ incurs at most $y$ times as much switching costs as $\mx$ if we set $x_i'\coloneqq \floor{u}$. Further, we are going to ensure that $x_t'\ge \floor{u}$ holds for any $i\le t\le j$ after every transformation step. Since $x_t\le \floor{u}$, this guarantees that $\mx'$ will be feasible.

For the initial start-up process (i.e.\ $i=1$), we can simply set $x_1'\coloneqq \floor{u} \in B$. Since we know that $x_1'\le yx_1$, we can conclude that $\beta x_1'\le\beta yx_1$; thus, the start-up switching costs of $\mx'$ are $y$-competitive and the invariant initially holds. The difference of switching costs $\beta(yx_1-x_1')=\beta(yx_1-\floor{u})$ can be used as a credit for our amortized analysis.

Now, let us have a closer look on the possible behaviors of $\mx$ between its $y$-state changes. Recall the four possible cases:
\begin{enumerate}[label=(\alph*)]
	\item $\mx$ enters $[l,u)$ from below and then descends to $[l',u')$.\label{itm:schedule_behavior_up_down_y}
	\item $\mx$ enters $[l,u)$ from above and then descends to $[l',u')$.\label{itm:schedule_behavior_down_down_y}
	\item $\mx$ enters $[l,u)$ from below and then ascends to $[l',u')$.\label{itm:schedule_behavior_up_up_y} 
	\item $\mx$ enters $[l,u)$ from above and then ascends to $[l',u')$.\label{itm:schedule_behavior_down_up_y} 
\end{enumerate}
To finish the proof, we need to show that any case can be transformed to a $y$-approximative period in $\mx'$ while ensuring that our invariant will hold at the beginning of the next period. Additionally, we have to verify that the credit of our amortized analysis stays non-negative. We will in particular show that our credit will be greater than or equal to $\beta(yx_{j+1}-\floor{u'})$ if $\mx$ exits $[l,u)$ by ascending to $[l',u')$ at time $j+1$. Since the periods are consecutive, this equivalently means that the credit will be greater than or equal to $\beta(yx_i-\floor{u})$ if $\mx$ enters $[l,u)$ at time~$i$ from below. We already showed that this claim holds for our initial start-up process.

We start with cases~\ref{itm:schedule_behavior_up_down_y} and~\ref{itm:schedule_behavior_down_down_y}. Due to our invariant, we know that we can set $x_i'\coloneqq \floor{u}$ with $y$-approximative switching costs. Consequently, setting $x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq \floor{u}\in B$ gives us a strategy with $y$-approximative switching costs between $i$ and $j$. Further, since $x_t\le \floor{u}\le yx_t$ for any $i\le t\le j$, and $f$ is non-negative and monotonically increasing, the operating costs of $\mx'_{i,j}$ can be estimated by
\begin{align*}
	\opcosts(\mx_{i,j}')&\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}\sum\limits_{t=i}^j\left(\floor{u} f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\le\sum\limits_{t=i}^j\left(yx_tf\left(\frac{\lambda_t}{\floor{u}}\right)\right)\le y\sum\limits_{t=i}^j\left(x_tf\left(\frac{\lambda_t}{x_t}\right)\right)\\
	&\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}y\opcosts(\mx_{i,j})
\end{align*}
Thus, the operating costs of $\mx_{i,j}'$ are $y$-approximative. Further, since $\floor{u'}<\floor{u}$, we do not need to account for additional switching costs to satisfy our invariant at time $j+1$. Lastly, the credit of our amortized analysis stays untouched, i.e.\ it stays non-negative.
	
For case~\ref{itm:schedule_behavior_up_up_y}, we again know that setting \makebox{$x_i'\coloneqq x_{i+1}'\coloneqq\dotsb\coloneqq x_j'\coloneqq \floor{u}\in B$} gives us a strategy with $y$-approximative switching costs due to our invariant. Moreover, as in the previous case, the operating costs of $\mx_{i,j}'$ are $y$-approximative. To verify our invariant at time $j+1$, we use the fact that $\mx$ has to power on at least $x_{j+1}-x_i$ servers between~$i$ and $j+1$. Thus, we would have to show that $\beta(\floor{u'}-\floor{u})\le y\beta(x_{j+1}-x_i)$ holds. However, since we entered $[l,u)$ from below, we can additionally use our credit to compensate for our costs:
\begin{align*}
	y\beta(x_{j+1}-x_i)+\overbrace{\beta(yx_i-\floor{u})}^{\text{credit}}=\beta(yx_{j+1}-\floor{u})\ge\beta(\floor{u'}-\floor{u})
\end{align*}
where the last inequality follows from $\floor{u'}\le yx_{j+1}$. Thus, our invariant at time $j+1$ is satisfied. The difference $\beta(yx_{j+1}-\floor{u})-\beta(\floor{u'}-\floor{u})=\beta(yx_{j+1}-\floor{u'})$ can further be used as our new credit. 
	
Finally, we have to consider case~\ref{itm:schedule_behavior_down_up_y}, for which we again consider two different strategies, namely
\begin{flalign*}
	&&\mx_{i,j}^{\floor{u}}&\coloneqq\bigl(x_i^{\floor{u}},\dotsc,x_j^{\floor{u}}\bigr),&&\text{where}&x_t^{\floor{u}}\coloneqq \floor{u},\,\text{for } i\le t\le j&&&&\\
	&&\text{and}\quad\mx_{i,j}^{\floor{\hat{u}}}&\coloneqq\bigl(x_i^{\floor{\hat{u}}},\dotsc,x_j^{\floor{\hat{u}}}\bigr),&&\text{where}&x_t^{\floor{\hat{u}}}\coloneqq \floor{\hat{u}},\,\text{for } i\le t\le j&&&&
\end{flalign*}
with $\hat{u}\coloneqq\min\{yu,m\}$. Due to our invariant and the fact that $\mx$ descends at time $i$, we know that $x_{i-1}'\ge\floor{\hat{u}}\ge\floor{u}$, and thus both strategies do no incur switching costs up to time $j$.
We are now going to prove that either the cost of $\mx_{i,j}^{\floor{u}}$ or of $\mx_{i,j}^{\floor{\hat{u}}}$, including possible switching costs to satisfy our invariant at time $j+1$, must be $y$-approximative. 
First, we define $d\coloneqq\min\{x_t\mid i\le t\le j\}$ to be the smallest number of active servers used by $\mx_{i,j}$. We can use $d$ to give a lower bound for the switching costs of $\mx$ between $i$ and~$j+1$:
\begin{equation*}
	\sum\limits_{t=i+1}^{j+1}\swcosts(x_{t-1},x_t)\ge\beta(x_{j+1}-d)
\end{equation*}
Then, we again split $\mx_{i,j}$ and its associated switching costs at the final switching step into two parts. This time, we split $\beta(x_{j+1}-d)$ into $\beta(x_{j+1}-\frac{\floor{\hat{u}}}{y})$ and $\beta(\frac{\floor{\hat{u}}}{y}-d)$. We have to check that this split is ``well-defined'', i.e.\ that $d\le	\frac{\floor{\hat{u}}}{y}\le x_{j+1}$ holds. Since $u\le x_{j+1}$ and $\hat{u}\le yu$, we have $\frac{\floor{\hat{u}}}{y}\le x_{j+1}$.

The single step that fails. \unsure{what if $d>\floor{\hat{u}}/y$?}
	
It now suffices to show that either $\mx_{i,j}^u$ or $\mx_{i,j}^{\hat{u}}$ can process the loads $\lambda_i,\dotsc,\lambda_j$ and eventually switch to $\floor{\hat{u}}$ with $y$-approximative costs under the assumption that $\mx_{i,j}$ only switches to~$\frac{\floor{\hat{u}}}{y}$ as a first step. This is due to the fact that the remaining switching costs to ascend from $\floor{\hat{u}}$ to $\floor{u'}$ are then $y$-competitive anyway:
\begin{equation*}
	\beta(\floor{u'}-\floor{\hat{u}})\le\beta(yx_{j+1}-\floor{\hat{u}})=y\beta\left(x_{j+1}-\frac{\floor{\hat{u}}}{y}\right)
\end{equation*}
Further, we can then again use the difference of costs for our credit:
\begin{equation*}
	y\beta\left(x_{j+1}-\frac{\floor{\hat{u}}}{y}\right)-\beta(\floor{u'}-\floor{\hat{u}})=\beta(yx_{j+1}-\floor{u'})
\end{equation*}
To proceed, we can assume that $\mx_{i,j}^{\floor{u}}$ is not $y$-approximative; otherwise, we are already done. Hence, assume that 
\begin{equation*}
	\opcosts(\mx_{i,j}^{\floor{u}})+\beta(\floor{\hat{u}}-\floor{u})>y\left(\opcosts(\mx_{i,j})+\beta\left(\frac{\floor{\hat{u}}}{y}-d\right)\right)
\end{equation*}
We then have to show that $\mx_{i,j}^{\floor{\hat{u}}}$ is $y$-approximative.
Rearranging the previous inequality gives us an estimation for $\beta$:
\begin{align*}
	&&\opcosts(\mx_{i,j}^{\floor{u}})+\beta(\floor{\hat{u}}-\floor{u})&>y\left(\opcosts(\mx_{i,j})+\beta\left(\frac{\floor{\hat{u}}}{y}-d\right)\right)
&\\
	&\stackrel{\phantom{yd-\floor{u}>0}}{\iff}&\beta(yd-\floor{u})&>y\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^{\floor{u}})&\\
	&\stackrel{yd-\floor{u}>0}{\iff}&\beta&>\frac{y\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}
\end{align*}
where the last step is justified since $l\le d\le u\le yl$.
This allows us to find a lower bound for the cost of $\mx_{i,j}$:
\begin{align*}
	\opcosts(\mx_{i,j})+\beta\left(\frac{\floor{\hat{u}}}{y}-d\right)&>\opcosts(\mx_{i,j})+\frac{y\opcosts(\mx_{i,j})-\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}\left(\frac{\floor{\hat{u}}}{y}-d\right)\\
	&=\frac{(yd-\floor{u})\opcosts(\mx_{i,j})+(\floor{\hat{u}}-yd)\opcosts(\mx_{i,j})-\left(\frac{\floor{\hat{u}}}{y}-d\right)\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}\\
	&=\frac{(\floor{\hat{u}}-\floor{u})\opcosts(\mx_{i,j})-\left(\frac{\floor{\hat{u}}}{y}-d\right)\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}
\end{align*}
Next, since we want to prove that $\opcosts(\mx_{i,j}^{\floor{\hat{u}}})$ is $y$-approximative, we have to show that the following cost difference is non-negative:
\begin{align*}
	y\left(\opcosts(\mx_{i,j})+\beta\left(\frac{\floor{\hat{u}}}{y}-d\right)\right)-\opcosts(\mx_{i,j}^{\floor{\hat{u}}})
\end{align*}
Recall that $\mx_{i,j}^{\floor{\hat{u}}}$ does not incur switching costs to move to $\floor{\hat{u}}$. We can now use our just derived lower bound for $\opcosts(\mx_{i,j})+\beta(u-d)$ to estimate the difference:
\begin{align*}
	&y\left(\opcosts(\mx_{i,j})+\beta\left(\frac{\floor{\hat{u}}}{y}-d\right)\right)-\opcosts(\mx_{i,j}^{\floor{\hat{u}}})\\
	>\quad&y\, \frac{(\floor{\hat{u}}-\floor{u})\opcosts(\mx_{i,j})-\left(\frac{\floor{\hat{u}}}{y}-d\right)\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}-\opcosts(\mx_{i,j}^{\floor{\hat{u}}})
\end{align*}
Thus, it suffices to show that
\begin{align*}
	y\,\frac{(\floor{\hat{u}}-\floor{u})\opcosts(\mx_{i,j})-\left(\frac{\floor{\hat{u}}}{y}-d\right)\opcosts(\mx_{i,j}^{\floor{u}})}{yd-\floor{u}}-\opcosts(\mx_{i,j}^{\floor{\hat{u}}})\ge 0
\end{align*}
which can be simplified to
\begin{equation*}
	y(\floor{\hat{u}}-\floor{u})\opcosts(\mx_{i,j})-(\floor{\hat{u}}-yd)\opcosts(\mx_{i,j}^{\floor{u}})-(yd-\floor{u})\opcosts(\mx_{i,j}^{\floor{\hat{u}}})\ge 0
\end{equation*}
To show this inequality, we have to take a closer look on the the schedules' operating costs:
\begin{align*}
	&&&y(\floor{\hat{u}}-\floor{u})\opcosts(\mx_{i,j})-(\floor{\hat{u}}-yd)\opcosts(\mx_{i,j}^{\floor{u}})-(yd-\floor{u})\opcosts(\mx_{i,j}^{\floor{\hat{u}}})\\
	&\stackrel{\eqref{eq:mx_schedule_op_costs_complete}}{=}&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\left(\frac{\lambda_t}{x_t}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\right)
\end{align*}
First, we notice that $\floor{u}\le\floor{\hat{u}}$ and $x_t\le\floor{u}$ for $i\le t\le j$. Together with the fact that $f$ is monotonically increasing, we infer that
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\left(\frac{\lambda_t}{x_t}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{\hat{u}}}\right)\right)\\
	&\ge&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\left(\frac{\lambda_t}{\floor{u}}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{u}}\right)\right)
\end{align*}
Since $d$ is the smallest number of active servers scheduled by $\mx$, and $f$ is non-negative, we can conclude that
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})x_tf\left(\frac{\lambda_t}{\floor{u}}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\\
	&\ge&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})df\left(\frac{\lambda_t}{\floor{u}}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{u}}\right)\right)
\end{align*}
Hence, to finish the case, it suffices to show that
\begin{align*}
	\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})df\left(\frac{\lambda_t}{\floor{u}}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\ge 0
\end{align*}
Further rearranging the left hand side gives us
\begin{align*}
	&&&\sum\limits_{t=i}^j\left(y(\floor{\hat{u}}-\floor{u})df\left(\frac{\lambda_t}{\floor{u}}\right)-(\floor{\hat{u}}-yd)\floor{u}f\left(\frac{\lambda_t}{\floor{u}}\right)-(yd-\floor{u})\floor{\hat{u}}f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(\Bigl(y(\floor{\hat{u}}-\floor{u})d-(\floor{\hat{u}}-yd)\floor{u}-(yd-\floor{u})\floor{\hat{u}}\Bigr)f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(\Bigl(yd\floor{\hat{u}}-yd\floor{u}-\floor{u}\floor{\hat{u}}+yd\floor{u}-yd\floor{\hat{u}}+\floor{u}\floor{\hat{u}}\Bigr)f\left(\frac{\lambda_t}{\floor{u}}\right)\right)\\
	&=&&\sum\limits_{t=i}^j\left(0\cdot f\left(\frac{\lambda_t}{\floor{u}}\right)\right)=0
\end{align*}
which finishes the case.

By iteratively applying above procedure to $\mx$, starting with $i=1$ and stopping with $j=T+1$, we obtain a $B$-restricted schedule $\mx'$ that satisfies $\costs(\mx')\le y\costs(\mx)$.
\end{proof}

\begin{algorithm}[H]
  \caption{$(1+\beps)$-optimal linear-time offline scheduling}
  \label{alg:approx_2_offline_linear}
  \begin{algorithmic}[1]
  \Function{$(1+\beps)$\_optimal\_offline\_scheduling}{$m,T,\Lambda,\beta,f,\beps$}
	  \Let{$(C,S)$}{\Call{shortest\_paths}{$m,T,\Lambda,\beta,f,1+\beps$}}
	  \Let{$\mx$}{\Call{extract\_schedule}{$S,T,m,1+\beps$}}
	  \State \Return{$\mx$}
  \EndFunction
  \Statex
  \Function{node}{$i,m,y$}
	  \State \Return{$\min\bigl\{m,\floor{y^{i-1}}\bigr\}$}
  \EndFunction
  \Statex
  \Function{shortest\_paths}{$m,T,\Lambda,\beta,f,y$}
	\Let{$b$}{$\ceil{\log_y(m)}$}
	\Blet{$C[0\dotso b+1,1\dotso T]$ and $S[0\dotso b+1,1\dotso T]$}{new tables}
	\Statex \Comment{Allocate cost and selection tables}
	\Let{$S[b+1,1]$}{$b+1$ and $C[b+1,1]\gets\costs(0,m,\lambda_1)$}\Comment{Initialize first node in first layer}
	\For{$i \gets b \textrm{ to } 0$}\Comment{Initialize first layer (downward minimization step)}
		\If {$C[i+1,1]<\costs\bigl(0,\Call{node}{i,m,y},\lambda_1\bigr)$}
			\Let{$S[i,1]$}{$S[i+1,1]$ and $C[i,1]\gets C[i+1,1]$}
		\Else
			\Let{$S[i,1]$}{$i$ and $C[i,1]\gets\costs\bigl(0,\Call{node}{i,m,y},\lambda_1\bigr)$}
		\EndIf
	\EndFor
	\For{$t \gets 1 \textrm{ to } T-1$}\Comment{Iterative calculate costs and selections}
		\For{$i \gets 1 \textrm{ to } b+1$}\Comment{Upward minimization step}
			\If {$C[i-1,t]+\beta\bigl(\Call{node}{i,m,y}-\Call{node}{i-1,m,y}\bigr)<C[i,t]$}
			    \Let{$S[i,t]$}{$S[i-1,t]$}
			    \Let{$C[i,t]$}{$C[i-1,t]+\beta\bigl(\Call{node}{i,m,y}-\Call{node}{i-1,m,y}\bigr)$}
			\EndIf
		\EndFor
		\Let{$S[b+1,t+1]$}{$b+1$ and $C[b+1,t+1]\gets C[b+1,t]+\opcosts(m,\lambda_{t+1})$}
		\For{$i \gets b \textrm{ to } 0$}\Comment{Downward minimization step}
			\If {$C[i+1,t+1]<C[i,t]+\opcosts\bigl(\Call{node}{i,m,y},\lambda_{t+1}\bigr)$}
				\Let{$S[i,t+1]$}{$S[i+1,t+1]$ and $C[i,t+1]\gets C[i+1,t+1]$}
			\Else
				\Let{$S[i,t+1]$}{$i$ and $C[i,t+1]\gets C[i,t]+\opcosts\bigl(\Call{node}{i,m,y},\lambda_{t+1}\bigr)$}
			\EndIf
		\EndFor
	\EndFor
	\State \Return{$(C,S)$}
  \EndFunction
  \Statex
  \Function{extract\_schedule}{$S,T,m,y$}
	\Blet{$\mx[1\dotso T]$}{a new array}
	\Let{$i$}{$S[0,T]$}\Comment{Get index of best selection for last time slot}
    	\Let{$\mx[T]$}{$\Call{node}{i,m,y}$}\Comment{Calculate best selection for last time slot}
        \For{$t \gets T-1 \textrm{ to } 1$}\Comment{Iteratively obtain schedule from selection table}
		\Let{$i$}{$S[i,t]$}
		\Let{$\mx[t]$}{$\Call{node}{i,m,y}$}	
	\EndFor
	\State \Return{$\mx$}
  \EndFunction
  \end{algorithmic}
\end{algorithm}
